{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-11T09:35:58.379465Z",
     "iopub.status.busy": "2025-08-11T09:35:58.379186Z",
     "iopub.status.idle": "2025-08-11T09:35:58.780245Z",
     "shell.execute_reply": "2025-08-11T09:35:58.779090Z",
     "shell.execute_reply.started": "2025-08-11T09:35:58.379442Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import kaggle as kg\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"USERNAME\"] = \"<Your Kaggle User Name\"\n",
    "os.environ[\"KEY\"] = \"<Your Kaggle API Key from the key stored at ~/.kaggle/kaggle.json>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg.api.authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kg.api.dataset_download_files(dataset=\"eslamelsolya/laptop-price-prediction\",\n",
    "                              path=\"dataset\",unzip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T09:35:59.680860Z",
     "iopub.status.busy": "2025-08-11T09:35:59.679834Z",
     "iopub.status.idle": "2025-08-11T09:35:59.715659Z",
     "shell.execute_reply": "2025-08-11T09:35:59.714528Z",
     "shell.execute_reply.started": "2025-08-11T09:35:59.680828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"dataset/laptop_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T09:36:04.071338Z",
     "iopub.status.busy": "2025-08-11T09:36:04.071013Z",
     "iopub.status.idle": "2025-08-11T09:36:04.108645Z",
     "shell.execute_reply": "2025-08-11T09:36:04.107797Z",
     "shell.execute_reply.started": "2025-08-11T09:36:04.071294Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Company</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>Inches</th>\n",
       "      <th>ScreenResolution</th>\n",
       "      <th>Cpu</th>\n",
       "      <th>Ram</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Gpu</th>\n",
       "      <th>OpSys</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Retina Display 2560x1600</td>\n",
       "      <td>Intel Core i5 2.3GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>128GB SSD</td>\n",
       "      <td>Intel Iris Plus Graphics 640</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>71378.6832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1440x900</td>\n",
       "      <td>Intel Core i5 1.8GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>128GB Flash Storage</td>\n",
       "      <td>Intel HD Graphics 6000</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.34kg</td>\n",
       "      <td>47895.5232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>HP</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>15.6</td>\n",
       "      <td>Full HD 1920x1080</td>\n",
       "      <td>Intel Core i5 7200U 2.5GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>256GB SSD</td>\n",
       "      <td>Intel HD Graphics 620</td>\n",
       "      <td>No OS</td>\n",
       "      <td>1.86kg</td>\n",
       "      <td>30636.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>15.4</td>\n",
       "      <td>IPS Panel Retina Display 2880x1800</td>\n",
       "      <td>Intel Core i7 2.7GHz</td>\n",
       "      <td>16GB</td>\n",
       "      <td>512GB SSD</td>\n",
       "      <td>AMD Radeon Pro 455</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.83kg</td>\n",
       "      <td>135195.3360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Apple</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Retina Display 2560x1600</td>\n",
       "      <td>Intel Core i5 3.1GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>256GB SSD</td>\n",
       "      <td>Intel Iris Plus Graphics 650</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>96095.8080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 Company   TypeName  Inches                    ScreenResolution  \\\n",
       "0           0   Apple  Ultrabook    13.3  IPS Panel Retina Display 2560x1600   \n",
       "1           1   Apple  Ultrabook    13.3                            1440x900   \n",
       "2           2      HP   Notebook    15.6                   Full HD 1920x1080   \n",
       "3           3   Apple  Ultrabook    15.4  IPS Panel Retina Display 2880x1800   \n",
       "4           4   Apple  Ultrabook    13.3  IPS Panel Retina Display 2560x1600   \n",
       "\n",
       "                          Cpu   Ram               Memory  \\\n",
       "0        Intel Core i5 2.3GHz   8GB            128GB SSD   \n",
       "1        Intel Core i5 1.8GHz   8GB  128GB Flash Storage   \n",
       "2  Intel Core i5 7200U 2.5GHz   8GB            256GB SSD   \n",
       "3        Intel Core i7 2.7GHz  16GB            512GB SSD   \n",
       "4        Intel Core i5 3.1GHz   8GB            256GB SSD   \n",
       "\n",
       "                            Gpu  OpSys  Weight        Price  \n",
       "0  Intel Iris Plus Graphics 640  macOS  1.37kg   71378.6832  \n",
       "1        Intel HD Graphics 6000  macOS  1.34kg   47895.5232  \n",
       "2         Intel HD Graphics 620  No OS  1.86kg   30636.0000  \n",
       "3            AMD Radeon Pro 455  macOS  1.83kg  135195.3360  \n",
       "4  Intel Iris Plus Graphics 650  macOS  1.37kg   96095.8080  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T09:36:06.697675Z",
     "iopub.status.busy": "2025-08-11T09:36:06.697349Z",
     "iopub.status.idle": "2025-08-11T09:36:06.704376Z",
     "shell.execute_reply": "2025-08-11T09:36:06.703429Z",
     "shell.execute_reply.started": "2025-08-11T09:36:06.697649Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1303, 12)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T09:36:08.075397Z",
     "iopub.status.busy": "2025-08-11T09:36:08.074577Z",
     "iopub.status.idle": "2025-08-11T09:36:08.088345Z",
     "shell.execute_reply": "2025-08-11T09:36:08.087507Z",
     "shell.execute_reply.started": "2025-08-11T09:36:08.075368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.drop(labels=\"Unnamed: 0\",axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T09:36:09.675835Z",
     "iopub.status.busy": "2025-08-11T09:36:09.675397Z",
     "iopub.status.idle": "2025-08-11T09:36:09.682369Z",
     "shell.execute_reply": "2025-08-11T09:36:09.681367Z",
     "shell.execute_reply.started": "2025-08-11T09:36:09.675802Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1303, 11)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T09:36:11.265542Z",
     "iopub.status.busy": "2025-08-11T09:36:11.264641Z",
     "iopub.status.idle": "2025-08-11T09:36:11.278846Z",
     "shell.execute_reply": "2025-08-11T09:36:11.277781Z",
     "shell.execute_reply.started": "2025-08-11T09:36:11.265511Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>Inches</th>\n",
       "      <th>ScreenResolution</th>\n",
       "      <th>Cpu</th>\n",
       "      <th>Ram</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Gpu</th>\n",
       "      <th>OpSys</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Retina Display 2560x1600</td>\n",
       "      <td>Intel Core i5 2.3GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>128GB SSD</td>\n",
       "      <td>Intel Iris Plus Graphics 640</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>71378.6832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>1440x900</td>\n",
       "      <td>Intel Core i5 1.8GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>128GB Flash Storage</td>\n",
       "      <td>Intel HD Graphics 6000</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.34kg</td>\n",
       "      <td>47895.5232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HP</td>\n",
       "      <td>Notebook</td>\n",
       "      <td>15.6</td>\n",
       "      <td>Full HD 1920x1080</td>\n",
       "      <td>Intel Core i5 7200U 2.5GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>256GB SSD</td>\n",
       "      <td>Intel HD Graphics 620</td>\n",
       "      <td>No OS</td>\n",
       "      <td>1.86kg</td>\n",
       "      <td>30636.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>15.4</td>\n",
       "      <td>IPS Panel Retina Display 2880x1800</td>\n",
       "      <td>Intel Core i7 2.7GHz</td>\n",
       "      <td>16GB</td>\n",
       "      <td>512GB SSD</td>\n",
       "      <td>AMD Radeon Pro 455</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.83kg</td>\n",
       "      <td>135195.3360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Apple</td>\n",
       "      <td>Ultrabook</td>\n",
       "      <td>13.3</td>\n",
       "      <td>IPS Panel Retina Display 2560x1600</td>\n",
       "      <td>Intel Core i5 3.1GHz</td>\n",
       "      <td>8GB</td>\n",
       "      <td>256GB SSD</td>\n",
       "      <td>Intel Iris Plus Graphics 650</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>96095.8080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Company   TypeName  Inches                    ScreenResolution  \\\n",
       "0   Apple  Ultrabook    13.3  IPS Panel Retina Display 2560x1600   \n",
       "1   Apple  Ultrabook    13.3                            1440x900   \n",
       "2      HP   Notebook    15.6                   Full HD 1920x1080   \n",
       "3   Apple  Ultrabook    15.4  IPS Panel Retina Display 2880x1800   \n",
       "4   Apple  Ultrabook    13.3  IPS Panel Retina Display 2560x1600   \n",
       "\n",
       "                          Cpu   Ram               Memory  \\\n",
       "0        Intel Core i5 2.3GHz   8GB            128GB SSD   \n",
       "1        Intel Core i5 1.8GHz   8GB  128GB Flash Storage   \n",
       "2  Intel Core i5 7200U 2.5GHz   8GB            256GB SSD   \n",
       "3        Intel Core i7 2.7GHz  16GB            512GB SSD   \n",
       "4        Intel Core i5 3.1GHz   8GB            256GB SSD   \n",
       "\n",
       "                            Gpu  OpSys  Weight        Price  \n",
       "0  Intel Iris Plus Graphics 640  macOS  1.37kg   71378.6832  \n",
       "1        Intel HD Graphics 6000  macOS  1.34kg   47895.5232  \n",
       "2         Intel HD Graphics 620  No OS  1.86kg   30636.0000  \n",
       "3            AMD Radeon Pro 455  macOS  1.83kg  135195.3360  \n",
       "4  Intel Iris Plus Graphics 650  macOS  1.37kg   96095.8080  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T09:59:22.945782Z",
     "iopub.status.busy": "2025-08-11T09:59:22.945459Z",
     "iopub.status.idle": "2025-08-11T09:59:22.951076Z",
     "shell.execute_reply": "2025-08-11T09:59:22.950162Z",
     "shell.execute_reply.started": "2025-08-11T09:59:22.945760Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def ordinal_encode_column(column_name):\n",
    "\n",
    "    groupby_colname = data.groupby(by=column_name)[\"Price\"].mean()\n",
    "    groupby_colname = groupby_colname.sort_values()\n",
    "    colname2idx = dict(zip(groupby_colname.index,list(range(len(groupby_colname)))))\n",
    "    data[column_name] = data[column_name].map(colname2idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T09:59:24.025381Z",
     "iopub.status.busy": "2025-08-11T09:59:24.025015Z",
     "iopub.status.idle": "2025-08-11T09:59:24.031436Z",
     "shell.execute_reply": "2025-08-11T09:59:24.030594Z",
     "shell.execute_reply.started": "2025-08-11T09:59:24.025357Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Company', 'TypeName', 'Inches', 'ScreenResolution', 'Cpu', 'Ram',\n",
       "       'Memory', 'Gpu', 'OpSys', 'Weight', 'Price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T09:59:25.976659Z",
     "iopub.status.busy": "2025-08-11T09:59:25.976373Z",
     "iopub.status.idle": "2025-08-11T09:59:25.992285Z",
     "shell.execute_reply": "2025-08-11T09:59:25.991382Z",
     "shell.execute_reply.started": "2025-08-11T09:59:25.976637Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "ordinal_encode_column(data.columns[0])\n",
    "ordinal_encode_column(data.columns[1])\n",
    "ordinal_encode_column(data.columns[3])\n",
    "ordinal_encode_column(data.columns[4])\n",
    "ordinal_encode_column(data.columns[6])\n",
    "ordinal_encode_column(data.columns[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T09:59:32.547597Z",
     "iopub.status.busy": "2025-08-11T09:59:32.547287Z",
     "iopub.status.idle": "2025-08-11T09:59:32.561384Z",
     "shell.execute_reply": "2025-08-11T09:59:32.560285Z",
     "shell.execute_reply.started": "2025-08-11T09:59:32.547575Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>Inches</th>\n",
       "      <th>ScreenResolution</th>\n",
       "      <th>Cpu</th>\n",
       "      <th>Ram</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Gpu</th>\n",
       "      <th>OpSys</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>85</td>\n",
       "      <td>8GB</td>\n",
       "      <td>13</td>\n",
       "      <td>80</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>71378.6832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>13</td>\n",
       "      <td>64</td>\n",
       "      <td>8GB</td>\n",
       "      <td>14</td>\n",
       "      <td>45</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.34kg</td>\n",
       "      <td>47895.5232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>15</td>\n",
       "      <td>58</td>\n",
       "      <td>8GB</td>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "      <td>No OS</td>\n",
       "      <td>1.86kg</td>\n",
       "      <td>30636.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>15.4</td>\n",
       "      <td>36</td>\n",
       "      <td>111</td>\n",
       "      <td>16GB</td>\n",
       "      <td>30</td>\n",
       "      <td>95</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.83kg</td>\n",
       "      <td>135195.3360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>99</td>\n",
       "      <td>8GB</td>\n",
       "      <td>20</td>\n",
       "      <td>84</td>\n",
       "      <td>macOS</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>96095.8080</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Company  TypeName  Inches  ScreenResolution  Cpu   Ram  Memory  Gpu  OpSys  \\\n",
       "0       13         3    13.3                26   85   8GB      13   80  macOS   \n",
       "1       13         3    13.3                13   64   8GB      14   45  macOS   \n",
       "2        5         1    15.6                15   58   8GB      20   54  No OS   \n",
       "3       13         3    15.4                36  111  16GB      30   95  macOS   \n",
       "4       13         3    13.3                26   99   8GB      20   84  macOS   \n",
       "\n",
       "   Weight        Price  \n",
       "0  1.37kg   71378.6832  \n",
       "1  1.34kg   47895.5232  \n",
       "2  1.86kg   30636.0000  \n",
       "3  1.83kg  135195.3360  \n",
       "4  1.37kg   96095.8080  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:04:05.105044Z",
     "iopub.status.busy": "2025-08-11T10:04:05.104410Z",
     "iopub.status.idle": "2025-08-11T10:04:05.111676Z",
     "shell.execute_reply": "2025-08-11T10:04:05.110727Z",
     "shell.execute_reply.started": "2025-08-11T10:04:05.105019Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['macOS', 'No OS', 'Windows 10', 'Mac OS X', 'Linux', 'Android',\n",
       "       'Windows 10 S', 'Chrome OS', 'Windows 7'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data.columns[8]].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:10:46.151775Z",
     "iopub.status.busy": "2025-08-11T10:10:46.151018Z",
     "iopub.status.idle": "2025-08-11T10:10:46.195577Z",
     "shell.execute_reply": "2025-08-11T10:10:46.194777Z",
     "shell.execute_reply.started": "2025-08-11T10:10:46.151744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "os_ohe = pd.crosstab(index=data.index,columns=data[data.columns[8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:10:50.850248Z",
     "iopub.status.busy": "2025-08-11T10:10:50.849968Z",
     "iopub.status.idle": "2025-08-11T10:10:50.862151Z",
     "shell.execute_reply": "2025-08-11T10:10:50.861165Z",
     "shell.execute_reply.started": "2025-08-11T10:10:50.850228Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>OpSys</th>\n",
       "      <th>Android</th>\n",
       "      <th>Chrome OS</th>\n",
       "      <th>Linux</th>\n",
       "      <th>Mac OS X</th>\n",
       "      <th>No OS</th>\n",
       "      <th>Windows 10</th>\n",
       "      <th>Windows 10 S</th>\n",
       "      <th>Windows 7</th>\n",
       "      <th>macOS</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1303 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "OpSys  Android  Chrome OS  Linux  Mac OS X  No OS  Windows 10  Windows 10 S  \\\n",
       "row_0                                                                         \n",
       "0            0          0      0         0      0           0             0   \n",
       "1            0          0      0         0      0           0             0   \n",
       "2            0          0      0         0      1           0             0   \n",
       "3            0          0      0         0      0           0             0   \n",
       "4            0          0      0         0      0           0             0   \n",
       "...        ...        ...    ...       ...    ...         ...           ...   \n",
       "1298         0          0      0         0      0           1             0   \n",
       "1299         0          0      0         0      0           1             0   \n",
       "1300         0          0      0         0      0           1             0   \n",
       "1301         0          0      0         0      0           1             0   \n",
       "1302         0          0      0         0      0           1             0   \n",
       "\n",
       "OpSys  Windows 7  macOS  \n",
       "row_0                    \n",
       "0              0      1  \n",
       "1              0      1  \n",
       "2              0      0  \n",
       "3              0      1  \n",
       "4              0      1  \n",
       "...          ...    ...  \n",
       "1298           0      0  \n",
       "1299           0      0  \n",
       "1300           0      0  \n",
       "1301           0      0  \n",
       "1302           0      0  \n",
       "\n",
       "[1303 rows x 9 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os_ohe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:19:25.484610Z",
     "iopub.status.busy": "2025-08-11T10:19:25.484304Z",
     "iopub.status.idle": "2025-08-11T10:19:25.491491Z",
     "shell.execute_reply": "2025-08-11T10:19:25.490310Z",
     "shell.execute_reply.started": "2025-08-11T10:19:25.484587Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data.drop(labels=data.columns[8],inplace=True,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:21:10.896509Z",
     "iopub.status.busy": "2025-08-11T10:21:10.895729Z",
     "iopub.status.idle": "2025-08-11T10:21:10.901860Z",
     "shell.execute_reply": "2025-08-11T10:21:10.900974Z",
     "shell.execute_reply.started": "2025-08-11T10:21:10.896480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "y = data[\"Price\"]\n",
    "X = data.iloc[:,0:9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:22:32.491131Z",
     "iopub.status.busy": "2025-08-11T10:22:32.490825Z",
     "iopub.status.idle": "2025-08-11T10:22:32.497879Z",
     "shell.execute_reply": "2025-08-11T10:22:32.496955Z",
     "shell.execute_reply.started": "2025-08-11T10:22:32.491107Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X = pd.concat((X,os_ohe),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:22:34.323650Z",
     "iopub.status.busy": "2025-08-11T10:22:34.323353Z",
     "iopub.status.idle": "2025-08-11T10:22:34.340168Z",
     "shell.execute_reply": "2025-08-11T10:22:34.339183Z",
     "shell.execute_reply.started": "2025-08-11T10:22:34.323626Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>Inches</th>\n",
       "      <th>ScreenResolution</th>\n",
       "      <th>Cpu</th>\n",
       "      <th>Ram</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Gpu</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Android</th>\n",
       "      <th>Chrome OS</th>\n",
       "      <th>Linux</th>\n",
       "      <th>Mac OS X</th>\n",
       "      <th>No OS</th>\n",
       "      <th>Windows 10</th>\n",
       "      <th>Windows 10 S</th>\n",
       "      <th>Windows 7</th>\n",
       "      <th>macOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>85</td>\n",
       "      <td>8GB</td>\n",
       "      <td>13</td>\n",
       "      <td>80</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>13</td>\n",
       "      <td>64</td>\n",
       "      <td>8GB</td>\n",
       "      <td>14</td>\n",
       "      <td>45</td>\n",
       "      <td>1.34kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>15</td>\n",
       "      <td>58</td>\n",
       "      <td>8GB</td>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "      <td>1.86kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>15.4</td>\n",
       "      <td>36</td>\n",
       "      <td>111</td>\n",
       "      <td>16GB</td>\n",
       "      <td>30</td>\n",
       "      <td>95</td>\n",
       "      <td>1.83kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>99</td>\n",
       "      <td>8GB</td>\n",
       "      <td>20</td>\n",
       "      <td>84</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1298</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>14.0</td>\n",
       "      <td>16</td>\n",
       "      <td>77</td>\n",
       "      <td>4GB</td>\n",
       "      <td>13</td>\n",
       "      <td>47</td>\n",
       "      <td>1.8kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>13.3</td>\n",
       "      <td>22</td>\n",
       "      <td>77</td>\n",
       "      <td>16GB</td>\n",
       "      <td>30</td>\n",
       "      <td>47</td>\n",
       "      <td>1.3kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1300</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>14.0</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>2GB</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>1.5kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1301</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>6</td>\n",
       "      <td>77</td>\n",
       "      <td>6GB</td>\n",
       "      <td>12</td>\n",
       "      <td>32</td>\n",
       "      <td>2.19kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1302</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>6</td>\n",
       "      <td>16</td>\n",
       "      <td>4GB</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>2.2kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1303 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Company  TypeName  Inches  ScreenResolution  Cpu   Ram  Memory  Gpu  \\\n",
       "0          13         3    13.3                26   85   8GB      13   80   \n",
       "1          13         3    13.3                13   64   8GB      14   45   \n",
       "2           5         1    15.6                15   58   8GB      20   54   \n",
       "3          13         3    15.4                36  111  16GB      30   95   \n",
       "4          13         3    13.3                26   99   8GB      20   84   \n",
       "...       ...       ...     ...               ...  ...   ...     ...  ...   \n",
       "1298        6         2    14.0                16   77   4GB      13   47   \n",
       "1299        6         2    13.3                22   77  16GB      30   47   \n",
       "1300        6         1    14.0                 6   16   2GB       6    6   \n",
       "1301        5         1    15.6                 6   77   6GB      12   32   \n",
       "1302        7         1    15.6                 6   16   4GB       9    6   \n",
       "\n",
       "      Weight  Android  Chrome OS  Linux  Mac OS X  No OS  Windows 10  \\\n",
       "0     1.37kg        0          0      0         0      0           0   \n",
       "1     1.34kg        0          0      0         0      0           0   \n",
       "2     1.86kg        0          0      0         0      1           0   \n",
       "3     1.83kg        0          0      0         0      0           0   \n",
       "4     1.37kg        0          0      0         0      0           0   \n",
       "...      ...      ...        ...    ...       ...    ...         ...   \n",
       "1298   1.8kg        0          0      0         0      0           1   \n",
       "1299   1.3kg        0          0      0         0      0           1   \n",
       "1300   1.5kg        0          0      0         0      0           1   \n",
       "1301  2.19kg        0          0      0         0      0           1   \n",
       "1302   2.2kg        0          0      0         0      0           1   \n",
       "\n",
       "      Windows 10 S  Windows 7  macOS  \n",
       "0                0          0      1  \n",
       "1                0          0      1  \n",
       "2                0          0      0  \n",
       "3                0          0      1  \n",
       "4                0          0      1  \n",
       "...            ...        ...    ...  \n",
       "1298             0          0      0  \n",
       "1299             0          0      0  \n",
       "1300             0          0      0  \n",
       "1301             0          0      0  \n",
       "1302             0          0      0  \n",
       "\n",
       "[1303 rows x 18 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:31:54.239067Z",
     "iopub.status.busy": "2025-08-11T10:31:54.238795Z",
     "iopub.status.idle": "2025-08-11T10:31:54.245186Z",
     "shell.execute_reply": "2025-08-11T10:31:54.244341Z",
     "shell.execute_reply.started": "2025-08-11T10:31:54.239045Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X[X.columns[5]] = X[X.columns[5]].apply(lambda x: int(x.split(\"GB\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:31:59.202053Z",
     "iopub.status.busy": "2025-08-11T10:31:59.201783Z",
     "iopub.status.idle": "2025-08-11T10:31:59.216000Z",
     "shell.execute_reply": "2025-08-11T10:31:59.215049Z",
     "shell.execute_reply.started": "2025-08-11T10:31:59.202035Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>Inches</th>\n",
       "      <th>ScreenResolution</th>\n",
       "      <th>Cpu</th>\n",
       "      <th>Ram</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Gpu</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Android</th>\n",
       "      <th>Chrome OS</th>\n",
       "      <th>Linux</th>\n",
       "      <th>Mac OS X</th>\n",
       "      <th>No OS</th>\n",
       "      <th>Windows 10</th>\n",
       "      <th>Windows 10 S</th>\n",
       "      <th>Windows 7</th>\n",
       "      <th>macOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>85</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>80</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>13</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>45</td>\n",
       "      <td>1.34kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>15</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "      <td>1.86kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>15.4</td>\n",
       "      <td>36</td>\n",
       "      <td>111</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>95</td>\n",
       "      <td>1.83kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>99</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>84</td>\n",
       "      <td>1.37kg</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Company  TypeName  Inches  ScreenResolution  Cpu  Ram  Memory  Gpu  Weight  \\\n",
       "0       13         3    13.3                26   85    8      13   80  1.37kg   \n",
       "1       13         3    13.3                13   64    8      14   45  1.34kg   \n",
       "2        5         1    15.6                15   58    8      20   54  1.86kg   \n",
       "3       13         3    15.4                36  111   16      30   95  1.83kg   \n",
       "4       13         3    13.3                26   99    8      20   84  1.37kg   \n",
       "\n",
       "   Android  Chrome OS  Linux  Mac OS X  No OS  Windows 10  Windows 10 S  \\\n",
       "0        0          0      0         0      0           0             0   \n",
       "1        0          0      0         0      0           0             0   \n",
       "2        0          0      0         0      1           0             0   \n",
       "3        0          0      0         0      0           0             0   \n",
       "4        0          0      0         0      0           0             0   \n",
       "\n",
       "   Windows 7  macOS  \n",
       "0          0      1  \n",
       "1          0      1  \n",
       "2          0      0  \n",
       "3          0      1  \n",
       "4          0      1  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:32:20.128581Z",
     "iopub.status.busy": "2025-08-11T10:32:20.128203Z",
     "iopub.status.idle": "2025-08-11T10:32:20.135153Z",
     "shell.execute_reply": "2025-08-11T10:32:20.134166Z",
     "shell.execute_reply.started": "2025-08-11T10:32:20.128557Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "X[X.columns[8]] = X[X.columns[8]].apply(lambda x: float(x.split(\"kg\")[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-08-11T10:32:25.006479Z",
     "iopub.status.busy": "2025-08-11T10:32:25.005400Z",
     "iopub.status.idle": "2025-08-11T10:32:25.022223Z",
     "shell.execute_reply": "2025-08-11T10:32:25.021018Z",
     "shell.execute_reply.started": "2025-08-11T10:32:25.006439Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>Inches</th>\n",
       "      <th>ScreenResolution</th>\n",
       "      <th>Cpu</th>\n",
       "      <th>Ram</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Gpu</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Android</th>\n",
       "      <th>Chrome OS</th>\n",
       "      <th>Linux</th>\n",
       "      <th>Mac OS X</th>\n",
       "      <th>No OS</th>\n",
       "      <th>Windows 10</th>\n",
       "      <th>Windows 10 S</th>\n",
       "      <th>Windows 7</th>\n",
       "      <th>macOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>85</td>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>80</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>13</td>\n",
       "      <td>64</td>\n",
       "      <td>8</td>\n",
       "      <td>14</td>\n",
       "      <td>45</td>\n",
       "      <td>1.34</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>15.6</td>\n",
       "      <td>15</td>\n",
       "      <td>58</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>54</td>\n",
       "      <td>1.86</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>15.4</td>\n",
       "      <td>36</td>\n",
       "      <td>111</td>\n",
       "      <td>16</td>\n",
       "      <td>30</td>\n",
       "      <td>95</td>\n",
       "      <td>1.83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>13.3</td>\n",
       "      <td>26</td>\n",
       "      <td>99</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>84</td>\n",
       "      <td>1.37</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Company  TypeName  Inches  ScreenResolution  Cpu  Ram  Memory  Gpu  Weight  \\\n",
       "0       13         3    13.3                26   85    8      13   80    1.37   \n",
       "1       13         3    13.3                13   64    8      14   45    1.34   \n",
       "2        5         1    15.6                15   58    8      20   54    1.86   \n",
       "3       13         3    15.4                36  111   16      30   95    1.83   \n",
       "4       13         3    13.3                26   99    8      20   84    1.37   \n",
       "\n",
       "   Android  Chrome OS  Linux  Mac OS X  No OS  Windows 10  Windows 10 S  \\\n",
       "0        0          0      0         0      0           0             0   \n",
       "1        0          0      0         0      0           0             0   \n",
       "2        0          0      0         0      1           0             0   \n",
       "3        0          0      0         0      0           0             0   \n",
       "4        0          0      0         0      0           0             0   \n",
       "\n",
       "   Windows 7  macOS  \n",
       "0          0      1  \n",
       "1          0      1  \n",
       "2          0      0  \n",
       "3          0      1  \n",
       "4          0      1  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_columns = X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "y = y.reshape(y.shape[0],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = (X - np.mean(X,axis=0))/np.std(X,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.DataFrame(data=X,columns=X_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>TypeName</th>\n",
       "      <th>Inches</th>\n",
       "      <th>ScreenResolution</th>\n",
       "      <th>Cpu</th>\n",
       "      <th>Ram</th>\n",
       "      <th>Memory</th>\n",
       "      <th>Gpu</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Android</th>\n",
       "      <th>Chrome OS</th>\n",
       "      <th>Linux</th>\n",
       "      <th>Mac OS X</th>\n",
       "      <th>No OS</th>\n",
       "      <th>Windows 10</th>\n",
       "      <th>Windows 10 S</th>\n",
       "      <th>Windows 7</th>\n",
       "      <th>macOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.910326</td>\n",
       "      <td>0.838766</td>\n",
       "      <td>-1.204407</td>\n",
       "      <td>1.631160</td>\n",
       "      <td>0.806055</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>-0.659348</td>\n",
       "      <td>1.311399</td>\n",
       "      <td>-1.005283</td>\n",
       "      <td>-0.039208</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.223517</td>\n",
       "      <td>-0.078598</td>\n",
       "      <td>-0.230987</td>\n",
       "      <td>-2.154227</td>\n",
       "      <td>-0.078598</td>\n",
       "      <td>-0.189132</td>\n",
       "      <td>9.961464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.910326</td>\n",
       "      <td>0.838766</td>\n",
       "      <td>-1.204407</td>\n",
       "      <td>-0.250238</td>\n",
       "      <td>-0.018441</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>-0.527296</td>\n",
       "      <td>-0.153976</td>\n",
       "      <td>-1.050381</td>\n",
       "      <td>-0.039208</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.223517</td>\n",
       "      <td>-0.078598</td>\n",
       "      <td>-0.230987</td>\n",
       "      <td>-2.154227</td>\n",
       "      <td>-0.078598</td>\n",
       "      <td>-0.189132</td>\n",
       "      <td>9.961464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.711217</td>\n",
       "      <td>-0.737171</td>\n",
       "      <td>0.408772</td>\n",
       "      <td>0.039207</td>\n",
       "      <td>-0.254011</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>0.265016</td>\n",
       "      <td>0.222835</td>\n",
       "      <td>-0.268684</td>\n",
       "      <td>-0.039208</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.223517</td>\n",
       "      <td>-0.078598</td>\n",
       "      <td>4.329252</td>\n",
       "      <td>-2.154227</td>\n",
       "      <td>-0.078598</td>\n",
       "      <td>-0.189132</td>\n",
       "      <td>-0.100387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.910326</td>\n",
       "      <td>0.838766</td>\n",
       "      <td>0.268495</td>\n",
       "      <td>3.078389</td>\n",
       "      <td>1.826860</td>\n",
       "      <td>1.498767</td>\n",
       "      <td>1.585537</td>\n",
       "      <td>1.939416</td>\n",
       "      <td>-0.313782</td>\n",
       "      <td>-0.039208</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.223517</td>\n",
       "      <td>-0.078598</td>\n",
       "      <td>-0.230987</td>\n",
       "      <td>-2.154227</td>\n",
       "      <td>-0.078598</td>\n",
       "      <td>-0.189132</td>\n",
       "      <td>9.961464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.910326</td>\n",
       "      <td>0.838766</td>\n",
       "      <td>-1.204407</td>\n",
       "      <td>1.631160</td>\n",
       "      <td>1.355719</td>\n",
       "      <td>-0.075195</td>\n",
       "      <td>0.265016</td>\n",
       "      <td>1.478870</td>\n",
       "      <td>-1.005283</td>\n",
       "      <td>-0.039208</td>\n",
       "      <td>-0.145464</td>\n",
       "      <td>-0.223517</td>\n",
       "      <td>-0.078598</td>\n",
       "      <td>-0.230987</td>\n",
       "      <td>-2.154227</td>\n",
       "      <td>-0.078598</td>\n",
       "      <td>-0.189132</td>\n",
       "      <td>9.961464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Company  TypeName    Inches  ScreenResolution       Cpu       Ram  \\\n",
       "0  1.910326  0.838766 -1.204407          1.631160  0.806055 -0.075195   \n",
       "1  1.910326  0.838766 -1.204407         -0.250238 -0.018441 -0.075195   \n",
       "2 -0.711217 -0.737171  0.408772          0.039207 -0.254011 -0.075195   \n",
       "3  1.910326  0.838766  0.268495          3.078389  1.826860  1.498767   \n",
       "4  1.910326  0.838766 -1.204407          1.631160  1.355719 -0.075195   \n",
       "\n",
       "     Memory       Gpu    Weight   Android  Chrome OS     Linux  Mac OS X  \\\n",
       "0 -0.659348  1.311399 -1.005283 -0.039208  -0.145464 -0.223517 -0.078598   \n",
       "1 -0.527296 -0.153976 -1.050381 -0.039208  -0.145464 -0.223517 -0.078598   \n",
       "2  0.265016  0.222835 -0.268684 -0.039208  -0.145464 -0.223517 -0.078598   \n",
       "3  1.585537  1.939416 -0.313782 -0.039208  -0.145464 -0.223517 -0.078598   \n",
       "4  0.265016  1.478870 -1.005283 -0.039208  -0.145464 -0.223517 -0.078598   \n",
       "\n",
       "      No OS  Windows 10  Windows 10 S  Windows 7     macOS  \n",
       "0 -0.230987   -2.154227     -0.078598  -0.189132  9.961464  \n",
       "1 -0.230987   -2.154227     -0.078598  -0.189132  9.961464  \n",
       "2  4.329252   -2.154227     -0.078598  -0.189132 -0.100387  \n",
       "3 -0.230987   -2.154227     -0.078598  -0.189132  9.961464  \n",
       "4 -0.230987   -2.154227     -0.078598  -0.189132  9.961464  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x71c7b8d5ac30>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 583.875x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.displot(data=pd.DataFrame(y,columns=[\"Price\"]),kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.FacetGrid at 0x71c7b6aeec30>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 583.875x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "y = np.log(y)\n",
    "sns.displot(data=pd.DataFrame(data=y,columns=[\"Price\"]),kde=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[0:int(0.75*X.shape[0]),:]\n",
    "X_test = X[int(0.75*X.shape[0]):,:]\n",
    "\n",
    "y_train = y[0:int(0.75*y.shape[0])]\n",
    "y_test = y[int(0.75*y.shape[0]):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(977, 18)\n",
      "(326, 18)\n",
      "(977, 1)\n",
      "(326, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def del_by_del_w(w0,w_vec):\n",
    "\n",
    "    del_by_del_w0 = (-2/y_train.shape[0])*np.sum(y_train - (w0 + np.matmul(X_train,w_vec)))\n",
    "    del_by_del_w_vec = (-2/y_train.shape[0])*np.matmul((y_train - (w0 + np.matmul(X_train,w_vec))).T,X_train).T\n",
    "\n",
    "    return [del_by_del_w0, del_by_del_w_vec]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(w0,w_vec,X,y):\n",
    "\n",
    "    y_hat = (w0 + np.matmul(X,w_vec))\n",
    "    error_vec = (y - y_hat)\n",
    "    return (1/y_hat.shape[0])*np.matmul(error_vec.T,error_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch # 0, Training Loss = [[117.40907975]], Testing Loss = [[117.1623268]]\n",
      "Epoch # 1, Training Loss = [[116.91840414]], Testing Loss = [[116.73487246]]\n",
      "Epoch # 2, Training Loss = [[116.42994491]], Testing Loss = [[116.30916556]]\n",
      "Epoch # 3, Training Loss = [[115.9436895]], Testing Loss = [[115.88519713]]\n",
      "Epoch # 4, Training Loss = [[115.45962546]], Testing Loss = [[115.46295826]]\n",
      "Epoch # 5, Training Loss = [[114.97774046]], Testing Loss = [[115.04244012]]\n",
      "Epoch # 6, Training Loss = [[114.49802226]], Testing Loss = [[114.62363395]]\n",
      "Epoch # 7, Training Loss = [[114.02045874]], Testing Loss = [[114.20653108]]\n",
      "Epoch # 8, Training Loss = [[113.54503789]], Testing Loss = [[113.79112291]]\n",
      "Epoch # 9, Training Loss = [[113.07174779]], Testing Loss = [[113.37740092]]\n",
      "Epoch # 10, Training Loss = [[112.60057665]], Testing Loss = [[112.96535666]]\n",
      "Epoch # 11, Training Loss = [[112.13151275]], Testing Loss = [[112.55498173]]\n",
      "Epoch # 12, Training Loss = [[111.66454449]], Testing Loss = [[112.14626785]]\n",
      "Epoch # 13, Training Loss = [[111.19966037]], Testing Loss = [[111.73920676]]\n",
      "Epoch # 14, Training Loss = [[110.73684898]], Testing Loss = [[111.33379029]]\n",
      "Epoch # 15, Training Loss = [[110.27609901]], Testing Loss = [[110.93001035]]\n",
      "Epoch # 16, Training Loss = [[109.81739924]], Testing Loss = [[110.5278589]]\n",
      "Epoch # 17, Training Loss = [[109.36073856]], Testing Loss = [[110.12732797]]\n",
      "Epoch # 18, Training Loss = [[108.90610594]], Testing Loss = [[109.72840966]]\n",
      "Epoch # 19, Training Loss = [[108.45349043]], Testing Loss = [[109.33109612]]\n",
      "Epoch # 20, Training Loss = [[108.0028812]], Testing Loss = [[108.93537959]]\n",
      "Epoch # 21, Training Loss = [[107.55426749]], Testing Loss = [[108.54125234]]\n",
      "Epoch # 22, Training Loss = [[107.10763863]], Testing Loss = [[108.14870672]]\n",
      "Epoch # 23, Training Loss = [[106.66298403]], Testing Loss = [[107.75773514]]\n",
      "Epoch # 24, Training Loss = [[106.22029321]], Testing Loss = [[107.36833007]]\n",
      "Epoch # 25, Training Loss = [[105.77955574]], Testing Loss = [[106.98048404]]\n",
      "Epoch # 26, Training Loss = [[105.34076132]], Testing Loss = [[106.59418962]]\n",
      "Epoch # 27, Training Loss = [[104.90389968]], Testing Loss = [[106.20943946]]\n",
      "Epoch # 28, Training Loss = [[104.46896067]], Testing Loss = [[105.82622625]]\n",
      "Epoch # 29, Training Loss = [[104.0359342]], Testing Loss = [[105.44454276]]\n",
      "Epoch # 30, Training Loss = [[103.60481027]], Testing Loss = [[105.06438178]]\n",
      "Epoch # 31, Training Loss = [[103.17557896]], Testing Loss = [[104.68573618]]\n",
      "Epoch # 32, Training Loss = [[102.74823042]], Testing Loss = [[104.30859888]]\n",
      "Epoch # 33, Training Loss = [[102.32275488]], Testing Loss = [[103.93296284]]\n",
      "Epoch # 34, Training Loss = [[101.89914264]], Testing Loss = [[103.55882108]]\n",
      "Epoch # 35, Training Loss = [[101.47738408]], Testing Loss = [[103.18616668]]\n",
      "Epoch # 36, Training Loss = [[101.05746966]], Testing Loss = [[102.81499275]]\n",
      "Epoch # 37, Training Loss = [[100.63938989]], Testing Loss = [[102.44529247]]\n",
      "Epoch # 38, Training Loss = [[100.22313539]], Testing Loss = [[102.07705905]]\n",
      "Epoch # 39, Training Loss = [[99.80869682]], Testing Loss = [[101.71028577]]\n",
      "Epoch # 40, Training Loss = [[99.39606491]], Testing Loss = [[101.34496595]]\n",
      "Epoch # 41, Training Loss = [[98.98523048]], Testing Loss = [[100.98109294]]\n",
      "Epoch # 42, Training Loss = [[98.5761844]], Testing Loss = [[100.61866016]]\n",
      "Epoch # 43, Training Loss = [[98.16891762]], Testing Loss = [[100.25766106]]\n",
      "Epoch # 44, Training Loss = [[97.76342114]], Testing Loss = [[99.89808915]]\n",
      "Epoch # 45, Training Loss = [[97.35968605]], Testing Loss = [[99.53993797]]\n",
      "Epoch # 46, Training Loss = [[96.95770349]], Testing Loss = [[99.18320112]]\n",
      "Epoch # 47, Training Loss = [[96.55746466]], Testing Loss = [[98.82787223]]\n",
      "Epoch # 48, Training Loss = [[96.15896084]], Testing Loss = [[98.47394498]]\n",
      "Epoch # 49, Training Loss = [[95.76218335]], Testing Loss = [[98.12141308]]\n",
      "Epoch # 50, Training Loss = [[95.3671236]], Testing Loss = [[97.77027031]]\n",
      "Epoch # 51, Training Loss = [[94.97377304]], Testing Loss = [[97.42051046]]\n",
      "Epoch # 52, Training Loss = [[94.58212319]], Testing Loss = [[97.07212738]]\n",
      "Epoch # 53, Training Loss = [[94.19216562]], Testing Loss = [[96.72511496]]\n",
      "Epoch # 54, Training Loss = [[93.80389198]], Testing Loss = [[96.37946711]]\n",
      "Epoch # 55, Training Loss = [[93.41729397]], Testing Loss = [[96.03517782]]\n",
      "Epoch # 56, Training Loss = [[93.03236332]], Testing Loss = [[95.69224107]]\n",
      "Epoch # 57, Training Loss = [[92.64909186]], Testing Loss = [[95.35065091]]\n",
      "Epoch # 58, Training Loss = [[92.26747146]], Testing Loss = [[95.01040143]]\n",
      "Epoch # 59, Training Loss = [[91.88749403]], Testing Loss = [[94.67148674]]\n",
      "Epoch # 60, Training Loss = [[91.50915157]], Testing Loss = [[94.33390099]]\n",
      "Epoch # 61, Training Loss = [[91.13243609]], Testing Loss = [[93.99763838]]\n",
      "Epoch # 62, Training Loss = [[90.7573397]], Testing Loss = [[93.66269314]]\n",
      "Epoch # 63, Training Loss = [[90.38385453]], Testing Loss = [[93.32905953]]\n",
      "Epoch # 64, Training Loss = [[90.01197277]], Testing Loss = [[92.99673185]]\n",
      "Epoch # 65, Training Loss = [[89.64168669]], Testing Loss = [[92.66570443]]\n",
      "Epoch # 66, Training Loss = [[89.27298857]], Testing Loss = [[92.33597165]]\n",
      "Epoch # 67, Training Loss = [[88.90587076]], Testing Loss = [[92.0075279]]\n",
      "Epoch # 68, Training Loss = [[88.54032568]], Testing Loss = [[91.68036763]]\n",
      "Epoch # 69, Training Loss = [[88.17634576]], Testing Loss = [[91.3544853]]\n",
      "Epoch # 70, Training Loss = [[87.81392351]], Testing Loss = [[91.02987542]]\n",
      "Epoch # 71, Training Loss = [[87.45305149]], Testing Loss = [[90.70653252]]\n",
      "Epoch # 72, Training Loss = [[87.09372229]], Testing Loss = [[90.38445117]]\n",
      "Epoch # 73, Training Loss = [[86.73592855]], Testing Loss = [[90.06362598]]\n",
      "Epoch # 74, Training Loss = [[86.37966297]], Testing Loss = [[89.74405157]]\n",
      "Epoch # 75, Training Loss = [[86.0249183]], Testing Loss = [[89.42572262]]\n",
      "Epoch # 76, Training Loss = [[85.67168732]], Testing Loss = [[89.1086338]]\n",
      "Epoch # 77, Training Loss = [[85.31996286]], Testing Loss = [[88.79277985]]\n",
      "Epoch # 78, Training Loss = [[84.96973781]], Testing Loss = [[88.47815552]]\n",
      "Epoch # 79, Training Loss = [[84.62100509]], Testing Loss = [[88.1647556]]\n",
      "Epoch # 80, Training Loss = [[84.27375767]], Testing Loss = [[87.8525749]]\n",
      "Epoch # 81, Training Loss = [[83.92798857]], Testing Loss = [[87.54160827]]\n",
      "Epoch # 82, Training Loss = [[83.58369083]], Testing Loss = [[87.23185057]]\n",
      "Epoch # 83, Training Loss = [[83.24085757]], Testing Loss = [[86.92329671]]\n",
      "Epoch # 84, Training Loss = [[82.89948192]], Testing Loss = [[86.61594163]]\n",
      "Epoch # 85, Training Loss = [[82.55955707]], Testing Loss = [[86.30978026]]\n",
      "Epoch # 86, Training Loss = [[82.22107626]], Testing Loss = [[86.00480761]]\n",
      "Epoch # 87, Training Loss = [[81.88403274]], Testing Loss = [[85.70101869]]\n",
      "Epoch # 88, Training Loss = [[81.54841983]], Testing Loss = [[85.39840853]]\n",
      "Epoch # 89, Training Loss = [[81.21423089]], Testing Loss = [[85.09697221]]\n",
      "Epoch # 90, Training Loss = [[80.8814593]], Testing Loss = [[84.79670481]]\n",
      "Epoch # 91, Training Loss = [[80.5500985]], Testing Loss = [[84.49760146]]\n",
      "Epoch # 92, Training Loss = [[80.22014195]], Testing Loss = [[84.1996573]]\n",
      "Epoch # 93, Training Loss = [[79.89158317]], Testing Loss = [[83.9028675]]\n",
      "Epoch # 94, Training Loss = [[79.56441571]], Testing Loss = [[83.60722728]]\n",
      "Epoch # 95, Training Loss = [[79.23863315]], Testing Loss = [[83.31273183]]\n",
      "Epoch # 96, Training Loss = [[78.91422913]], Testing Loss = [[83.01937643]]\n",
      "Epoch # 97, Training Loss = [[78.5911973]], Testing Loss = [[82.72715633]]\n",
      "Epoch # 98, Training Loss = [[78.26953137]], Testing Loss = [[82.43606684]]\n",
      "Epoch # 99, Training Loss = [[77.94922507]], Testing Loss = [[82.14610328]]\n",
      "Epoch # 100, Training Loss = [[77.63027218]], Testing Loss = [[81.85726099]]\n",
      "Epoch # 101, Training Loss = [[77.31266652]], Testing Loss = [[81.56953535]]\n",
      "Epoch # 102, Training Loss = [[76.99640192]], Testing Loss = [[81.28292175]]\n",
      "Epoch # 103, Training Loss = [[76.68147227]], Testing Loss = [[80.99741561]]\n",
      "Epoch # 104, Training Loss = [[76.36787149]], Testing Loss = [[80.71301237]]\n",
      "Epoch # 105, Training Loss = [[76.05559353]], Testing Loss = [[80.4297075]]\n",
      "Epoch # 106, Training Loss = [[75.74463238]], Testing Loss = [[80.14749648]]\n",
      "Epoch # 107, Training Loss = [[75.43498207]], Testing Loss = [[79.86637481]]\n",
      "Epoch # 108, Training Loss = [[75.12663664]], Testing Loss = [[79.58633805]]\n",
      "Epoch # 109, Training Loss = [[74.81959019]], Testing Loss = [[79.30738173]]\n",
      "Epoch # 110, Training Loss = [[74.51383684]], Testing Loss = [[79.02950143]]\n",
      "Epoch # 111, Training Loss = [[74.20937074]], Testing Loss = [[78.75269276]]\n",
      "Epoch # 112, Training Loss = [[73.9061861]], Testing Loss = [[78.47695134]]\n",
      "Epoch # 113, Training Loss = [[73.60427712]], Testing Loss = [[78.2022728]]\n",
      "Epoch # 114, Training Loss = [[73.30363806]], Testing Loss = [[77.92865281]]\n",
      "Epoch # 115, Training Loss = [[73.00426322]], Testing Loss = [[77.65608706]]\n",
      "Epoch # 116, Training Loss = [[72.7061469]], Testing Loss = [[77.38457124]]\n",
      "Epoch # 117, Training Loss = [[72.40928346]], Testing Loss = [[77.11410109]]\n",
      "Epoch # 118, Training Loss = [[72.11366727]], Testing Loss = [[76.84467235]]\n",
      "Epoch # 119, Training Loss = [[71.81929276]], Testing Loss = [[76.57628079]]\n",
      "Epoch # 120, Training Loss = [[71.52615435]], Testing Loss = [[76.3089222]]\n",
      "Epoch # 121, Training Loss = [[71.23424653]], Testing Loss = [[76.04259239]]\n",
      "Epoch # 122, Training Loss = [[70.94356379]], Testing Loss = [[75.77728717]]\n",
      "Epoch # 123, Training Loss = [[70.65410067]], Testing Loss = [[75.51300241]]\n",
      "Epoch # 124, Training Loss = [[70.36585173]], Testing Loss = [[75.24973397]]\n",
      "Epoch # 125, Training Loss = [[70.07881156]], Testing Loss = [[74.98747772]]\n",
      "Epoch # 126, Training Loss = [[69.79297478]], Testing Loss = [[74.72622959]]\n",
      "Epoch # 127, Training Loss = [[69.50833605]], Testing Loss = [[74.46598549]]\n",
      "Epoch # 128, Training Loss = [[69.22489003]], Testing Loss = [[74.20674137]]\n",
      "Epoch # 129, Training Loss = [[68.94263144]], Testing Loss = [[73.9484932]]\n",
      "Epoch # 130, Training Loss = [[68.66155502]], Testing Loss = [[73.69123695]]\n",
      "Epoch # 131, Training Loss = [[68.38165551]], Testing Loss = [[73.43496862]]\n",
      "Epoch # 132, Training Loss = [[68.10292772]], Testing Loss = [[73.17968423]]\n",
      "Epoch # 133, Training Loss = [[67.82536647]], Testing Loss = [[72.92537983]]\n",
      "Epoch # 134, Training Loss = [[67.54896659]], Testing Loss = [[72.67205146]]\n",
      "Epoch # 135, Training Loss = [[67.27372296]], Testing Loss = [[72.4196952]]\n",
      "Epoch # 136, Training Loss = [[66.99963048]], Testing Loss = [[72.16830714]]\n",
      "Epoch # 137, Training Loss = [[66.72668408]], Testing Loss = [[71.91788339]]\n",
      "Epoch # 138, Training Loss = [[66.45487871]], Testing Loss = [[71.66842008]]\n",
      "Epoch # 139, Training Loss = [[66.18420934]], Testing Loss = [[71.41991335]]\n",
      "Epoch # 140, Training Loss = [[65.91467099]], Testing Loss = [[71.17235936]]\n",
      "Epoch # 141, Training Loss = [[65.64625867]], Testing Loss = [[70.92575429]]\n",
      "Epoch # 142, Training Loss = [[65.37896746]], Testing Loss = [[70.68009434]]\n",
      "Epoch # 143, Training Loss = [[65.11279242]], Testing Loss = [[70.43537572]]\n",
      "Epoch # 144, Training Loss = [[64.84772867]], Testing Loss = [[70.19159466]]\n",
      "Epoch # 145, Training Loss = [[64.58377134]], Testing Loss = [[69.94874741]]\n",
      "Epoch # 146, Training Loss = [[64.32091559]], Testing Loss = [[69.70683024]]\n",
      "Epoch # 147, Training Loss = [[64.05915659]], Testing Loss = [[69.46583941]]\n",
      "Epoch # 148, Training Loss = [[63.79848956]], Testing Loss = [[69.22577123]]\n",
      "Epoch # 149, Training Loss = [[63.53890972]], Testing Loss = [[68.98662202]]\n",
      "Epoch # 150, Training Loss = [[63.28041233]], Testing Loss = [[68.7483881]]\n",
      "Epoch # 151, Training Loss = [[63.02299267]], Testing Loss = [[68.51106581]]\n",
      "Epoch # 152, Training Loss = [[62.76664603]], Testing Loss = [[68.27465152]]\n",
      "Epoch # 153, Training Loss = [[62.51136775]], Testing Loss = [[68.03914161]]\n",
      "Epoch # 154, Training Loss = [[62.25715318]], Testing Loss = [[67.80453246]]\n",
      "Epoch # 155, Training Loss = [[62.00399768]], Testing Loss = [[67.57082049]]\n",
      "Epoch # 156, Training Loss = [[61.75189666]], Testing Loss = [[67.33800213]]\n",
      "Epoch # 157, Training Loss = [[61.50084552]], Testing Loss = [[67.1060738]]\n",
      "Epoch # 158, Training Loss = [[61.25083972]], Testing Loss = [[66.87503198]]\n",
      "Epoch # 159, Training Loss = [[61.00187472]], Testing Loss = [[66.64487312]]\n",
      "Epoch # 160, Training Loss = [[60.753946]], Testing Loss = [[66.41559372]]\n",
      "Epoch # 161, Training Loss = [[60.50704908]], Testing Loss = [[66.18719027]]\n",
      "Epoch # 162, Training Loss = [[60.26117948]], Testing Loss = [[65.95965929]]\n",
      "Epoch # 163, Training Loss = [[60.01633275]], Testing Loss = [[65.73299732]]\n",
      "Epoch # 164, Training Loss = [[59.77250448]], Testing Loss = [[65.50720089]]\n",
      "Epoch # 165, Training Loss = [[59.52969026]], Testing Loss = [[65.28226658]]\n",
      "Epoch # 166, Training Loss = [[59.2878857]], Testing Loss = [[65.05819095]]\n",
      "Epoch # 167, Training Loss = [[59.04708645]], Testing Loss = [[64.83497059]]\n",
      "Epoch # 168, Training Loss = [[58.80728816]], Testing Loss = [[64.61260212]]\n",
      "Epoch # 169, Training Loss = [[58.56848653]], Testing Loss = [[64.39108214]]\n",
      "Epoch # 170, Training Loss = [[58.33067725]], Testing Loss = [[64.1704073]]\n",
      "Epoch # 171, Training Loss = [[58.09385604]], Testing Loss = [[63.95057423]]\n",
      "Epoch # 172, Training Loss = [[57.85801865]], Testing Loss = [[63.73157961]]\n",
      "Epoch # 173, Training Loss = [[57.62316085]], Testing Loss = [[63.5134201]]\n",
      "Epoch # 174, Training Loss = [[57.38927842]], Testing Loss = [[63.29609239]]\n",
      "Epoch # 175, Training Loss = [[57.15636716]], Testing Loss = [[63.0795932]]\n",
      "Epoch # 176, Training Loss = [[56.92442291]], Testing Loss = [[62.86391924]]\n",
      "Epoch # 177, Training Loss = [[56.6934415]], Testing Loss = [[62.64906723]]\n",
      "Epoch # 178, Training Loss = [[56.46341881]], Testing Loss = [[62.43503392]]\n",
      "Epoch # 179, Training Loss = [[56.23435071]], Testing Loss = [[62.22181608]]\n",
      "Epoch # 180, Training Loss = [[56.00623312]], Testing Loss = [[62.00941047]]\n",
      "Epoch # 181, Training Loss = [[55.77906196]], Testing Loss = [[61.79781389]]\n",
      "Epoch # 182, Training Loss = [[55.55283317]], Testing Loss = [[61.58702312]]\n",
      "Epoch # 183, Training Loss = [[55.32754271]], Testing Loss = [[61.37703497]]\n",
      "Epoch # 184, Training Loss = [[55.10318657]], Testing Loss = [[61.16784629]]\n",
      "Epoch # 185, Training Loss = [[54.87976075]], Testing Loss = [[60.9594539]]\n",
      "Epoch # 186, Training Loss = [[54.65726127]], Testing Loss = [[60.75185466]]\n",
      "Epoch # 187, Training Loss = [[54.43568416]], Testing Loss = [[60.54504542]]\n",
      "Epoch # 188, Training Loss = [[54.21502549]], Testing Loss = [[60.33902308]]\n",
      "Epoch # 189, Training Loss = [[53.99528133]], Testing Loss = [[60.13378451]]\n",
      "Epoch # 190, Training Loss = [[53.77644778]], Testing Loss = [[59.92932663]]\n",
      "Epoch # 191, Training Loss = [[53.55852095]], Testing Loss = [[59.72564634]]\n",
      "Epoch # 192, Training Loss = [[53.34149696]], Testing Loss = [[59.52274058]]\n",
      "Epoch # 193, Training Loss = [[53.12537198]], Testing Loss = [[59.32060629]]\n",
      "Epoch # 194, Training Loss = [[52.91014216]], Testing Loss = [[59.11924042]]\n",
      "Epoch # 195, Training Loss = [[52.69580369]], Testing Loss = [[58.91863994]]\n",
      "Epoch # 196, Training Loss = [[52.48235278]], Testing Loss = [[58.71880182]]\n",
      "Epoch # 197, Training Loss = [[52.26978563]], Testing Loss = [[58.51972307]]\n",
      "Epoch # 198, Training Loss = [[52.0580985]], Testing Loss = [[58.32140067]]\n",
      "Epoch # 199, Training Loss = [[51.84728764]], Testing Loss = [[58.12383166]]\n",
      "Epoch # 200, Training Loss = [[51.63734932]], Testing Loss = [[57.92701305]]\n",
      "Epoch # 201, Training Loss = [[51.42827982]], Testing Loss = [[57.73094188]]\n",
      "Epoch # 202, Training Loss = [[51.22007546]], Testing Loss = [[57.53561521]]\n",
      "Epoch # 203, Training Loss = [[51.01273256]], Testing Loss = [[57.3410301]]\n",
      "Epoch # 204, Training Loss = [[50.80624747]], Testing Loss = [[57.14718363]]\n",
      "Epoch # 205, Training Loss = [[50.60061653]], Testing Loss = [[56.95407288]]\n",
      "Epoch # 206, Training Loss = [[50.39583612]], Testing Loss = [[56.76169496]]\n",
      "Epoch # 207, Training Loss = [[50.19190264]], Testing Loss = [[56.57004698]]\n",
      "Epoch # 208, Training Loss = [[49.98881249]], Testing Loss = [[56.37912605]]\n",
      "Epoch # 209, Training Loss = [[49.78656209]], Testing Loss = [[56.18892931]]\n",
      "Epoch # 210, Training Loss = [[49.58514789]], Testing Loss = [[55.99945392]]\n",
      "Epoch # 211, Training Loss = [[49.38456634]], Testing Loss = [[55.81069702]]\n",
      "Epoch # 212, Training Loss = [[49.18481392]], Testing Loss = [[55.62265579]]\n",
      "Epoch # 213, Training Loss = [[48.98588711]], Testing Loss = [[55.43532741]]\n",
      "Epoch # 214, Training Loss = [[48.78778242]], Testing Loss = [[55.24870906]]\n",
      "Epoch # 215, Training Loss = [[48.59049637]], Testing Loss = [[55.06279796]]\n",
      "Epoch # 216, Training Loss = [[48.3940255]], Testing Loss = [[54.87759132]]\n",
      "Epoch # 217, Training Loss = [[48.19836635]], Testing Loss = [[54.69308635]]\n",
      "Epoch # 218, Training Loss = [[48.00351549]], Testing Loss = [[54.50928031]]\n",
      "Epoch # 219, Training Loss = [[47.80946952]], Testing Loss = [[54.32617044]]\n",
      "Epoch # 220, Training Loss = [[47.61622502]], Testing Loss = [[54.14375399]]\n",
      "Epoch # 221, Training Loss = [[47.42377861]], Testing Loss = [[53.96202824]]\n",
      "Epoch # 222, Training Loss = [[47.23212692]], Testing Loss = [[53.78099047]]\n",
      "Epoch # 223, Training Loss = [[47.0412666]], Testing Loss = [[53.60063797]]\n",
      "Epoch # 224, Training Loss = [[46.8511943]], Testing Loss = [[53.42096804]]\n",
      "Epoch # 225, Training Loss = [[46.6619067]], Testing Loss = [[53.241978]]\n",
      "Epoch # 226, Training Loss = [[46.47340049]], Testing Loss = [[53.06366517]]\n",
      "Epoch # 227, Training Loss = [[46.28567237]], Testing Loss = [[52.88602688]]\n",
      "Epoch # 228, Training Loss = [[46.09871905]], Testing Loss = [[52.70906049]]\n",
      "Epoch # 229, Training Loss = [[45.91253729]], Testing Loss = [[52.53276334]]\n",
      "Epoch # 230, Training Loss = [[45.72712382]], Testing Loss = [[52.35713281]]\n",
      "Epoch # 231, Training Loss = [[45.5424754]], Testing Loss = [[52.18216627]]\n",
      "Epoch # 232, Training Loss = [[45.35858881]], Testing Loss = [[52.0078611]]\n",
      "Epoch # 233, Training Loss = [[45.17546085]], Testing Loss = [[51.83421472]]\n",
      "Epoch # 234, Training Loss = [[44.99308833]], Testing Loss = [[51.66122453]]\n",
      "Epoch # 235, Training Loss = [[44.81146805]], Testing Loss = [[51.48888793]]\n",
      "Epoch # 236, Training Loss = [[44.63059685]], Testing Loss = [[51.31720238]]\n",
      "Epoch # 237, Training Loss = [[44.45047159]], Testing Loss = [[51.14616529]]\n",
      "Epoch # 238, Training Loss = [[44.27108913]], Testing Loss = [[50.97577413]]\n",
      "Epoch # 239, Training Loss = [[44.09244633]], Testing Loss = [[50.80602636]]\n",
      "Epoch # 240, Training Loss = [[43.9145401]], Testing Loss = [[50.63691943]]\n",
      "Epoch # 241, Training Loss = [[43.73736734]], Testing Loss = [[50.46845084]]\n",
      "Epoch # 242, Training Loss = [[43.56092496]], Testing Loss = [[50.30061807]]\n",
      "Epoch # 243, Training Loss = [[43.38520989]], Testing Loss = [[50.13341863]]\n",
      "Epoch # 244, Training Loss = [[43.21021908]], Testing Loss = [[49.96685002]]\n",
      "Epoch # 245, Training Loss = [[43.03594949]], Testing Loss = [[49.80090976]]\n",
      "Epoch # 246, Training Loss = [[42.86239808]], Testing Loss = [[49.63559538]]\n",
      "Epoch # 247, Training Loss = [[42.68956186]], Testing Loss = [[49.47090442]]\n",
      "Epoch # 248, Training Loss = [[42.5174378]], Testing Loss = [[49.30683443]]\n",
      "Epoch # 249, Training Loss = [[42.34602293]], Testing Loss = [[49.14338297]]\n",
      "Epoch # 250, Training Loss = [[42.17531426]], Testing Loss = [[48.98054761]]\n",
      "Epoch # 251, Training Loss = [[42.00530884]], Testing Loss = [[48.81832592]]\n",
      "Epoch # 252, Training Loss = [[41.83600372]], Testing Loss = [[48.6567155]]\n",
      "Epoch # 253, Training Loss = [[41.66739595]], Testing Loss = [[48.49571393]]\n",
      "Epoch # 254, Training Loss = [[41.49948262]], Testing Loss = [[48.33531883]]\n",
      "Epoch # 255, Training Loss = [[41.33226082]], Testing Loss = [[48.17552782]]\n",
      "Epoch # 256, Training Loss = [[41.16572765]], Testing Loss = [[48.01633851]]\n",
      "Epoch # 257, Training Loss = [[40.99988022]], Testing Loss = [[47.85774855]]\n",
      "Epoch # 258, Training Loss = [[40.83471565]], Testing Loss = [[47.69975558]]\n",
      "Epoch # 259, Training Loss = [[40.6702311]], Testing Loss = [[47.54235725]]\n",
      "Epoch # 260, Training Loss = [[40.50642371]], Testing Loss = [[47.38555122]]\n",
      "Epoch # 261, Training Loss = [[40.34329064]], Testing Loss = [[47.22933517]]\n",
      "Epoch # 262, Training Loss = [[40.18082908]], Testing Loss = [[47.07370678]]\n",
      "Epoch # 263, Training Loss = [[40.01903621]], Testing Loss = [[46.91866374]]\n",
      "Epoch # 264, Training Loss = [[39.85790923]], Testing Loss = [[46.76420374]]\n",
      "Epoch # 265, Training Loss = [[39.69744536]], Testing Loss = [[46.61032451]]\n",
      "Epoch # 266, Training Loss = [[39.53764183]], Testing Loss = [[46.45702375]]\n",
      "Epoch # 267, Training Loss = [[39.37849587]], Testing Loss = [[46.30429919]]\n",
      "Epoch # 268, Training Loss = [[39.22000472]], Testing Loss = [[46.15214857]]\n",
      "Epoch # 269, Training Loss = [[39.06216567]], Testing Loss = [[46.00056963]]\n",
      "Epoch # 270, Training Loss = [[38.90497597]], Testing Loss = [[45.84956012]]\n",
      "Epoch # 271, Training Loss = [[38.74843291]], Testing Loss = [[45.69911782]]\n",
      "Epoch # 272, Training Loss = [[38.5925338]], Testing Loss = [[45.54924049]]\n",
      "Epoch # 273, Training Loss = [[38.43727593]], Testing Loss = [[45.3999259]]\n",
      "Epoch # 274, Training Loss = [[38.28265664]], Testing Loss = [[45.25117186]]\n",
      "Epoch # 275, Training Loss = [[38.12867325]], Testing Loss = [[45.10297615]]\n",
      "Epoch # 276, Training Loss = [[37.9753231]], Testing Loss = [[44.95533659]]\n",
      "Epoch # 277, Training Loss = [[37.82260356]], Testing Loss = [[44.80825098]]\n",
      "Epoch # 278, Training Loss = [[37.67051198]], Testing Loss = [[44.66171716]]\n",
      "Epoch # 279, Training Loss = [[37.51904575]], Testing Loss = [[44.51573295]]\n",
      "Epoch # 280, Training Loss = [[37.36820226]], Testing Loss = [[44.37029619]]\n",
      "Epoch # 281, Training Loss = [[37.2179789]], Testing Loss = [[44.22540473]]\n",
      "Epoch # 282, Training Loss = [[37.06837309]], Testing Loss = [[44.08105644]]\n",
      "Epoch # 283, Training Loss = [[36.91938226]], Testing Loss = [[43.93724918]]\n",
      "Epoch # 284, Training Loss = [[36.77100383]], Testing Loss = [[43.79398081]]\n",
      "Epoch # 285, Training Loss = [[36.62323525]], Testing Loss = [[43.65124923]]\n",
      "Epoch # 286, Training Loss = [[36.47607398]], Testing Loss = [[43.50905233]]\n",
      "Epoch # 287, Training Loss = [[36.32951748]], Testing Loss = [[43.367388]]\n",
      "Epoch # 288, Training Loss = [[36.18356323]], Testing Loss = [[43.22625416]]\n",
      "Epoch # 289, Training Loss = [[36.03820873]], Testing Loss = [[43.08564871]]\n",
      "Epoch # 290, Training Loss = [[35.89345146]], Testing Loss = [[42.94556959]]\n",
      "Epoch # 291, Training Loss = [[35.74928895]], Testing Loss = [[42.80601473]]\n",
      "Epoch # 292, Training Loss = [[35.60571871]], Testing Loss = [[42.66698206]]\n",
      "Epoch # 293, Training Loss = [[35.46273827]], Testing Loss = [[42.52846954]]\n",
      "Epoch # 294, Training Loss = [[35.32034518]], Testing Loss = [[42.39047512]]\n",
      "Epoch # 295, Training Loss = [[35.17853698]], Testing Loss = [[42.25299676]]\n",
      "Epoch # 296, Training Loss = [[35.03731125]], Testing Loss = [[42.11603245]]\n",
      "Epoch # 297, Training Loss = [[34.89666556]], Testing Loss = [[41.97958016]]\n",
      "Epoch # 298, Training Loss = [[34.75659748]], Testing Loss = [[41.84363787]]\n",
      "Epoch # 299, Training Loss = [[34.61710462]], Testing Loss = [[41.7082036]]\n",
      "Epoch # 300, Training Loss = [[34.47818457]], Testing Loss = [[41.57327533]]\n",
      "Epoch # 301, Training Loss = [[34.33983496]], Testing Loss = [[41.43885109]]\n",
      "Epoch # 302, Training Loss = [[34.20205341]], Testing Loss = [[41.30492889]]\n",
      "Epoch # 303, Training Loss = [[34.06483755]], Testing Loss = [[41.17150676]]\n",
      "Epoch # 304, Training Loss = [[33.92818503]], Testing Loss = [[41.03858274]]\n",
      "Epoch # 305, Training Loss = [[33.79209351]], Testing Loss = [[40.90615487]]\n",
      "Epoch # 306, Training Loss = [[33.65656064]], Testing Loss = [[40.77422121]]\n",
      "Epoch # 307, Training Loss = [[33.52158411]], Testing Loss = [[40.6427798]]\n",
      "Epoch # 308, Training Loss = [[33.38716161]], Testing Loss = [[40.51182873]]\n",
      "Epoch # 309, Training Loss = [[33.25329081]], Testing Loss = [[40.38136605]]\n",
      "Epoch # 310, Training Loss = [[33.11996944]], Testing Loss = [[40.25138987]]\n",
      "Epoch # 311, Training Loss = [[32.98719521]], Testing Loss = [[40.12189825]]\n",
      "Epoch # 312, Training Loss = [[32.85496583]], Testing Loss = [[39.99288931]]\n",
      "Epoch # 313, Training Loss = [[32.72327905]], Testing Loss = [[39.86436114]]\n",
      "Epoch # 314, Training Loss = [[32.5921326]], Testing Loss = [[39.73631186]]\n",
      "Epoch # 315, Training Loss = [[32.46152425]], Testing Loss = [[39.60873958]]\n",
      "Epoch # 316, Training Loss = [[32.33145175]], Testing Loss = [[39.48164244]]\n",
      "Epoch # 317, Training Loss = [[32.20191287]], Testing Loss = [[39.35501856]]\n",
      "Epoch # 318, Training Loss = [[32.07290541]], Testing Loss = [[39.2288661]]\n",
      "Epoch # 319, Training Loss = [[31.94442714]], Testing Loss = [[39.10318319]]\n",
      "Epoch # 320, Training Loss = [[31.81647588]], Testing Loss = [[38.97796799]]\n",
      "Epoch # 321, Training Loss = [[31.68904942]], Testing Loss = [[38.85321867]]\n",
      "Epoch # 322, Training Loss = [[31.5621456]], Testing Loss = [[38.7289334]]\n",
      "Epoch # 323, Training Loss = [[31.43576223]], Testing Loss = [[38.60511036]]\n",
      "Epoch # 324, Training Loss = [[31.30989716]], Testing Loss = [[38.48174772]]\n",
      "Epoch # 325, Training Loss = [[31.18454823]], Testing Loss = [[38.35884369]]\n",
      "Epoch # 326, Training Loss = [[31.05971331]], Testing Loss = [[38.23639646]]\n",
      "Epoch # 327, Training Loss = [[30.93539024]], Testing Loss = [[38.11440424]]\n",
      "Epoch # 328, Training Loss = [[30.81157692]], Testing Loss = [[37.99286524]]\n",
      "Epoch # 329, Training Loss = [[30.68827121]], Testing Loss = [[37.87177768]]\n",
      "Epoch # 330, Training Loss = [[30.56547102]], Testing Loss = [[37.7511398]]\n",
      "Epoch # 331, Training Loss = [[30.44317425]], Testing Loss = [[37.63094981]]\n",
      "Epoch # 332, Training Loss = [[30.3213788]], Testing Loss = [[37.51120598]]\n",
      "Epoch # 333, Training Loss = [[30.20008259]], Testing Loss = [[37.39190653]]\n",
      "Epoch # 334, Training Loss = [[30.07928355]], Testing Loss = [[37.27304974]]\n",
      "Epoch # 335, Training Loss = [[29.95897962]], Testing Loss = [[37.15463386]]\n",
      "Epoch # 336, Training Loss = [[29.83916875]], Testing Loss = [[37.03665716]]\n",
      "Epoch # 337, Training Loss = [[29.71984888]], Testing Loss = [[36.91911791]]\n",
      "Epoch # 338, Training Loss = [[29.60101798]], Testing Loss = [[36.80201441]]\n",
      "Epoch # 339, Training Loss = [[29.48267402]], Testing Loss = [[36.68534493]]\n",
      "Epoch # 340, Training Loss = [[29.36481498]], Testing Loss = [[36.56910778]]\n",
      "Epoch # 341, Training Loss = [[29.24743885]], Testing Loss = [[36.45330126]]\n",
      "Epoch # 342, Training Loss = [[29.13054362]], Testing Loss = [[36.33792368]]\n",
      "Epoch # 343, Training Loss = [[29.0141273]], Testing Loss = [[36.22297335]]\n",
      "Epoch # 344, Training Loss = [[28.89818791]], Testing Loss = [[36.10844859]]\n",
      "Epoch # 345, Training Loss = [[28.78272346]], Testing Loss = [[35.99434775]]\n",
      "Epoch # 346, Training Loss = [[28.66773198]], Testing Loss = [[35.88066914]]\n",
      "Epoch # 347, Training Loss = [[28.55321152]], Testing Loss = [[35.76741113]]\n",
      "Epoch # 348, Training Loss = [[28.43916012]], Testing Loss = [[35.65457204]]\n",
      "Epoch # 349, Training Loss = [[28.32557583]], Testing Loss = [[35.54215025]]\n",
      "Epoch # 350, Training Loss = [[28.21245673]], Testing Loss = [[35.43014411]]\n",
      "Epoch # 351, Training Loss = [[28.09980087]], Testing Loss = [[35.318552]]\n",
      "Epoch # 352, Training Loss = [[27.98760634]], Testing Loss = [[35.20737228]]\n",
      "Epoch # 353, Training Loss = [[27.87587123]], Testing Loss = [[35.09660333]]\n",
      "Epoch # 354, Training Loss = [[27.76459363]], Testing Loss = [[34.98624356]]\n",
      "Epoch # 355, Training Loss = [[27.65377165]], Testing Loss = [[34.87629135]]\n",
      "Epoch # 356, Training Loss = [[27.5434034]], Testing Loss = [[34.76674509]]\n",
      "Epoch # 357, Training Loss = [[27.43348699]], Testing Loss = [[34.65760321]]\n",
      "Epoch # 358, Training Loss = [[27.32402056]], Testing Loss = [[34.54886411]]\n",
      "Epoch # 359, Training Loss = [[27.21500224]], Testing Loss = [[34.44052621]]\n",
      "Epoch # 360, Training Loss = [[27.10643018]], Testing Loss = [[34.33258794]]\n",
      "Epoch # 361, Training Loss = [[26.99830252]], Testing Loss = [[34.22504773]]\n",
      "Epoch # 362, Training Loss = [[26.89061742]], Testing Loss = [[34.11790402]]\n",
      "Epoch # 363, Training Loss = [[26.78337305]], Testing Loss = [[34.01115525]]\n",
      "Epoch # 364, Training Loss = [[26.67656759]], Testing Loss = [[33.90479988]]\n",
      "Epoch # 365, Training Loss = [[26.57019921]], Testing Loss = [[33.79883635]]\n",
      "Epoch # 366, Training Loss = [[26.46426611]], Testing Loss = [[33.69326314]]\n",
      "Epoch # 367, Training Loss = [[26.35876648]], Testing Loss = [[33.58807871]]\n",
      "Epoch # 368, Training Loss = [[26.25369853]], Testing Loss = [[33.48328154]]\n",
      "Epoch # 369, Training Loss = [[26.14906047]], Testing Loss = [[33.3788701]]\n",
      "Epoch # 370, Training Loss = [[26.04485052]], Testing Loss = [[33.27484289]]\n",
      "Epoch # 371, Training Loss = [[25.94106691]], Testing Loss = [[33.1711984]]\n",
      "Epoch # 372, Training Loss = [[25.83770787]], Testing Loss = [[33.06793513]]\n",
      "Epoch # 373, Training Loss = [[25.73477164]], Testing Loss = [[32.96505158]]\n",
      "Epoch # 374, Training Loss = [[25.63225648]], Testing Loss = [[32.86254626]]\n",
      "Epoch # 375, Training Loss = [[25.53016063]], Testing Loss = [[32.76041769]]\n",
      "Epoch # 376, Training Loss = [[25.42848237]], Testing Loss = [[32.65866439]]\n",
      "Epoch # 377, Training Loss = [[25.32721997]], Testing Loss = [[32.5572849]]\n",
      "Epoch # 378, Training Loss = [[25.2263717]], Testing Loss = [[32.45627774]]\n",
      "Epoch # 379, Training Loss = [[25.12593585]], Testing Loss = [[32.35564145]]\n",
      "Epoch # 380, Training Loss = [[25.02591071]], Testing Loss = [[32.25537459]]\n",
      "Epoch # 381, Training Loss = [[24.92629459]], Testing Loss = [[32.1554757]]\n",
      "Epoch # 382, Training Loss = [[24.82708578]], Testing Loss = [[32.05594334]]\n",
      "Epoch # 383, Training Loss = [[24.72828262]], Testing Loss = [[31.95677607]]\n",
      "Epoch # 384, Training Loss = [[24.62988341]], Testing Loss = [[31.85797247]]\n",
      "Epoch # 385, Training Loss = [[24.53188648]], Testing Loss = [[31.7595311]]\n",
      "Epoch # 386, Training Loss = [[24.43429017]], Testing Loss = [[31.66145055]]\n",
      "Epoch # 387, Training Loss = [[24.33709283]], Testing Loss = [[31.5637294]]\n",
      "Epoch # 388, Training Loss = [[24.2402928]], Testing Loss = [[31.46636625]]\n",
      "Epoch # 389, Training Loss = [[24.14388844]], Testing Loss = [[31.36935969]]\n",
      "Epoch # 390, Training Loss = [[24.04787811]], Testing Loss = [[31.27270832]]\n",
      "Epoch # 391, Training Loss = [[23.95226018]], Testing Loss = [[31.17641075]]\n",
      "Epoch # 392, Training Loss = [[23.85703303]], Testing Loss = [[31.0804656]]\n",
      "Epoch # 393, Training Loss = [[23.76219505]], Testing Loss = [[30.98487148]]\n",
      "Epoch # 394, Training Loss = [[23.66774461]], Testing Loss = [[30.88962702]]\n",
      "Epoch # 395, Training Loss = [[23.57368013]], Testing Loss = [[30.79473085]]\n",
      "Epoch # 396, Training Loss = [[23.48]], Testing Loss = [[30.7001816]]\n",
      "Epoch # 397, Training Loss = [[23.38670264]], Testing Loss = [[30.60597792]]\n",
      "Epoch # 398, Training Loss = [[23.29378646]], Testing Loss = [[30.51211845]]\n",
      "Epoch # 399, Training Loss = [[23.20124988]], Testing Loss = [[30.41860184]]\n",
      "Epoch # 400, Training Loss = [[23.10909134]], Testing Loss = [[30.32542675]]\n",
      "Epoch # 401, Training Loss = [[23.01730927]], Testing Loss = [[30.23259185]]\n",
      "Epoch # 402, Training Loss = [[22.92590212]], Testing Loss = [[30.14009579]]\n",
      "Epoch # 403, Training Loss = [[22.83486834]], Testing Loss = [[30.04793726]]\n",
      "Epoch # 404, Training Loss = [[22.74420637]], Testing Loss = [[29.95611492]]\n",
      "Epoch # 405, Training Loss = [[22.6539147]], Testing Loss = [[29.86462747]]\n",
      "Epoch # 406, Training Loss = [[22.56399178]], Testing Loss = [[29.7734736]]\n",
      "Epoch # 407, Training Loss = [[22.47443609]], Testing Loss = [[29.68265199]]\n",
      "Epoch # 408, Training Loss = [[22.38524611]], Testing Loss = [[29.59216135]]\n",
      "Epoch # 409, Training Loss = [[22.29642034]], Testing Loss = [[29.50200037]]\n",
      "Epoch # 410, Training Loss = [[22.20795726]], Testing Loss = [[29.41216778]]\n",
      "Epoch # 411, Training Loss = [[22.11985538]], Testing Loss = [[29.32266228]]\n",
      "Epoch # 412, Training Loss = [[22.0321132]], Testing Loss = [[29.23348259]]\n",
      "Epoch # 413, Training Loss = [[21.94472924]], Testing Loss = [[29.14462744]]\n",
      "Epoch # 414, Training Loss = [[21.85770202]], Testing Loss = [[29.05609556]]\n",
      "Epoch # 415, Training Loss = [[21.77103006]], Testing Loss = [[28.96788568]]\n",
      "Epoch # 416, Training Loss = [[21.6847119]], Testing Loss = [[28.87999654]]\n",
      "Epoch # 417, Training Loss = [[21.59874607]], Testing Loss = [[28.79242689]]\n",
      "Epoch # 418, Training Loss = [[21.51313113]], Testing Loss = [[28.70517548]]\n",
      "Epoch # 419, Training Loss = [[21.42786561]], Testing Loss = [[28.61824106]]\n",
      "Epoch # 420, Training Loss = [[21.34294807]], Testing Loss = [[28.53162239]]\n",
      "Epoch # 421, Training Loss = [[21.25837709]], Testing Loss = [[28.44531824]]\n",
      "Epoch # 422, Training Loss = [[21.17415121]], Testing Loss = [[28.35932737]]\n",
      "Epoch # 423, Training Loss = [[21.09026904]], Testing Loss = [[28.27364856]]\n",
      "Epoch # 424, Training Loss = [[21.00672913]], Testing Loss = [[28.18828059]]\n",
      "Epoch # 425, Training Loss = [[20.92353007]], Testing Loss = [[28.10322225]]\n",
      "Epoch # 426, Training Loss = [[20.84067047]], Testing Loss = [[28.01847231]]\n",
      "Epoch # 427, Training Loss = [[20.75814892]], Testing Loss = [[27.93402959]]\n",
      "Epoch # 428, Training Loss = [[20.67596401]], Testing Loss = [[27.84989287]]\n",
      "Epoch # 429, Training Loss = [[20.59411437]], Testing Loss = [[27.76606096]]\n",
      "Epoch # 430, Training Loss = [[20.5125986]], Testing Loss = [[27.68253266]]\n",
      "Epoch # 431, Training Loss = [[20.43141533]], Testing Loss = [[27.5993068]]\n",
      "Epoch # 432, Training Loss = [[20.35056319]], Testing Loss = [[27.51638218]]\n",
      "Epoch # 433, Training Loss = [[20.27004081]], Testing Loss = [[27.43375763]]\n",
      "Epoch # 434, Training Loss = [[20.18984682]], Testing Loss = [[27.35143198]]\n",
      "Epoch # 435, Training Loss = [[20.10997987]], Testing Loss = [[27.26940406]]\n",
      "Epoch # 436, Training Loss = [[20.03043861]], Testing Loss = [[27.1876727]]\n",
      "Epoch # 437, Training Loss = [[19.95122169]], Testing Loss = [[27.10623675]]\n",
      "Epoch # 438, Training Loss = [[19.87232778]], Testing Loss = [[27.02509505]]\n",
      "Epoch # 439, Training Loss = [[19.79375555]], Testing Loss = [[26.94424645]]\n",
      "Epoch # 440, Training Loss = [[19.71550366]], Testing Loss = [[26.86368981]]\n",
      "Epoch # 441, Training Loss = [[19.63757079]], Testing Loss = [[26.78342398]]\n",
      "Epoch # 442, Training Loss = [[19.55995563]], Testing Loss = [[26.70344783]]\n",
      "Epoch # 443, Training Loss = [[19.48265686]], Testing Loss = [[26.62376023]]\n",
      "Epoch # 444, Training Loss = [[19.40567318]], Testing Loss = [[26.54436006]]\n",
      "Epoch # 445, Training Loss = [[19.32900329]], Testing Loss = [[26.46524618]]\n",
      "Epoch # 446, Training Loss = [[19.25264589]], Testing Loss = [[26.38641748]]\n",
      "Epoch # 447, Training Loss = [[19.1765997]], Testing Loss = [[26.30787285]]\n",
      "Epoch # 448, Training Loss = [[19.10086342]], Testing Loss = [[26.22961117]]\n",
      "Epoch # 449, Training Loss = [[19.02543579]], Testing Loss = [[26.15163135]]\n",
      "Epoch # 450, Training Loss = [[18.95031552]], Testing Loss = [[26.07393228]]\n",
      "Epoch # 451, Training Loss = [[18.87550135]], Testing Loss = [[25.99651286]]\n",
      "Epoch # 452, Training Loss = [[18.80099201]], Testing Loss = [[25.91937202]]\n",
      "Epoch # 453, Training Loss = [[18.72678626]], Testing Loss = [[25.84250864]]\n",
      "Epoch # 454, Training Loss = [[18.65288282]], Testing Loss = [[25.76592167]]\n",
      "Epoch # 455, Training Loss = [[18.57928047]], Testing Loss = [[25.68961]]\n",
      "Epoch # 456, Training Loss = [[18.50597795]], Testing Loss = [[25.61357258]]\n",
      "Epoch # 457, Training Loss = [[18.43297403]], Testing Loss = [[25.53780833]]\n",
      "Epoch # 458, Training Loss = [[18.36026748]], Testing Loss = [[25.46231619]]\n",
      "Epoch # 459, Training Loss = [[18.28785706]], Testing Loss = [[25.38709509]]\n",
      "Epoch # 460, Training Loss = [[18.21574157]], Testing Loss = [[25.31214398]]\n",
      "Epoch # 461, Training Loss = [[18.14391978]], Testing Loss = [[25.2374618]]\n",
      "Epoch # 462, Training Loss = [[18.07239047]], Testing Loss = [[25.1630475]]\n",
      "Epoch # 463, Training Loss = [[18.00115245]], Testing Loss = [[25.08890005]]\n",
      "Epoch # 464, Training Loss = [[17.93020452]], Testing Loss = [[25.01501839]]\n",
      "Epoch # 465, Training Loss = [[17.85954547]], Testing Loss = [[24.9414015]]\n",
      "Epoch # 466, Training Loss = [[17.78917412]], Testing Loss = [[24.86804833]]\n",
      "Epoch # 467, Training Loss = [[17.71908927]], Testing Loss = [[24.79495787]]\n",
      "Epoch # 468, Training Loss = [[17.64928975]], Testing Loss = [[24.72212909]]\n",
      "Epoch # 469, Training Loss = [[17.57977438]], Testing Loss = [[24.64956097]]\n",
      "Epoch # 470, Training Loss = [[17.51054199]], Testing Loss = [[24.57725249]]\n",
      "Epoch # 471, Training Loss = [[17.44159141]], Testing Loss = [[24.50520264]]\n",
      "Epoch # 472, Training Loss = [[17.37292148]], Testing Loss = [[24.43341041]]\n",
      "Epoch # 473, Training Loss = [[17.30453105]], Testing Loss = [[24.36187481]]\n",
      "Epoch # 474, Training Loss = [[17.23641895]], Testing Loss = [[24.29059482]]\n",
      "Epoch # 475, Training Loss = [[17.16858405]], Testing Loss = [[24.21956945]]\n",
      "Epoch # 476, Training Loss = [[17.10102519]], Testing Loss = [[24.14879772]]\n",
      "Epoch # 477, Training Loss = [[17.03374125]], Testing Loss = [[24.07827863]]\n",
      "Epoch # 478, Training Loss = [[16.96673109]], Testing Loss = [[24.00801121]]\n",
      "Epoch # 479, Training Loss = [[16.89999357]], Testing Loss = [[23.93799446]]\n",
      "Epoch # 480, Training Loss = [[16.83352759]], Testing Loss = [[23.86822741]]\n",
      "Epoch # 481, Training Loss = [[16.76733201]], Testing Loss = [[23.7987091]]\n",
      "Epoch # 482, Training Loss = [[16.70140572]], Testing Loss = [[23.72943855]]\n",
      "Epoch # 483, Training Loss = [[16.63574761]], Testing Loss = [[23.6604148]]\n",
      "Epoch # 484, Training Loss = [[16.57035658]], Testing Loss = [[23.59163689]]\n",
      "Epoch # 485, Training Loss = [[16.50523152]], Testing Loss = [[23.52310387]]\n",
      "Epoch # 486, Training Loss = [[16.44037135]], Testing Loss = [[23.45481477]]\n",
      "Epoch # 487, Training Loss = [[16.37577496]], Testing Loss = [[23.38676865]]\n",
      "Epoch # 488, Training Loss = [[16.31144127]], Testing Loss = [[23.31896456]]\n",
      "Epoch # 489, Training Loss = [[16.2473692]], Testing Loss = [[23.25140157]]\n",
      "Epoch # 490, Training Loss = [[16.18355768]], Testing Loss = [[23.18407873]]\n",
      "Epoch # 491, Training Loss = [[16.12000561]], Testing Loss = [[23.11699512]]\n",
      "Epoch # 492, Training Loss = [[16.05671195]], Testing Loss = [[23.0501498]]\n",
      "Epoch # 493, Training Loss = [[15.99367562]], Testing Loss = [[22.98354184]]\n",
      "Epoch # 494, Training Loss = [[15.93089556]], Testing Loss = [[22.91717032]]\n",
      "Epoch # 495, Training Loss = [[15.86837072]], Testing Loss = [[22.85103432]]\n",
      "Epoch # 496, Training Loss = [[15.80610004]], Testing Loss = [[22.78513293]]\n",
      "Epoch # 497, Training Loss = [[15.74408248]], Testing Loss = [[22.71946524]]\n",
      "Epoch # 498, Training Loss = [[15.68231699]], Testing Loss = [[22.65403033]]\n",
      "Epoch # 499, Training Loss = [[15.62080255]], Testing Loss = [[22.58882731]]\n",
      "Epoch # 500, Training Loss = [[15.5595381]], Testing Loss = [[22.52385526]]\n",
      "Epoch # 501, Training Loss = [[15.49852263]], Testing Loss = [[22.4591133]]\n",
      "Epoch # 502, Training Loss = [[15.4377551]], Testing Loss = [[22.39460053]]\n",
      "Epoch # 503, Training Loss = [[15.3772345]], Testing Loss = [[22.33031606]]\n",
      "Epoch # 504, Training Loss = [[15.31695981]], Testing Loss = [[22.266259]]\n",
      "Epoch # 505, Training Loss = [[15.25693001]], Testing Loss = [[22.20242847]]\n",
      "Epoch # 506, Training Loss = [[15.1971441]], Testing Loss = [[22.13882359]]\n",
      "Epoch # 507, Training Loss = [[15.13760107]], Testing Loss = [[22.07544349]]\n",
      "Epoch # 508, Training Loss = [[15.07829993]], Testing Loss = [[22.01228729]]\n",
      "Epoch # 509, Training Loss = [[15.01923967]], Testing Loss = [[21.94935412]]\n",
      "Epoch # 510, Training Loss = [[14.9604193]], Testing Loss = [[21.88664312]]\n",
      "Epoch # 511, Training Loss = [[14.90183784]], Testing Loss = [[21.82415342]]\n",
      "Epoch # 512, Training Loss = [[14.84349431]], Testing Loss = [[21.76188417]]\n",
      "Epoch # 513, Training Loss = [[14.78538771]], Testing Loss = [[21.69983451]]\n",
      "Epoch # 514, Training Loss = [[14.72751709]], Testing Loss = [[21.63800359]]\n",
      "Epoch # 515, Training Loss = [[14.66988146]], Testing Loss = [[21.57639056]]\n",
      "Epoch # 516, Training Loss = [[14.61247985]], Testing Loss = [[21.51499457]]\n",
      "Epoch # 517, Training Loss = [[14.55531131]], Testing Loss = [[21.45381479]]\n",
      "Epoch # 518, Training Loss = [[14.49837488]], Testing Loss = [[21.39285038]]\n",
      "Epoch # 519, Training Loss = [[14.44166959]], Testing Loss = [[21.33210049]]\n",
      "Epoch # 520, Training Loss = [[14.3851945]], Testing Loss = [[21.27156431]]\n",
      "Epoch # 521, Training Loss = [[14.32894866]], Testing Loss = [[21.21124099]]\n",
      "Epoch # 522, Training Loss = [[14.27293113]], Testing Loss = [[21.15112972]]\n",
      "Epoch # 523, Training Loss = [[14.21714096]], Testing Loss = [[21.09122968]]\n",
      "Epoch # 524, Training Loss = [[14.16157722]], Testing Loss = [[21.03154004]]\n",
      "Epoch # 525, Training Loss = [[14.10623898]], Testing Loss = [[20.97205999]]\n",
      "Epoch # 526, Training Loss = [[14.05112531]], Testing Loss = [[20.91278872]]\n",
      "Epoch # 527, Training Loss = [[13.99623529]], Testing Loss = [[20.85372542]]\n",
      "Epoch # 528, Training Loss = [[13.94156799]], Testing Loss = [[20.79486928]]\n",
      "Epoch # 529, Training Loss = [[13.8871225]], Testing Loss = [[20.73621951]]\n",
      "Epoch # 530, Training Loss = [[13.8328979]], Testing Loss = [[20.6777753]]\n",
      "Epoch # 531, Training Loss = [[13.7788933]], Testing Loss = [[20.61953585]]\n",
      "Epoch # 532, Training Loss = [[13.72510777]], Testing Loss = [[20.56150039]]\n",
      "Epoch # 533, Training Loss = [[13.67154042]], Testing Loss = [[20.5036681]]\n",
      "Epoch # 534, Training Loss = [[13.61819035]], Testing Loss = [[20.44603822]]\n",
      "Epoch # 535, Training Loss = [[13.56505667]], Testing Loss = [[20.38860996]]\n",
      "Epoch # 536, Training Loss = [[13.51213849]], Testing Loss = [[20.33138253]]\n",
      "Epoch # 537, Training Loss = [[13.45943492]], Testing Loss = [[20.27435516]]\n",
      "Epoch # 538, Training Loss = [[13.40694508]], Testing Loss = [[20.21752707]]\n",
      "Epoch # 539, Training Loss = [[13.35466808]], Testing Loss = [[20.16089751]]\n",
      "Epoch # 540, Training Loss = [[13.30260306]], Testing Loss = [[20.10446568]]\n",
      "Epoch # 541, Training Loss = [[13.25074914]], Testing Loss = [[20.04823085]]\n",
      "Epoch # 542, Training Loss = [[13.19910544]], Testing Loss = [[19.99219223]]\n",
      "Epoch # 543, Training Loss = [[13.14767112]], Testing Loss = [[19.93634908]]\n",
      "Epoch # 544, Training Loss = [[13.0964453]], Testing Loss = [[19.88070063]]\n",
      "Epoch # 545, Training Loss = [[13.04542713]], Testing Loss = [[19.82524614]]\n",
      "Epoch # 546, Training Loss = [[12.99461575]], Testing Loss = [[19.76998486]]\n",
      "Epoch # 547, Training Loss = [[12.94401032]], Testing Loss = [[19.71491604]]\n",
      "Epoch # 548, Training Loss = [[12.89360998]], Testing Loss = [[19.66003893]]\n",
      "Epoch # 549, Training Loss = [[12.84341389]], Testing Loss = [[19.6053528]]\n",
      "Epoch # 550, Training Loss = [[12.79342122]], Testing Loss = [[19.55085691]]\n",
      "Epoch # 551, Training Loss = [[12.74363112]], Testing Loss = [[19.49655052]]\n",
      "Epoch # 552, Training Loss = [[12.69404276]], Testing Loss = [[19.44243291]]\n",
      "Epoch # 553, Training Loss = [[12.64465532]], Testing Loss = [[19.38850334]]\n",
      "Epoch # 554, Training Loss = [[12.59546796]], Testing Loss = [[19.33476109]]\n",
      "Epoch # 555, Training Loss = [[12.54647987]], Testing Loss = [[19.28120543]]\n",
      "Epoch # 556, Training Loss = [[12.49769022]], Testing Loss = [[19.22783565]]\n",
      "Epoch # 557, Training Loss = [[12.4490982]], Testing Loss = [[19.17465103]]\n",
      "Epoch # 558, Training Loss = [[12.40070299]], Testing Loss = [[19.12165085]]\n",
      "Epoch # 559, Training Loss = [[12.35250379]], Testing Loss = [[19.06883441]]\n",
      "Epoch # 560, Training Loss = [[12.30449979]], Testing Loss = [[19.01620099]]\n",
      "Epoch # 561, Training Loss = [[12.25669019]], Testing Loss = [[18.96374989]]\n",
      "Epoch # 562, Training Loss = [[12.20907419]], Testing Loss = [[18.91148041]]\n",
      "Epoch # 563, Training Loss = [[12.16165099]], Testing Loss = [[18.85939185]]\n",
      "Epoch # 564, Training Loss = [[12.11441979]], Testing Loss = [[18.8074835]]\n",
      "Epoch # 565, Training Loss = [[12.06737982]], Testing Loss = [[18.75575468]]\n",
      "Epoch # 566, Training Loss = [[12.02053029]], Testing Loss = [[18.7042047]]\n",
      "Epoch # 567, Training Loss = [[11.9738704]], Testing Loss = [[18.65283285]]\n",
      "Epoch # 568, Training Loss = [[11.92739939]], Testing Loss = [[18.60163847]]\n",
      "Epoch # 569, Training Loss = [[11.88111647]], Testing Loss = [[18.55062086]]\n",
      "Epoch # 570, Training Loss = [[11.83502088]], Testing Loss = [[18.49977935]]\n",
      "Epoch # 571, Training Loss = [[11.78911183]], Testing Loss = [[18.44911325]]\n",
      "Epoch # 572, Training Loss = [[11.74338858]], Testing Loss = [[18.3986219]]\n",
      "Epoch # 573, Training Loss = [[11.69785034]], Testing Loss = [[18.34830461]]\n",
      "Epoch # 574, Training Loss = [[11.65249637]], Testing Loss = [[18.29816073]]\n",
      "Epoch # 575, Training Loss = [[11.6073259]], Testing Loss = [[18.24818958]]\n",
      "Epoch # 576, Training Loss = [[11.56233818]], Testing Loss = [[18.19839049]]\n",
      "Epoch # 577, Training Loss = [[11.51753246]], Testing Loss = [[18.14876281]]\n",
      "Epoch # 578, Training Loss = [[11.47290799]], Testing Loss = [[18.09930588]]\n",
      "Epoch # 579, Training Loss = [[11.42846403]], Testing Loss = [[18.05001904]]\n",
      "Epoch # 580, Training Loss = [[11.38419983]], Testing Loss = [[18.00090163]]\n",
      "Epoch # 581, Training Loss = [[11.34011466]], Testing Loss = [[17.95195302]]\n",
      "Epoch # 582, Training Loss = [[11.29620778]], Testing Loss = [[17.90317253]]\n",
      "Epoch # 583, Training Loss = [[11.25247846]], Testing Loss = [[17.85455954]]\n",
      "Epoch # 584, Training Loss = [[11.20892596]], Testing Loss = [[17.8061134]]\n",
      "Epoch # 585, Training Loss = [[11.16554957]], Testing Loss = [[17.75783346]]\n",
      "Epoch # 586, Training Loss = [[11.12234855]], Testing Loss = [[17.70971909]]\n",
      "Epoch # 587, Training Loss = [[11.0793222]], Testing Loss = [[17.66176966]]\n",
      "Epoch # 588, Training Loss = [[11.03646978]], Testing Loss = [[17.61398452]]\n",
      "Epoch # 589, Training Loss = [[10.99379059]], Testing Loss = [[17.56636305]]\n",
      "Epoch # 590, Training Loss = [[10.95128391]], Testing Loss = [[17.51890463]]\n",
      "Epoch # 591, Training Loss = [[10.90894904]], Testing Loss = [[17.47160861]]\n",
      "Epoch # 592, Training Loss = [[10.86678527]], Testing Loss = [[17.42447439]]\n",
      "Epoch # 593, Training Loss = [[10.8247919]], Testing Loss = [[17.37750134]]\n",
      "Epoch # 594, Training Loss = [[10.78296822]], Testing Loss = [[17.33068884]]\n",
      "Epoch # 595, Training Loss = [[10.74131354]], Testing Loss = [[17.28403628]]\n",
      "Epoch # 596, Training Loss = [[10.69982717]], Testing Loss = [[17.23754305]]\n",
      "Epoch # 597, Training Loss = [[10.65850842]], Testing Loss = [[17.19120852]]\n",
      "Epoch # 598, Training Loss = [[10.61735659]], Testing Loss = [[17.1450321]]\n",
      "Epoch # 599, Training Loss = [[10.576371]], Testing Loss = [[17.09901318]]\n",
      "Epoch # 600, Training Loss = [[10.53555097]], Testing Loss = [[17.05315116]]\n",
      "Epoch # 601, Training Loss = [[10.49489582]], Testing Loss = [[17.00744543]]\n",
      "Epoch # 602, Training Loss = [[10.45440487]], Testing Loss = [[16.9618954]]\n",
      "Epoch # 603, Training Loss = [[10.41407745]], Testing Loss = [[16.91650046]]\n",
      "Epoch # 604, Training Loss = [[10.37391288]], Testing Loss = [[16.87126003]]\n",
      "Epoch # 605, Training Loss = [[10.3339105]], Testing Loss = [[16.82617352]]\n",
      "Epoch # 606, Training Loss = [[10.29406965]], Testing Loss = [[16.78124033]]\n",
      "Epoch # 607, Training Loss = [[10.25438965]], Testing Loss = [[16.73645987]]\n",
      "Epoch # 608, Training Loss = [[10.21486984]], Testing Loss = [[16.69183157]]\n",
      "Epoch # 609, Training Loss = [[10.17550958]], Testing Loss = [[16.64735485]]\n",
      "Epoch # 610, Training Loss = [[10.13630821]], Testing Loss = [[16.60302911]]\n",
      "Epoch # 611, Training Loss = [[10.09726506]], Testing Loss = [[16.55885379]]\n",
      "Epoch # 612, Training Loss = [[10.0583795]], Testing Loss = [[16.51482831]]\n",
      "Epoch # 613, Training Loss = [[10.01965088]], Testing Loss = [[16.47095209]]\n",
      "Epoch # 614, Training Loss = [[9.98107855]], Testing Loss = [[16.42722457]]\n",
      "Epoch # 615, Training Loss = [[9.94266186]], Testing Loss = [[16.38364518]]\n",
      "Epoch # 616, Training Loss = [[9.9044002]], Testing Loss = [[16.34021335]]\n",
      "Epoch # 617, Training Loss = [[9.8662929]], Testing Loss = [[16.29692851]]\n",
      "Epoch # 618, Training Loss = [[9.82833935]], Testing Loss = [[16.25379012]]\n",
      "Epoch # 619, Training Loss = [[9.79053891]], Testing Loss = [[16.2107976]]\n",
      "Epoch # 620, Training Loss = [[9.75289096]], Testing Loss = [[16.16795039]]\n",
      "Epoch # 621, Training Loss = [[9.71539486]], Testing Loss = [[16.12524796]]\n",
      "Epoch # 622, Training Loss = [[9.67804999]], Testing Loss = [[16.08268974]]\n",
      "Epoch # 623, Training Loss = [[9.64085574]], Testing Loss = [[16.04027518]]\n",
      "Epoch # 624, Training Loss = [[9.60381148]], Testing Loss = [[15.99800373]]\n",
      "Epoch # 625, Training Loss = [[9.5669166]], Testing Loss = [[15.95587485]]\n",
      "Epoch # 626, Training Loss = [[9.5301705]], Testing Loss = [[15.913888]]\n",
      "Epoch # 627, Training Loss = [[9.49357254]], Testing Loss = [[15.87204263]]\n",
      "Epoch # 628, Training Loss = [[9.45712214]], Testing Loss = [[15.8303382]]\n",
      "Epoch # 629, Training Loss = [[9.42081868]], Testing Loss = [[15.78877417]]\n",
      "Epoch # 630, Training Loss = [[9.38466156]], Testing Loss = [[15.74735002]]\n",
      "Epoch # 631, Training Loss = [[9.34865018]], Testing Loss = [[15.70606521]]\n",
      "Epoch # 632, Training Loss = [[9.31278394]], Testing Loss = [[15.6649192]]\n",
      "Epoch # 633, Training Loss = [[9.27706225]], Testing Loss = [[15.62391147]]\n",
      "Epoch # 634, Training Loss = [[9.24148452]], Testing Loss = [[15.5830415]]\n",
      "Epoch # 635, Training Loss = [[9.20605014]], Testing Loss = [[15.54230875]]\n",
      "Epoch # 636, Training Loss = [[9.17075855]], Testing Loss = [[15.5017127]]\n",
      "Epoch # 637, Training Loss = [[9.13560914]], Testing Loss = [[15.46125284]]\n",
      "Epoch # 638, Training Loss = [[9.10060134]], Testing Loss = [[15.42092865]]\n",
      "Epoch # 639, Training Loss = [[9.06573457]], Testing Loss = [[15.3807396]]\n",
      "Epoch # 640, Training Loss = [[9.03100824]], Testing Loss = [[15.34068519]]\n",
      "Epoch # 641, Training Loss = [[8.99642179]], Testing Loss = [[15.3007649]]\n",
      "Epoch # 642, Training Loss = [[8.96197463]], Testing Loss = [[15.26097823]]\n",
      "Epoch # 643, Training Loss = [[8.92766621]], Testing Loss = [[15.22132466]]\n",
      "Epoch # 644, Training Loss = [[8.89349594]], Testing Loss = [[15.1818037]]\n",
      "Epoch # 645, Training Loss = [[8.85946326]], Testing Loss = [[15.14241482]]\n",
      "Epoch # 646, Training Loss = [[8.82556761]], Testing Loss = [[15.10315754]]\n",
      "Epoch # 647, Training Loss = [[8.79180842]], Testing Loss = [[15.06403136]]\n",
      "Epoch # 648, Training Loss = [[8.75818514]], Testing Loss = [[15.02503577]]\n",
      "Epoch # 649, Training Loss = [[8.72469721]], Testing Loss = [[14.98617028]]\n",
      "Epoch # 650, Training Loss = [[8.69134408]], Testing Loss = [[14.9474344]]\n",
      "Epoch # 651, Training Loss = [[8.65812518]], Testing Loss = [[14.90882763]]\n",
      "Epoch # 652, Training Loss = [[8.62503998]], Testing Loss = [[14.87034948]]\n",
      "Epoch # 653, Training Loss = [[8.59208792]], Testing Loss = [[14.83199947]]\n",
      "Epoch # 654, Training Loss = [[8.55926846]], Testing Loss = [[14.79377711]]\n",
      "Epoch # 655, Training Loss = [[8.52658105]], Testing Loss = [[14.75568191]]\n",
      "Epoch # 656, Training Loss = [[8.49402515]], Testing Loss = [[14.7177134]]\n",
      "Epoch # 657, Training Loss = [[8.46160023]], Testing Loss = [[14.67987109]]\n",
      "Epoch # 658, Training Loss = [[8.42930575]], Testing Loss = [[14.6421545]]\n",
      "Epoch # 659, Training Loss = [[8.39714116]], Testing Loss = [[14.60456316]]\n",
      "Epoch # 660, Training Loss = [[8.36510595]], Testing Loss = [[14.5670966]]\n",
      "Epoch # 661, Training Loss = [[8.33319958]], Testing Loss = [[14.52975434]]\n",
      "Epoch # 662, Training Loss = [[8.30142152]], Testing Loss = [[14.4925359]]\n",
      "Epoch # 663, Training Loss = [[8.26977124]], Testing Loss = [[14.45544083]]\n",
      "Epoch # 664, Training Loss = [[8.23824823]], Testing Loss = [[14.41846865]]\n",
      "Epoch # 665, Training Loss = [[8.20685195]], Testing Loss = [[14.3816189]]\n",
      "Epoch # 666, Training Loss = [[8.1755819]], Testing Loss = [[14.34489111]]\n",
      "Epoch # 667, Training Loss = [[8.14443755]], Testing Loss = [[14.30828483]]\n",
      "Epoch # 668, Training Loss = [[8.1134184]], Testing Loss = [[14.27179959]]\n",
      "Epoch # 669, Training Loss = [[8.08252392]], Testing Loss = [[14.23543494]]\n",
      "Epoch # 670, Training Loss = [[8.0517536]], Testing Loss = [[14.19919041]]\n",
      "Epoch # 671, Training Loss = [[8.02110694]], Testing Loss = [[14.16306557]]\n",
      "Epoch # 672, Training Loss = [[7.99058343]], Testing Loss = [[14.12705994]]\n",
      "Epoch # 673, Training Loss = [[7.96018257]], Testing Loss = [[14.09117309]]\n",
      "Epoch # 674, Training Loss = [[7.92990385]], Testing Loss = [[14.05540457]]\n",
      "Epoch # 675, Training Loss = [[7.89974678]], Testing Loss = [[14.01975392]]\n",
      "Epoch # 676, Training Loss = [[7.86971085]], Testing Loss = [[13.9842207]]\n",
      "Epoch # 677, Training Loss = [[7.83979557]], Testing Loss = [[13.94880447]]\n",
      "Epoch # 678, Training Loss = [[7.81000044]], Testing Loss = [[13.91350479]]\n",
      "Epoch # 679, Training Loss = [[7.78032498]], Testing Loss = [[13.87832122]]\n",
      "Epoch # 680, Training Loss = [[7.75076869]], Testing Loss = [[13.84325331]]\n",
      "Epoch # 681, Training Loss = [[7.72133108]], Testing Loss = [[13.80830064]]\n",
      "Epoch # 682, Training Loss = [[7.69201167]], Testing Loss = [[13.77346277]]\n",
      "Epoch # 683, Training Loss = [[7.66280998]], Testing Loss = [[13.73873926]]\n",
      "Epoch # 684, Training Loss = [[7.63372552]], Testing Loss = [[13.70412968]]\n",
      "Epoch # 685, Training Loss = [[7.60475781]], Testing Loss = [[13.66963361]]\n",
      "Epoch # 686, Training Loss = [[7.57590637]], Testing Loss = [[13.63525062]]\n",
      "Epoch # 687, Training Loss = [[7.54717074]], Testing Loss = [[13.60098027]]\n",
      "Epoch # 688, Training Loss = [[7.51855043]], Testing Loss = [[13.56682216]]\n",
      "Epoch # 689, Training Loss = [[7.49004497]], Testing Loss = [[13.53277584]]\n",
      "Epoch # 690, Training Loss = [[7.46165389]], Testing Loss = [[13.49884091]]\n",
      "Epoch # 691, Training Loss = [[7.43337673]], Testing Loss = [[13.46501694]]\n",
      "Epoch # 692, Training Loss = [[7.40521301]], Testing Loss = [[13.43130352]]\n",
      "Epoch # 693, Training Loss = [[7.37716228]], Testing Loss = [[13.39770022]]\n",
      "Epoch # 694, Training Loss = [[7.34922406]], Testing Loss = [[13.36420665]]\n",
      "Epoch # 695, Training Loss = [[7.32139791]], Testing Loss = [[13.33082237]]\n",
      "Epoch # 696, Training Loss = [[7.29368336]], Testing Loss = [[13.29754699]]\n",
      "Epoch # 697, Training Loss = [[7.26607995]], Testing Loss = [[13.26438008]]\n",
      "Epoch # 698, Training Loss = [[7.23858724]], Testing Loss = [[13.23132126]]\n",
      "Epoch # 699, Training Loss = [[7.21120476]], Testing Loss = [[13.1983701]]\n",
      "Epoch # 700, Training Loss = [[7.18393206]], Testing Loss = [[13.1655262]]\n",
      "Epoch # 701, Training Loss = [[7.1567687]], Testing Loss = [[13.13278917]]\n",
      "Epoch # 702, Training Loss = [[7.12971423]], Testing Loss = [[13.10015859]]\n",
      "Epoch # 703, Training Loss = [[7.10276821]], Testing Loss = [[13.06763408]]\n",
      "Epoch # 704, Training Loss = [[7.07593018]], Testing Loss = [[13.03521523]]\n",
      "Epoch # 705, Training Loss = [[7.04919971]], Testing Loss = [[13.00290164]]\n",
      "Epoch # 706, Training Loss = [[7.02257636]], Testing Loss = [[12.97069292]]\n",
      "Epoch # 707, Training Loss = [[6.99605969]], Testing Loss = [[12.93858868]]\n",
      "Epoch # 708, Training Loss = [[6.96964926]], Testing Loss = [[12.90658853]]\n",
      "Epoch # 709, Training Loss = [[6.94334464]], Testing Loss = [[12.87469207]]\n",
      "Epoch # 710, Training Loss = [[6.9171454]], Testing Loss = [[12.84289891]]\n",
      "Epoch # 711, Training Loss = [[6.8910511]], Testing Loss = [[12.81120868]]\n",
      "Epoch # 712, Training Loss = [[6.86506131]], Testing Loss = [[12.77962097]]\n",
      "Epoch # 713, Training Loss = [[6.83917562]], Testing Loss = [[12.74813542]]\n",
      "Epoch # 714, Training Loss = [[6.81339358]], Testing Loss = [[12.71675164]]\n",
      "Epoch # 715, Training Loss = [[6.78771479]], Testing Loss = [[12.68546924]]\n",
      "Epoch # 716, Training Loss = [[6.76213881]], Testing Loss = [[12.65428784]]\n",
      "Epoch # 717, Training Loss = [[6.73666522]], Testing Loss = [[12.62320708]]\n",
      "Epoch # 718, Training Loss = [[6.71129361]], Testing Loss = [[12.59222657]]\n",
      "Epoch # 719, Training Loss = [[6.68602356]], Testing Loss = [[12.56134593]]\n",
      "Epoch # 720, Training Loss = [[6.66085466]], Testing Loss = [[12.5305648]]\n",
      "Epoch # 721, Training Loss = [[6.63578648]], Testing Loss = [[12.4998828]]\n",
      "Epoch # 722, Training Loss = [[6.61081863]], Testing Loss = [[12.46929956]]\n",
      "Epoch # 723, Training Loss = [[6.58595068]], Testing Loss = [[12.43881471]]\n",
      "Epoch # 724, Training Loss = [[6.56118223]], Testing Loss = [[12.40842788]]\n",
      "Epoch # 725, Training Loss = [[6.53651288]], Testing Loss = [[12.37813872]]\n",
      "Epoch # 726, Training Loss = [[6.51194221]], Testing Loss = [[12.34794684]]\n",
      "Epoch # 727, Training Loss = [[6.48746983]], Testing Loss = [[12.3178519]]\n",
      "Epoch # 728, Training Loss = [[6.46309533]], Testing Loss = [[12.28785352]]\n",
      "Epoch # 729, Training Loss = [[6.43881832]], Testing Loss = [[12.25795135]]\n",
      "Epoch # 730, Training Loss = [[6.41463839]], Testing Loss = [[12.22814504]]\n",
      "Epoch # 731, Training Loss = [[6.39055514]], Testing Loss = [[12.19843421]]\n",
      "Epoch # 732, Training Loss = [[6.36656819]], Testing Loss = [[12.16881852]]\n",
      "Epoch # 733, Training Loss = [[6.34267714]], Testing Loss = [[12.13929761]]\n",
      "Epoch # 734, Training Loss = [[6.3188816]], Testing Loss = [[12.10987112]]\n",
      "Epoch # 735, Training Loss = [[6.29518117]], Testing Loss = [[12.08053872]]\n",
      "Epoch # 736, Training Loss = [[6.27157547]], Testing Loss = [[12.05130003]]\n",
      "Epoch # 737, Training Loss = [[6.24806411]], Testing Loss = [[12.02215473]]\n",
      "Epoch # 738, Training Loss = [[6.2246467]], Testing Loss = [[11.99310245]]\n",
      "Epoch # 739, Training Loss = [[6.20132287]], Testing Loss = [[11.96414285]]\n",
      "Epoch # 740, Training Loss = [[6.17809223]], Testing Loss = [[11.93527559]]\n",
      "Epoch # 741, Training Loss = [[6.15495439]], Testing Loss = [[11.90650033]]\n",
      "Epoch # 742, Training Loss = [[6.13190898]], Testing Loss = [[11.87781672]]\n",
      "Epoch # 743, Training Loss = [[6.10895563]], Testing Loss = [[11.84922441]]\n",
      "Epoch # 744, Training Loss = [[6.08609395]], Testing Loss = [[11.82072308]]\n",
      "Epoch # 745, Training Loss = [[6.06332357]], Testing Loss = [[11.79231238]]\n",
      "Epoch # 746, Training Loss = [[6.04064412]], Testing Loss = [[11.76399198]]\n",
      "Epoch # 747, Training Loss = [[6.01805522]], Testing Loss = [[11.73576154]]\n",
      "Epoch # 748, Training Loss = [[5.99555651]], Testing Loss = [[11.70762073]]\n",
      "Epoch # 749, Training Loss = [[5.97314762]], Testing Loss = [[11.67956921]]\n",
      "Epoch # 750, Training Loss = [[5.95082819]], Testing Loss = [[11.65160665]]\n",
      "Epoch # 751, Training Loss = [[5.92859783]], Testing Loss = [[11.62373272]]\n",
      "Epoch # 752, Training Loss = [[5.9064562]], Testing Loss = [[11.5959471]]\n",
      "Epoch # 753, Training Loss = [[5.88440293]], Testing Loss = [[11.56824946]]\n",
      "Epoch # 754, Training Loss = [[5.86243766]], Testing Loss = [[11.54063946]]\n",
      "Epoch # 755, Training Loss = [[5.84056003]], Testing Loss = [[11.51311679]]\n",
      "Epoch # 756, Training Loss = [[5.81876968]], Testing Loss = [[11.48568112]]\n",
      "Epoch # 757, Training Loss = [[5.79706625]], Testing Loss = [[11.45833213]]\n",
      "Epoch # 758, Training Loss = [[5.7754494]], Testing Loss = [[11.43106949]]\n",
      "Epoch # 759, Training Loss = [[5.75391876]], Testing Loss = [[11.40389289]]\n",
      "Epoch # 760, Training Loss = [[5.73247399]], Testing Loss = [[11.37680202]]\n",
      "Epoch # 761, Training Loss = [[5.71111473]], Testing Loss = [[11.34979654]]\n",
      "Epoch # 762, Training Loss = [[5.68984064]], Testing Loss = [[11.32287615]]\n",
      "Epoch # 763, Training Loss = [[5.66865137]], Testing Loss = [[11.29604054]]\n",
      "Epoch # 764, Training Loss = [[5.64754657]], Testing Loss = [[11.26928938]]\n",
      "Epoch # 765, Training Loss = [[5.62652589]], Testing Loss = [[11.24262236]]\n",
      "Epoch # 766, Training Loss = [[5.605589]], Testing Loss = [[11.21603918]]\n",
      "Epoch # 767, Training Loss = [[5.58473555]], Testing Loss = [[11.18953952]]\n",
      "Epoch # 768, Training Loss = [[5.5639652]], Testing Loss = [[11.16312308]]\n",
      "Epoch # 769, Training Loss = [[5.54327761]], Testing Loss = [[11.13678955]]\n",
      "Epoch # 770, Training Loss = [[5.52267244]], Testing Loss = [[11.11053862]]\n",
      "Epoch # 771, Training Loss = [[5.50214937]], Testing Loss = [[11.08436999]]\n",
      "Epoch # 772, Training Loss = [[5.48170804]], Testing Loss = [[11.05828335]]\n",
      "Epoch # 773, Training Loss = [[5.46134814]], Testing Loss = [[11.0322784]]\n",
      "Epoch # 774, Training Loss = [[5.44106932]], Testing Loss = [[11.00635484]]\n",
      "Epoch # 775, Training Loss = [[5.42087126]], Testing Loss = [[10.98051237]]\n",
      "Epoch # 776, Training Loss = [[5.40075362]], Testing Loss = [[10.9547507]]\n",
      "Epoch # 777, Training Loss = [[5.38071609]], Testing Loss = [[10.92906951]]\n",
      "Epoch # 778, Training Loss = [[5.36075832]], Testing Loss = [[10.90346852]]\n",
      "Epoch # 779, Training Loss = [[5.34088]], Testing Loss = [[10.87794743]]\n",
      "Epoch # 780, Training Loss = [[5.32108081]], Testing Loss = [[10.85250595]]\n",
      "Epoch # 781, Training Loss = [[5.30136041]], Testing Loss = [[10.82714378]]\n",
      "Epoch # 782, Training Loss = [[5.28171849]], Testing Loss = [[10.80186064]]\n",
      "Epoch # 783, Training Loss = [[5.26215473]], Testing Loss = [[10.77665622]]\n",
      "Epoch # 784, Training Loss = [[5.24266881]], Testing Loss = [[10.75153025]]\n",
      "Epoch # 785, Training Loss = [[5.22326041]], Testing Loss = [[10.72648242]]\n",
      "Epoch # 786, Training Loss = [[5.20392921]], Testing Loss = [[10.70151247]]\n",
      "Epoch # 787, Training Loss = [[5.18467491]], Testing Loss = [[10.67662009]]\n",
      "Epoch # 788, Training Loss = [[5.16549718]], Testing Loss = [[10.65180501]]\n",
      "Epoch # 789, Training Loss = [[5.14639572]], Testing Loss = [[10.62706693]]\n",
      "Epoch # 790, Training Loss = [[5.12737021]], Testing Loss = [[10.60240559]]\n",
      "Epoch # 791, Training Loss = [[5.10842035]], Testing Loss = [[10.57782068]]\n",
      "Epoch # 792, Training Loss = [[5.08954582]], Testing Loss = [[10.55331195]]\n",
      "Epoch # 793, Training Loss = [[5.07074632]], Testing Loss = [[10.5288791]]\n",
      "Epoch # 794, Training Loss = [[5.05202155]], Testing Loss = [[10.50452186]]\n",
      "Epoch # 795, Training Loss = [[5.03337119]], Testing Loss = [[10.48023994]]\n",
      "Epoch # 796, Training Loss = [[5.01479495]], Testing Loss = [[10.45603308]]\n",
      "Epoch # 797, Training Loss = [[4.99629252]], Testing Loss = [[10.431901]]\n",
      "Epoch # 798, Training Loss = [[4.9778636]], Testing Loss = [[10.40784342]]\n",
      "Epoch # 799, Training Loss = [[4.9595079]], Testing Loss = [[10.38386008]]\n",
      "Epoch # 800, Training Loss = [[4.94122511]], Testing Loss = [[10.35995069]]\n",
      "Epoch # 801, Training Loss = [[4.92301493]], Testing Loss = [[10.336115]]\n",
      "Epoch # 802, Training Loss = [[4.90487708]], Testing Loss = [[10.31235272]]\n",
      "Epoch # 803, Training Loss = [[4.88681125]], Testing Loss = [[10.28866359]]\n",
      "Epoch # 804, Training Loss = [[4.86881716]], Testing Loss = [[10.26504735]]\n",
      "Epoch # 805, Training Loss = [[4.8508945]], Testing Loss = [[10.24150372]]\n",
      "Epoch # 806, Training Loss = [[4.833043]], Testing Loss = [[10.21803244]]\n",
      "Epoch # 807, Training Loss = [[4.81526235]], Testing Loss = [[10.19463325]]\n",
      "Epoch # 808, Training Loss = [[4.79755228]], Testing Loss = [[10.17130589]]\n",
      "Epoch # 809, Training Loss = [[4.77991249]], Testing Loss = [[10.14805008]]\n",
      "Epoch # 810, Training Loss = [[4.76234269]], Testing Loss = [[10.12486558]]\n",
      "Epoch # 811, Training Loss = [[4.74484261]], Testing Loss = [[10.10175211]]\n",
      "Epoch # 812, Training Loss = [[4.72741195]], Testing Loss = [[10.07870942]]\n",
      "Epoch # 813, Training Loss = [[4.71005044]], Testing Loss = [[10.05573726]]\n",
      "Epoch # 814, Training Loss = [[4.6927578]], Testing Loss = [[10.03283536]]\n",
      "Epoch # 815, Training Loss = [[4.67553373]], Testing Loss = [[10.01000346]]\n",
      "Epoch # 816, Training Loss = [[4.65837797]], Testing Loss = [[9.98724133]]\n",
      "Epoch # 817, Training Loss = [[4.64129024]], Testing Loss = [[9.96454869]]\n",
      "Epoch # 818, Training Loss = [[4.62427025]], Testing Loss = [[9.94192529]]\n",
      "Epoch # 819, Training Loss = [[4.60731774]], Testing Loss = [[9.91937089]]\n",
      "Epoch # 820, Training Loss = [[4.59043243]], Testing Loss = [[9.89688523]]\n",
      "Epoch # 821, Training Loss = [[4.57361403]], Testing Loss = [[9.87446807]]\n",
      "Epoch # 822, Training Loss = [[4.55686229]], Testing Loss = [[9.85211914]]\n",
      "Epoch # 823, Training Loss = [[4.54017694]], Testing Loss = [[9.82983822]]\n",
      "Epoch # 824, Training Loss = [[4.52355769]], Testing Loss = [[9.80762504]]\n",
      "Epoch # 825, Training Loss = [[4.50700428]], Testing Loss = [[9.78547936]]\n",
      "Epoch # 826, Training Loss = [[4.49051644]], Testing Loss = [[9.76340093]]\n",
      "Epoch # 827, Training Loss = [[4.47409392]], Testing Loss = [[9.74138952]]\n",
      "Epoch # 828, Training Loss = [[4.45773643]], Testing Loss = [[9.71944488]]\n",
      "Epoch # 829, Training Loss = [[4.44144371]], Testing Loss = [[9.69756676]]\n",
      "Epoch # 830, Training Loss = [[4.42521551]], Testing Loss = [[9.67575493]]\n",
      "Epoch # 831, Training Loss = [[4.40905156]], Testing Loss = [[9.65400914]]\n",
      "Epoch # 832, Training Loss = [[4.3929516]], Testing Loss = [[9.63232915]]\n",
      "Epoch # 833, Training Loss = [[4.37691536]], Testing Loss = [[9.61071473]]\n",
      "Epoch # 834, Training Loss = [[4.36094259]], Testing Loss = [[9.58916564]]\n",
      "Epoch # 835, Training Loss = [[4.34503304]], Testing Loss = [[9.56768164]]\n",
      "Epoch # 836, Training Loss = [[4.32918643]], Testing Loss = [[9.54626249]]\n",
      "Epoch # 837, Training Loss = [[4.31340252]], Testing Loss = [[9.52490797]]\n",
      "Epoch # 838, Training Loss = [[4.29768105]], Testing Loss = [[9.50361783]]\n",
      "Epoch # 839, Training Loss = [[4.28202177]], Testing Loss = [[9.48239184]]\n",
      "Epoch # 840, Training Loss = [[4.26642443]], Testing Loss = [[9.46122978]]\n",
      "Epoch # 841, Training Loss = [[4.25088877]], Testing Loss = [[9.4401314]]\n",
      "Epoch # 842, Training Loss = [[4.23541454]], Testing Loss = [[9.41909649]]\n",
      "Epoch # 843, Training Loss = [[4.22000149]], Testing Loss = [[9.3981248]]\n",
      "Epoch # 844, Training Loss = [[4.20464937]], Testing Loss = [[9.37721612]]\n",
      "Epoch # 845, Training Loss = [[4.18935794]], Testing Loss = [[9.35637021]]\n",
      "Epoch # 846, Training Loss = [[4.17412695]], Testing Loss = [[9.33558685]]\n",
      "Epoch # 847, Training Loss = [[4.15895615]], Testing Loss = [[9.31486581]]\n",
      "Epoch # 848, Training Loss = [[4.1438453]], Testing Loss = [[9.29420686]]\n",
      "Epoch # 849, Training Loss = [[4.12879415]], Testing Loss = [[9.27360979]]\n",
      "Epoch # 850, Training Loss = [[4.11380246]], Testing Loss = [[9.25307437]]\n",
      "Epoch # 851, Training Loss = [[4.09887]], Testing Loss = [[9.23260038]]\n",
      "Epoch # 852, Training Loss = [[4.08399651]], Testing Loss = [[9.21218759]]\n",
      "Epoch # 853, Training Loss = [[4.06918176]], Testing Loss = [[9.19183578]]\n",
      "Epoch # 854, Training Loss = [[4.05442551]], Testing Loss = [[9.17154474]]\n",
      "Epoch # 855, Training Loss = [[4.03972753]], Testing Loss = [[9.15131425]]\n",
      "Epoch # 856, Training Loss = [[4.02508757]], Testing Loss = [[9.13114409]]\n",
      "Epoch # 857, Training Loss = [[4.0105054]], Testing Loss = [[9.11103403]]\n",
      "Epoch # 858, Training Loss = [[3.99598078]], Testing Loss = [[9.09098387]]\n",
      "Epoch # 859, Training Loss = [[3.98151349]], Testing Loss = [[9.0709934]]\n",
      "Epoch # 860, Training Loss = [[3.96710328]], Testing Loss = [[9.05106238]]\n",
      "Epoch # 861, Training Loss = [[3.95274993]], Testing Loss = [[9.03119062]]\n",
      "Epoch # 862, Training Loss = [[3.9384532]], Testing Loss = [[9.0113779]]\n",
      "Epoch # 863, Training Loss = [[3.92421287]], Testing Loss = [[8.991624]]\n",
      "Epoch # 864, Training Loss = [[3.91002871]], Testing Loss = [[8.97192872]]\n",
      "Epoch # 865, Training Loss = [[3.89590048]], Testing Loss = [[8.95229184]]\n",
      "Epoch # 866, Training Loss = [[3.88182796]], Testing Loss = [[8.93271316]]\n",
      "Epoch # 867, Training Loss = [[3.86781093]], Testing Loss = [[8.91319246]]\n",
      "Epoch # 868, Training Loss = [[3.85384916]], Testing Loss = [[8.89372954]]\n",
      "Epoch # 869, Training Loss = [[3.83994242]], Testing Loss = [[8.87432419]]\n",
      "Epoch # 870, Training Loss = [[3.82609049]], Testing Loss = [[8.85497621]]\n",
      "Epoch # 871, Training Loss = [[3.81229315]], Testing Loss = [[8.83568539]]\n",
      "Epoch # 872, Training Loss = [[3.79855017]], Testing Loss = [[8.81645152]]\n",
      "Epoch # 873, Training Loss = [[3.78486134]], Testing Loss = [[8.79727441]]\n",
      "Epoch # 874, Training Loss = [[3.77122644]], Testing Loss = [[8.77815384]]\n",
      "Epoch # 875, Training Loss = [[3.75764524]], Testing Loss = [[8.75908961]]\n",
      "Epoch # 876, Training Loss = [[3.74411753]], Testing Loss = [[8.74008153]]\n",
      "Epoch # 877, Training Loss = [[3.73064309]], Testing Loss = [[8.72112939]]\n",
      "Epoch # 878, Training Loss = [[3.71722171]], Testing Loss = [[8.702233]]\n",
      "Epoch # 879, Training Loss = [[3.70385317]], Testing Loss = [[8.68339215]]\n",
      "Epoch # 880, Training Loss = [[3.69053725]], Testing Loss = [[8.66460665]]\n",
      "Epoch # 881, Training Loss = [[3.67727375]], Testing Loss = [[8.64587629]]\n",
      "Epoch # 882, Training Loss = [[3.66406244]], Testing Loss = [[8.62720088]]\n",
      "Epoch # 883, Training Loss = [[3.65090312]], Testing Loss = [[8.60858023]]\n",
      "Epoch # 884, Training Loss = [[3.63779557]], Testing Loss = [[8.59001414]]\n",
      "Epoch # 885, Training Loss = [[3.6247396]], Testing Loss = [[8.57150242]]\n",
      "Epoch # 886, Training Loss = [[3.61173497]], Testing Loss = [[8.55304486]]\n",
      "Epoch # 887, Training Loss = [[3.5987815]], Testing Loss = [[8.53464129]]\n",
      "Epoch # 888, Training Loss = [[3.58587897]], Testing Loss = [[8.5162915]]\n",
      "Epoch # 889, Training Loss = [[3.57302717]], Testing Loss = [[8.4979953]]\n",
      "Epoch # 890, Training Loss = [[3.56022589]], Testing Loss = [[8.47975251]]\n",
      "Epoch # 891, Training Loss = [[3.54747494]], Testing Loss = [[8.46156293]]\n",
      "Epoch # 892, Training Loss = [[3.53477411]], Testing Loss = [[8.44342638]]\n",
      "Epoch # 893, Training Loss = [[3.5221232]], Testing Loss = [[8.42534266]]\n",
      "Epoch # 894, Training Loss = [[3.509522]], Testing Loss = [[8.40731159]]\n",
      "Epoch # 895, Training Loss = [[3.49697031]], Testing Loss = [[8.38933298]]\n",
      "Epoch # 896, Training Loss = [[3.48446793]], Testing Loss = [[8.37140665]]\n",
      "Epoch # 897, Training Loss = [[3.47201466]], Testing Loss = [[8.35353241]]\n",
      "Epoch # 898, Training Loss = [[3.4596103]], Testing Loss = [[8.33571007]]\n",
      "Epoch # 899, Training Loss = [[3.44725465]], Testing Loss = [[8.31793944]]\n",
      "Epoch # 900, Training Loss = [[3.43494752]], Testing Loss = [[8.30022036]]\n",
      "Epoch # 901, Training Loss = [[3.42268871]], Testing Loss = [[8.28255263]]\n",
      "Epoch # 902, Training Loss = [[3.41047802]], Testing Loss = [[8.26493607]]\n",
      "Epoch # 903, Training Loss = [[3.39831526]], Testing Loss = [[8.2473705]]\n",
      "Epoch # 904, Training Loss = [[3.38620023]], Testing Loss = [[8.22985575]]\n",
      "Epoch # 905, Training Loss = [[3.37413274]], Testing Loss = [[8.21239162]]\n",
      "Epoch # 906, Training Loss = [[3.3621126]], Testing Loss = [[8.19497794]]\n",
      "Epoch # 907, Training Loss = [[3.35013961]], Testing Loss = [[8.17761454]]\n",
      "Epoch # 908, Training Loss = [[3.33821359]], Testing Loss = [[8.16030122]]\n",
      "Epoch # 909, Training Loss = [[3.32633434]], Testing Loss = [[8.14303783]]\n",
      "Epoch # 910, Training Loss = [[3.31450167]], Testing Loss = [[8.12582418]]\n",
      "Epoch # 911, Training Loss = [[3.3027154]], Testing Loss = [[8.10866009]]\n",
      "Epoch # 912, Training Loss = [[3.29097534]], Testing Loss = [[8.09154539]]\n",
      "Epoch # 913, Training Loss = [[3.2792813]], Testing Loss = [[8.07447991]]\n",
      "Epoch # 914, Training Loss = [[3.26763309]], Testing Loss = [[8.05746347]]\n",
      "Epoch # 915, Training Loss = [[3.25603052]], Testing Loss = [[8.0404959]]\n",
      "Epoch # 916, Training Loss = [[3.24447342]], Testing Loss = [[8.02357702]]\n",
      "Epoch # 917, Training Loss = [[3.2329616]], Testing Loss = [[8.00670667]]\n",
      "Epoch # 918, Training Loss = [[3.22149488]], Testing Loss = [[7.98988468]]\n",
      "Epoch # 919, Training Loss = [[3.21007306]], Testing Loss = [[7.97311086]]\n",
      "Epoch # 920, Training Loss = [[3.19869598]], Testing Loss = [[7.95638506]]\n",
      "Epoch # 921, Training Loss = [[3.18736344]], Testing Loss = [[7.93970711]]\n",
      "Epoch # 922, Training Loss = [[3.17607528]], Testing Loss = [[7.92307683]]\n",
      "Epoch # 923, Training Loss = [[3.1648313]], Testing Loss = [[7.90649406]]\n",
      "Epoch # 924, Training Loss = [[3.15363134]], Testing Loss = [[7.88995863]]\n",
      "Epoch # 925, Training Loss = [[3.1424752]], Testing Loss = [[7.87347038]]\n",
      "Epoch # 926, Training Loss = [[3.13136272]], Testing Loss = [[7.85702913]]\n",
      "Epoch # 927, Training Loss = [[3.12029372]], Testing Loss = [[7.84063474]]\n",
      "Epoch # 928, Training Loss = [[3.10926802]], Testing Loss = [[7.82428702]]\n",
      "Epoch # 929, Training Loss = [[3.09828545]], Testing Loss = [[7.80798581]]\n",
      "Epoch # 930, Training Loss = [[3.08734582]], Testing Loss = [[7.79173096]]\n",
      "Epoch # 931, Training Loss = [[3.07644898]], Testing Loss = [[7.7755223]]\n",
      "Epoch # 932, Training Loss = [[3.06559474]], Testing Loss = [[7.75935967]]\n",
      "Epoch # 933, Training Loss = [[3.05478293]], Testing Loss = [[7.7432429]]\n",
      "Epoch # 934, Training Loss = [[3.04401338]], Testing Loss = [[7.72717184]]\n",
      "Epoch # 935, Training Loss = [[3.03328593]], Testing Loss = [[7.71114633]]\n",
      "Epoch # 936, Training Loss = [[3.02260039]], Testing Loss = [[7.6951662]]\n",
      "Epoch # 937, Training Loss = [[3.0119566]], Testing Loss = [[7.6792313]]\n",
      "Epoch # 938, Training Loss = [[3.00135439]], Testing Loss = [[7.66334147]]\n",
      "Epoch # 939, Training Loss = [[2.99079359]], Testing Loss = [[7.64749655]]\n",
      "Epoch # 940, Training Loss = [[2.98027404]], Testing Loss = [[7.63169639]]\n",
      "Epoch # 941, Training Loss = [[2.96979557]], Testing Loss = [[7.61594082]]\n",
      "Epoch # 942, Training Loss = [[2.95935801]], Testing Loss = [[7.6002297]]\n",
      "Epoch # 943, Training Loss = [[2.94896119]], Testing Loss = [[7.58456287]]\n",
      "Epoch # 944, Training Loss = [[2.93860495]], Testing Loss = [[7.56894017]]\n",
      "Epoch # 945, Training Loss = [[2.92828914]], Testing Loss = [[7.55336145]]\n",
      "Epoch # 946, Training Loss = [[2.91801357]], Testing Loss = [[7.53782655]]\n",
      "Epoch # 947, Training Loss = [[2.9077781]], Testing Loss = [[7.52233533]]\n",
      "Epoch # 948, Training Loss = [[2.89758255]], Testing Loss = [[7.50688762]]\n",
      "Epoch # 949, Training Loss = [[2.88742678]], Testing Loss = [[7.49148329]]\n",
      "Epoch # 950, Training Loss = [[2.87731061]], Testing Loss = [[7.47612217]]\n",
      "Epoch # 951, Training Loss = [[2.86723388]], Testing Loss = [[7.46080413]]\n",
      "Epoch # 952, Training Loss = [[2.85719644]], Testing Loss = [[7.445529]]\n",
      "Epoch # 953, Training Loss = [[2.84719814]], Testing Loss = [[7.43029664]]\n",
      "Epoch # 954, Training Loss = [[2.8372388]], Testing Loss = [[7.41510689]]\n",
      "Epoch # 955, Training Loss = [[2.82731827]], Testing Loss = [[7.39995962]]\n",
      "Epoch # 956, Training Loss = [[2.8174364]], Testing Loss = [[7.38485468]]\n",
      "Epoch # 957, Training Loss = [[2.80759302]], Testing Loss = [[7.36979191]]\n",
      "Epoch # 958, Training Loss = [[2.79778799]], Testing Loss = [[7.35477117]]\n",
      "Epoch # 959, Training Loss = [[2.78802115]], Testing Loss = [[7.33979232]]\n",
      "Epoch # 960, Training Loss = [[2.77829235]], Testing Loss = [[7.32485521]]\n",
      "Epoch # 961, Training Loss = [[2.76860142]], Testing Loss = [[7.30995969]]\n",
      "Epoch # 962, Training Loss = [[2.75894822]], Testing Loss = [[7.29510562]]\n",
      "Epoch # 963, Training Loss = [[2.7493326]], Testing Loss = [[7.28029286]]\n",
      "Epoch # 964, Training Loss = [[2.7397544]], Testing Loss = [[7.26552126]]\n",
      "Epoch # 965, Training Loss = [[2.73021347]], Testing Loss = [[7.25079069]]\n",
      "Epoch # 966, Training Loss = [[2.72070966]], Testing Loss = [[7.23610099]]\n",
      "Epoch # 967, Training Loss = [[2.71124283]], Testing Loss = [[7.22145204]]\n",
      "Epoch # 968, Training Loss = [[2.70181282]], Testing Loss = [[7.20684368]]\n",
      "Epoch # 969, Training Loss = [[2.69241948]], Testing Loss = [[7.19227578]]\n",
      "Epoch # 970, Training Loss = [[2.68306266]], Testing Loss = [[7.17774819]]\n",
      "Epoch # 971, Training Loss = [[2.67374223]], Testing Loss = [[7.16326078]]\n",
      "Epoch # 972, Training Loss = [[2.66445802]], Testing Loss = [[7.14881342]]\n",
      "Epoch # 973, Training Loss = [[2.6552099]], Testing Loss = [[7.13440595]]\n",
      "Epoch # 974, Training Loss = [[2.64599772]], Testing Loss = [[7.12003825]]\n",
      "Epoch # 975, Training Loss = [[2.63682133]], Testing Loss = [[7.10571017]]\n",
      "Epoch # 976, Training Loss = [[2.62768059]], Testing Loss = [[7.09142159]]\n",
      "Epoch # 977, Training Loss = [[2.61857536]], Testing Loss = [[7.07717235]]\n",
      "Epoch # 978, Training Loss = [[2.60950549]], Testing Loss = [[7.06296234]]\n",
      "Epoch # 979, Training Loss = [[2.60047084]], Testing Loss = [[7.04879141]]\n",
      "Epoch # 980, Training Loss = [[2.59147127]], Testing Loss = [[7.03465943]]\n",
      "Epoch # 981, Training Loss = [[2.58250663]], Testing Loss = [[7.02056626]]\n",
      "Epoch # 982, Training Loss = [[2.57357678]], Testing Loss = [[7.00651178]]\n",
      "Epoch # 983, Training Loss = [[2.56468159]], Testing Loss = [[6.99249584]]\n",
      "Epoch # 984, Training Loss = [[2.55582092]], Testing Loss = [[6.97851832]]\n",
      "Epoch # 985, Training Loss = [[2.54699462]], Testing Loss = [[6.96457908]]\n",
      "Epoch # 986, Training Loss = [[2.53820256]], Testing Loss = [[6.950678]]\n",
      "Epoch # 987, Training Loss = [[2.52944459]], Testing Loss = [[6.93681494]]\n",
      "Epoch # 988, Training Loss = [[2.52072059]], Testing Loss = [[6.92298977]]\n",
      "Epoch # 989, Training Loss = [[2.51203041]], Testing Loss = [[6.90920236]]\n",
      "Epoch # 990, Training Loss = [[2.50337392]], Testing Loss = [[6.89545258]]\n",
      "Epoch # 991, Training Loss = [[2.49475098]], Testing Loss = [[6.88174031]]\n",
      "Epoch # 992, Training Loss = [[2.48616145]], Testing Loss = [[6.86806541]]\n",
      "Epoch # 993, Training Loss = [[2.47760521]], Testing Loss = [[6.85442776]]\n",
      "Epoch # 994, Training Loss = [[2.46908211]], Testing Loss = [[6.84082723]]\n",
      "Epoch # 995, Training Loss = [[2.46059203]], Testing Loss = [[6.82726369]]\n",
      "Epoch # 996, Training Loss = [[2.45213483]], Testing Loss = [[6.81373702]]\n",
      "Epoch # 997, Training Loss = [[2.44371038]], Testing Loss = [[6.80024709]]\n",
      "Epoch # 998, Training Loss = [[2.43531854]], Testing Loss = [[6.78679377]]\n",
      "Epoch # 999, Training Loss = [[2.42695919]], Testing Loss = [[6.77337694]]\n",
      "Epoch # 1000, Training Loss = [[2.41863219]], Testing Loss = [[6.75999648]]\n",
      "Epoch # 1001, Training Loss = [[2.41033741]], Testing Loss = [[6.74665226]]\n",
      "Epoch # 1002, Training Loss = [[2.40207473]], Testing Loss = [[6.73334416]]\n",
      "Epoch # 1003, Training Loss = [[2.39384401]], Testing Loss = [[6.72007205]]\n",
      "Epoch # 1004, Training Loss = [[2.38564512]], Testing Loss = [[6.70683581]]\n",
      "Epoch # 1005, Training Loss = [[2.37747795]], Testing Loss = [[6.69363533]]\n",
      "Epoch # 1006, Training Loss = [[2.36934235]], Testing Loss = [[6.68047047]]\n",
      "Epoch # 1007, Training Loss = [[2.3612382]], Testing Loss = [[6.66734112]]\n",
      "Epoch # 1008, Training Loss = [[2.35316538]], Testing Loss = [[6.65424715]]\n",
      "Epoch # 1009, Training Loss = [[2.34512376]], Testing Loss = [[6.64118845]]\n",
      "Epoch # 1010, Training Loss = [[2.33711321]], Testing Loss = [[6.6281649]]\n",
      "Epoch # 1011, Training Loss = [[2.32913361]], Testing Loss = [[6.61517637]]\n",
      "Epoch # 1012, Training Loss = [[2.32118484]], Testing Loss = [[6.60222275]]\n",
      "Epoch # 1013, Training Loss = [[2.31326676]], Testing Loss = [[6.58930392]]\n",
      "Epoch # 1014, Training Loss = [[2.30537926]], Testing Loss = [[6.57641976]]\n",
      "Epoch # 1015, Training Loss = [[2.29752221]], Testing Loss = [[6.56357016]]\n",
      "Epoch # 1016, Training Loss = [[2.28969549]], Testing Loss = [[6.55075499]]\n",
      "Epoch # 1017, Training Loss = [[2.28189898]], Testing Loss = [[6.53797414]]\n",
      "Epoch # 1018, Training Loss = [[2.27413256]], Testing Loss = [[6.5252275]]\n",
      "Epoch # 1019, Training Loss = [[2.2663961]], Testing Loss = [[6.51251494]]\n",
      "Epoch # 1020, Training Loss = [[2.25868948]], Testing Loss = [[6.49983636]]\n",
      "Epoch # 1021, Training Loss = [[2.25101259]], Testing Loss = [[6.48719163]]\n",
      "Epoch # 1022, Training Loss = [[2.24336531]], Testing Loss = [[6.47458065]]\n",
      "Epoch # 1023, Training Loss = [[2.23574751]], Testing Loss = [[6.4620033]]\n",
      "Epoch # 1024, Training Loss = [[2.22815908]], Testing Loss = [[6.44945946]]\n",
      "Epoch # 1025, Training Loss = [[2.22059989]], Testing Loss = [[6.43694903]]\n",
      "Epoch # 1026, Training Loss = [[2.21306984]], Testing Loss = [[6.42447189]]\n",
      "Epoch # 1027, Training Loss = [[2.2055688]], Testing Loss = [[6.41202792]]\n",
      "Epoch # 1028, Training Loss = [[2.19809666]], Testing Loss = [[6.39961702]]\n",
      "Epoch # 1029, Training Loss = [[2.1906533]], Testing Loss = [[6.38723907]]\n",
      "Epoch # 1030, Training Loss = [[2.18323861]], Testing Loss = [[6.37489397]]\n",
      "Epoch # 1031, Training Loss = [[2.17585247]], Testing Loss = [[6.3625816]]\n",
      "Epoch # 1032, Training Loss = [[2.16849476]], Testing Loss = [[6.35030186]]\n",
      "Epoch # 1033, Training Loss = [[2.16116537]], Testing Loss = [[6.33805462]]\n",
      "Epoch # 1034, Training Loss = [[2.15386419]], Testing Loss = [[6.32583979]]\n",
      "Epoch # 1035, Training Loss = [[2.1465911]], Testing Loss = [[6.31365726]]\n",
      "Epoch # 1036, Training Loss = [[2.13934599]], Testing Loss = [[6.30150691]]\n",
      "Epoch # 1037, Training Loss = [[2.13212875]], Testing Loss = [[6.28938864]]\n",
      "Epoch # 1038, Training Loss = [[2.12493927]], Testing Loss = [[6.27730234]]\n",
      "Epoch # 1039, Training Loss = [[2.11777743]], Testing Loss = [[6.2652479]]\n",
      "Epoch # 1040, Training Loss = [[2.11064313]], Testing Loss = [[6.25322523]]\n",
      "Epoch # 1041, Training Loss = [[2.10353625]], Testing Loss = [[6.2412342]]\n",
      "Epoch # 1042, Training Loss = [[2.09645668]], Testing Loss = [[6.22927471]]\n",
      "Epoch # 1043, Training Loss = [[2.08940431]], Testing Loss = [[6.21734666]]\n",
      "Epoch # 1044, Training Loss = [[2.08237904]], Testing Loss = [[6.20544995]]\n",
      "Epoch # 1045, Training Loss = [[2.07538075]], Testing Loss = [[6.19358446]]\n",
      "Epoch # 1046, Training Loss = [[2.06840933]], Testing Loss = [[6.1817501]]\n",
      "Epoch # 1047, Training Loss = [[2.06146469]], Testing Loss = [[6.16994676]]\n",
      "Epoch # 1048, Training Loss = [[2.05454671]], Testing Loss = [[6.15817434]]\n",
      "Epoch # 1049, Training Loss = [[2.04765528]], Testing Loss = [[6.14643272]]\n",
      "Epoch # 1050, Training Loss = [[2.04079029]], Testing Loss = [[6.13472182]]\n",
      "Epoch # 1051, Training Loss = [[2.03395165]], Testing Loss = [[6.12304153]]\n",
      "Epoch # 1052, Training Loss = [[2.02713924]], Testing Loss = [[6.11139174]]\n",
      "Epoch # 1053, Training Loss = [[2.02035297]], Testing Loss = [[6.09977236]]\n",
      "Epoch # 1054, Training Loss = [[2.01359271]], Testing Loss = [[6.08818327]]\n",
      "Epoch # 1055, Training Loss = [[2.00685838]], Testing Loss = [[6.07662439]]\n",
      "Epoch # 1056, Training Loss = [[2.00014986]], Testing Loss = [[6.06509562]]\n",
      "Epoch # 1057, Training Loss = [[1.99346706]], Testing Loss = [[6.05359684]]\n",
      "Epoch # 1058, Training Loss = [[1.98680986]], Testing Loss = [[6.04212796]]\n",
      "Epoch # 1059, Training Loss = [[1.98017818]], Testing Loss = [[6.03068889]]\n",
      "Epoch # 1060, Training Loss = [[1.97357189]], Testing Loss = [[6.01927952]]\n",
      "Epoch # 1061, Training Loss = [[1.96699091]], Testing Loss = [[6.00789975]]\n",
      "Epoch # 1062, Training Loss = [[1.96043513]], Testing Loss = [[5.99654949]]\n",
      "Epoch # 1063, Training Loss = [[1.95390445]], Testing Loss = [[5.98522864]]\n",
      "Epoch # 1064, Training Loss = [[1.94739876]], Testing Loss = [[5.9739371]]\n",
      "Epoch # 1065, Training Loss = [[1.94091798]], Testing Loss = [[5.96267477]]\n",
      "Epoch # 1066, Training Loss = [[1.93446199]], Testing Loss = [[5.95144156]]\n",
      "Epoch # 1067, Training Loss = [[1.92803071]], Testing Loss = [[5.94023737]]\n",
      "Epoch # 1068, Training Loss = [[1.92162402]], Testing Loss = [[5.9290621]]\n",
      "Epoch # 1069, Training Loss = [[1.91524184]], Testing Loss = [[5.91791566]]\n",
      "Epoch # 1070, Training Loss = [[1.90888406]], Testing Loss = [[5.90679796]]\n",
      "Epoch # 1071, Training Loss = [[1.90255058]], Testing Loss = [[5.89570889]]\n",
      "Epoch # 1072, Training Loss = [[1.89624131]], Testing Loss = [[5.88464837]]\n",
      "Epoch # 1073, Training Loss = [[1.88995616]], Testing Loss = [[5.8736163]]\n",
      "Epoch # 1074, Training Loss = [[1.88369501]], Testing Loss = [[5.86261258]]\n",
      "Epoch # 1075, Training Loss = [[1.87745779]], Testing Loss = [[5.85163712]]\n",
      "Epoch # 1076, Training Loss = [[1.87124439]], Testing Loss = [[5.84068984]]\n",
      "Epoch # 1077, Training Loss = [[1.86505471]], Testing Loss = [[5.82977063]]\n",
      "Epoch # 1078, Training Loss = [[1.85888866]], Testing Loss = [[5.8188794]]\n",
      "Epoch # 1079, Training Loss = [[1.85274616]], Testing Loss = [[5.80801606]]\n",
      "Epoch # 1080, Training Loss = [[1.84662709]], Testing Loss = [[5.79718053]]\n",
      "Epoch # 1081, Training Loss = [[1.84053137]], Testing Loss = [[5.7863727]]\n",
      "Epoch # 1082, Training Loss = [[1.83445891]], Testing Loss = [[5.77559249]]\n",
      "Epoch # 1083, Training Loss = [[1.82840961]], Testing Loss = [[5.7648398]]\n",
      "Epoch # 1084, Training Loss = [[1.82238338]], Testing Loss = [[5.75411455]]\n",
      "Epoch # 1085, Training Loss = [[1.81638013]], Testing Loss = [[5.74341665]]\n",
      "Epoch # 1086, Training Loss = [[1.81039976]], Testing Loss = [[5.732746]]\n",
      "Epoch # 1087, Training Loss = [[1.80444218]], Testing Loss = [[5.72210252]]\n",
      "Epoch # 1088, Training Loss = [[1.79850731]], Testing Loss = [[5.71148612]]\n",
      "Epoch # 1089, Training Loss = [[1.79259504]], Testing Loss = [[5.7008967]]\n",
      "Epoch # 1090, Training Loss = [[1.7867053]], Testing Loss = [[5.69033419]]\n",
      "Epoch # 1091, Training Loss = [[1.78083799]], Testing Loss = [[5.67979849]]\n",
      "Epoch # 1092, Training Loss = [[1.77499301]], Testing Loss = [[5.66928951]]\n",
      "Epoch # 1093, Training Loss = [[1.76917029]], Testing Loss = [[5.65880717]]\n",
      "Epoch # 1094, Training Loss = [[1.76336973]], Testing Loss = [[5.64835139]]\n",
      "Epoch # 1095, Training Loss = [[1.75759124]], Testing Loss = [[5.63792206]]\n",
      "Epoch # 1096, Training Loss = [[1.75183474]], Testing Loss = [[5.62751911]]\n",
      "Epoch # 1097, Training Loss = [[1.74610014]], Testing Loss = [[5.61714246]]\n",
      "Epoch # 1098, Training Loss = [[1.74038734]], Testing Loss = [[5.606792]]\n",
      "Epoch # 1099, Training Loss = [[1.73469627]], Testing Loss = [[5.59646767]]\n",
      "Epoch # 1100, Training Loss = [[1.72902683]], Testing Loss = [[5.58616938]]\n",
      "Epoch # 1101, Training Loss = [[1.72337894]], Testing Loss = [[5.57589703]]\n",
      "Epoch # 1102, Training Loss = [[1.71775251]], Testing Loss = [[5.56565055]]\n",
      "Epoch # 1103, Training Loss = [[1.71214746]], Testing Loss = [[5.55542985]]\n",
      "Epoch # 1104, Training Loss = [[1.7065637]], Testing Loss = [[5.54523484]]\n",
      "Epoch # 1105, Training Loss = [[1.70100114]], Testing Loss = [[5.53506545]]\n",
      "Epoch # 1106, Training Loss = [[1.69545971]], Testing Loss = [[5.5249216]]\n",
      "Epoch # 1107, Training Loss = [[1.68993931]], Testing Loss = [[5.51480318]]\n",
      "Epoch # 1108, Training Loss = [[1.68443987]], Testing Loss = [[5.50471014]]\n",
      "Epoch # 1109, Training Loss = [[1.67896129]], Testing Loss = [[5.49464238]]\n",
      "Epoch # 1110, Training Loss = [[1.6735035]], Testing Loss = [[5.48459981]]\n",
      "Epoch # 1111, Training Loss = [[1.66806642]], Testing Loss = [[5.47458237]]\n",
      "Epoch # 1112, Training Loss = [[1.66264995]], Testing Loss = [[5.46458997]]\n",
      "Epoch # 1113, Training Loss = [[1.65725402]], Testing Loss = [[5.45462252]]\n",
      "Epoch # 1114, Training Loss = [[1.65187854]], Testing Loss = [[5.44467995]]\n",
      "Epoch # 1115, Training Loss = [[1.64652344]], Testing Loss = [[5.43476217]]\n",
      "Epoch # 1116, Training Loss = [[1.64118863]], Testing Loss = [[5.42486911]]\n",
      "Epoch # 1117, Training Loss = [[1.63587403]], Testing Loss = [[5.41500069]]\n",
      "Epoch # 1118, Training Loss = [[1.63057956]], Testing Loss = [[5.40515682]]\n",
      "Epoch # 1119, Training Loss = [[1.62530514]], Testing Loss = [[5.39533742]]\n",
      "Epoch # 1120, Training Loss = [[1.62005069]], Testing Loss = [[5.38554243]]\n",
      "Epoch # 1121, Training Loss = [[1.61481613]], Testing Loss = [[5.37577175]]\n",
      "Epoch # 1122, Training Loss = [[1.60960138]], Testing Loss = [[5.36602532]]\n",
      "Epoch # 1123, Training Loss = [[1.60440637]], Testing Loss = [[5.35630305]]\n",
      "Epoch # 1124, Training Loss = [[1.599231]], Testing Loss = [[5.34660486]]\n",
      "Epoch # 1125, Training Loss = [[1.59407521]], Testing Loss = [[5.33693068]]\n",
      "Epoch # 1126, Training Loss = [[1.58893892]], Testing Loss = [[5.32728043]]\n",
      "Epoch # 1127, Training Loss = [[1.58382204]], Testing Loss = [[5.31765403]]\n",
      "Epoch # 1128, Training Loss = [[1.57872451]], Testing Loss = [[5.30805141]]\n",
      "Epoch # 1129, Training Loss = [[1.57364623]], Testing Loss = [[5.29847249]]\n",
      "Epoch # 1130, Training Loss = [[1.56858715]], Testing Loss = [[5.2889172]]\n",
      "Epoch # 1131, Training Loss = [[1.56354717]], Testing Loss = [[5.27938545]]\n",
      "Epoch # 1132, Training Loss = [[1.55852623]], Testing Loss = [[5.26987717]]\n",
      "Epoch # 1133, Training Loss = [[1.55352425]], Testing Loss = [[5.2603923]]\n",
      "Epoch # 1134, Training Loss = [[1.54854115]], Testing Loss = [[5.25093074]]\n",
      "Epoch # 1135, Training Loss = [[1.54357686]], Testing Loss = [[5.24149244]]\n",
      "Epoch # 1136, Training Loss = [[1.5386313]], Testing Loss = [[5.2320773]]\n",
      "Epoch # 1137, Training Loss = [[1.53370439]], Testing Loss = [[5.22268527]]\n",
      "Epoch # 1138, Training Loss = [[1.52879607]], Testing Loss = [[5.21331626]]\n",
      "Epoch # 1139, Training Loss = [[1.52390626]], Testing Loss = [[5.20397021]]\n",
      "Epoch # 1140, Training Loss = [[1.51903489]], Testing Loss = [[5.19464703]]\n",
      "Epoch # 1141, Training Loss = [[1.51418187]], Testing Loss = [[5.18534666]]\n",
      "Epoch # 1142, Training Loss = [[1.50934715]], Testing Loss = [[5.17606903]]\n",
      "Epoch # 1143, Training Loss = [[1.50453064]], Testing Loss = [[5.16681405]]\n",
      "Epoch # 1144, Training Loss = [[1.49973227]], Testing Loss = [[5.15758166]]\n",
      "Epoch # 1145, Training Loss = [[1.49495197]], Testing Loss = [[5.14837179]]\n",
      "Epoch # 1146, Training Loss = [[1.49018967]], Testing Loss = [[5.13918436]]\n",
      "Epoch # 1147, Training Loss = [[1.4854453]], Testing Loss = [[5.13001931]]\n",
      "Epoch # 1148, Training Loss = [[1.48071879]], Testing Loss = [[5.12087656]]\n",
      "Epoch # 1149, Training Loss = [[1.47601006]], Testing Loss = [[5.11175604]]\n",
      "Epoch # 1150, Training Loss = [[1.47131904]], Testing Loss = [[5.10265768]]\n",
      "Epoch # 1151, Training Loss = [[1.46664567]], Testing Loss = [[5.09358142]]\n",
      "Epoch # 1152, Training Loss = [[1.46198986]], Testing Loss = [[5.08452717]]\n",
      "Epoch # 1153, Training Loss = [[1.45735157]], Testing Loss = [[5.07549487]]\n",
      "Epoch # 1154, Training Loss = [[1.4527307]], Testing Loss = [[5.06648446]]\n",
      "Epoch # 1155, Training Loss = [[1.4481272]], Testing Loss = [[5.05749585]]\n",
      "Epoch # 1156, Training Loss = [[1.44354099]], Testing Loss = [[5.04852899]]\n",
      "Epoch # 1157, Training Loss = [[1.43897201]], Testing Loss = [[5.03958381]]\n",
      "Epoch # 1158, Training Loss = [[1.43442019]], Testing Loss = [[5.03066022]]\n",
      "Epoch # 1159, Training Loss = [[1.42988545]], Testing Loss = [[5.02175818]]\n",
      "Epoch # 1160, Training Loss = [[1.42536774]], Testing Loss = [[5.0128776]]\n",
      "Epoch # 1161, Training Loss = [[1.42086698]], Testing Loss = [[5.00401843]]\n",
      "Epoch # 1162, Training Loss = [[1.41638311]], Testing Loss = [[4.99518059]]\n",
      "Epoch # 1163, Training Loss = [[1.41191605]], Testing Loss = [[4.98636402]]\n",
      "Epoch # 1164, Training Loss = [[1.40746575]], Testing Loss = [[4.97756864]]\n",
      "Epoch # 1165, Training Loss = [[1.40303213]], Testing Loss = [[4.9687944]]\n",
      "Epoch # 1166, Training Loss = [[1.39861513]], Testing Loss = [[4.96004122]]\n",
      "Epoch # 1167, Training Loss = [[1.39421468]], Testing Loss = [[4.95130905]]\n",
      "Epoch # 1168, Training Loss = [[1.38983072]], Testing Loss = [[4.94259781]]\n",
      "Epoch # 1169, Training Loss = [[1.38546318]], Testing Loss = [[4.93390743]]\n",
      "Epoch # 1170, Training Loss = [[1.381112]], Testing Loss = [[4.92523786]]\n",
      "Epoch # 1171, Training Loss = [[1.3767771]], Testing Loss = [[4.91658903]]\n",
      "Epoch # 1172, Training Loss = [[1.37245844]], Testing Loss = [[4.90796086]]\n",
      "Epoch # 1173, Training Loss = [[1.36815593]], Testing Loss = [[4.89935331]]\n",
      "Epoch # 1174, Training Loss = [[1.36386953]], Testing Loss = [[4.89076629]]\n",
      "Epoch # 1175, Training Loss = [[1.35959915]], Testing Loss = [[4.88219976]]\n",
      "Epoch # 1176, Training Loss = [[1.35534475]], Testing Loss = [[4.87365364]]\n",
      "Epoch # 1177, Training Loss = [[1.35110625]], Testing Loss = [[4.86512787]]\n",
      "Epoch # 1178, Training Loss = [[1.3468836]], Testing Loss = [[4.85662238]]\n",
      "Epoch # 1179, Training Loss = [[1.34267673]], Testing Loss = [[4.84813712]]\n",
      "Epoch # 1180, Training Loss = [[1.33848557]], Testing Loss = [[4.83967202]]\n",
      "Epoch # 1181, Training Loss = [[1.33431007]], Testing Loss = [[4.83122702]]\n",
      "Epoch # 1182, Training Loss = [[1.33015017]], Testing Loss = [[4.82280205]]\n",
      "Epoch # 1183, Training Loss = [[1.32600579]], Testing Loss = [[4.81439705]]\n",
      "Epoch # 1184, Training Loss = [[1.32187688]], Testing Loss = [[4.80601196]]\n",
      "Epoch # 1185, Training Loss = [[1.31776339]], Testing Loss = [[4.79764672]]\n",
      "Epoch # 1186, Training Loss = [[1.31366524]], Testing Loss = [[4.78930127]]\n",
      "Epoch # 1187, Training Loss = [[1.30958237]], Testing Loss = [[4.78097554]]\n",
      "Epoch # 1188, Training Loss = [[1.30551473]], Testing Loss = [[4.77266947]]\n",
      "Epoch # 1189, Training Loss = [[1.30146226]], Testing Loss = [[4.764383]]\n",
      "Epoch # 1190, Training Loss = [[1.29742489]], Testing Loss = [[4.75611608]]\n",
      "Epoch # 1191, Training Loss = [[1.29340256]], Testing Loss = [[4.74786863]]\n",
      "Epoch # 1192, Training Loss = [[1.28939522]], Testing Loss = [[4.7396406]]\n",
      "Epoch # 1193, Training Loss = [[1.28540281]], Testing Loss = [[4.73143194]]\n",
      "Epoch # 1194, Training Loss = [[1.28142526]], Testing Loss = [[4.72324257]]\n",
      "Epoch # 1195, Training Loss = [[1.27746252]], Testing Loss = [[4.71507244]]\n",
      "Epoch # 1196, Training Loss = [[1.27351453]], Testing Loss = [[4.70692149]]\n",
      "Epoch # 1197, Training Loss = [[1.26958123]], Testing Loss = [[4.69878966]]\n",
      "Epoch # 1198, Training Loss = [[1.26566257]], Testing Loss = [[4.6906769]]\n",
      "Epoch # 1199, Training Loss = [[1.26175847]], Testing Loss = [[4.68258313]]\n",
      "Epoch # 1200, Training Loss = [[1.2578689]], Testing Loss = [[4.67450831]]\n",
      "Epoch # 1201, Training Loss = [[1.25399378]], Testing Loss = [[4.66645238]]\n",
      "Epoch # 1202, Training Loss = [[1.25013306]], Testing Loss = [[4.65841527]]\n",
      "Epoch # 1203, Training Loss = [[1.24628669]], Testing Loss = [[4.65039693]]\n",
      "Epoch # 1204, Training Loss = [[1.24245461]], Testing Loss = [[4.6423973]]\n",
      "Epoch # 1205, Training Loss = [[1.23863676]], Testing Loss = [[4.63441633]]\n",
      "Epoch # 1206, Training Loss = [[1.23483308]], Testing Loss = [[4.62645395]]\n",
      "Epoch # 1207, Training Loss = [[1.23104352]], Testing Loss = [[4.61851011]]\n",
      "Epoch # 1208, Training Loss = [[1.22726802]], Testing Loss = [[4.61058475]]\n",
      "Epoch # 1209, Training Loss = [[1.22350653]], Testing Loss = [[4.60267782]]\n",
      "Epoch # 1210, Training Loss = [[1.219759]], Testing Loss = [[4.59478926]]\n",
      "Epoch # 1211, Training Loss = [[1.21602536]], Testing Loss = [[4.58691901]]\n",
      "Epoch # 1212, Training Loss = [[1.21230556]], Testing Loss = [[4.57906701]]\n",
      "Epoch # 1213, Training Loss = [[1.20859954]], Testing Loss = [[4.57123321]]\n",
      "Epoch # 1214, Training Loss = [[1.20490726]], Testing Loss = [[4.56341756]]\n",
      "Epoch # 1215, Training Loss = [[1.20122866]], Testing Loss = [[4.55562]]\n",
      "Epoch # 1216, Training Loss = [[1.19756368]], Testing Loss = [[4.54784046]]\n",
      "Epoch # 1217, Training Loss = [[1.19391227]], Testing Loss = [[4.54007891]]\n",
      "Epoch # 1218, Training Loss = [[1.19027437]], Testing Loss = [[4.53233528]]\n",
      "Epoch # 1219, Training Loss = [[1.18664994]], Testing Loss = [[4.52460952]]\n",
      "Epoch # 1220, Training Loss = [[1.18303892]], Testing Loss = [[4.51690157]]\n",
      "Epoch # 1221, Training Loss = [[1.17944126]], Testing Loss = [[4.50921138]]\n",
      "Epoch # 1222, Training Loss = [[1.17585689]], Testing Loss = [[4.5015389]]\n",
      "Epoch # 1223, Training Loss = [[1.17228578]], Testing Loss = [[4.49388407]]\n",
      "Epoch # 1224, Training Loss = [[1.16872787]], Testing Loss = [[4.48624683]]\n",
      "Epoch # 1225, Training Loss = [[1.1651831]], Testing Loss = [[4.47862714]]\n",
      "Epoch # 1226, Training Loss = [[1.16165143]], Testing Loss = [[4.47102494]]\n",
      "Epoch # 1227, Training Loss = [[1.1581328]], Testing Loss = [[4.46344017]]\n",
      "Epoch # 1228, Training Loss = [[1.15462716]], Testing Loss = [[4.45587279]]\n",
      "Epoch # 1229, Training Loss = [[1.15113446]], Testing Loss = [[4.44832274]]\n",
      "Epoch # 1230, Training Loss = [[1.14765465]], Testing Loss = [[4.44078997]]\n",
      "Epoch # 1231, Training Loss = [[1.14418768]], Testing Loss = [[4.43327443]]\n",
      "Epoch # 1232, Training Loss = [[1.14073349]], Testing Loss = [[4.42577606]]\n",
      "Epoch # 1233, Training Loss = [[1.13729204]], Testing Loss = [[4.41829481]]\n",
      "Epoch # 1234, Training Loss = [[1.13386328]], Testing Loss = [[4.41083063]]\n",
      "Epoch # 1235, Training Loss = [[1.13044716]], Testing Loss = [[4.40338347]]\n",
      "Epoch # 1236, Training Loss = [[1.12704361]], Testing Loss = [[4.39595328]]\n",
      "Epoch # 1237, Training Loss = [[1.12365261]], Testing Loss = [[4.38854]]\n",
      "Epoch # 1238, Training Loss = [[1.12027409]], Testing Loss = [[4.38114359]]\n",
      "Epoch # 1239, Training Loss = [[1.11690801]], Testing Loss = [[4.37376399]]\n",
      "Epoch # 1240, Training Loss = [[1.11355432]], Testing Loss = [[4.36640115]]\n",
      "Epoch # 1241, Training Loss = [[1.11021297]], Testing Loss = [[4.35905503]]\n",
      "Epoch # 1242, Training Loss = [[1.10688391]], Testing Loss = [[4.35172557]]\n",
      "Epoch # 1243, Training Loss = [[1.1035671]], Testing Loss = [[4.34441272]]\n",
      "Epoch # 1244, Training Loss = [[1.10026248]], Testing Loss = [[4.33711643]]\n",
      "Epoch # 1245, Training Loss = [[1.09697]], Testing Loss = [[4.32983665]]\n",
      "Epoch # 1246, Training Loss = [[1.09368962]], Testing Loss = [[4.32257334]]\n",
      "Epoch # 1247, Training Loss = [[1.09042129]], Testing Loss = [[4.31532643]]\n",
      "Epoch # 1248, Training Loss = [[1.08716497]], Testing Loss = [[4.30809589]]\n",
      "Epoch # 1249, Training Loss = [[1.0839206]], Testing Loss = [[4.30088167]]\n",
      "Epoch # 1250, Training Loss = [[1.08068814]], Testing Loss = [[4.29368371]]\n",
      "Epoch # 1251, Training Loss = [[1.07746755]], Testing Loss = [[4.28650197]]\n",
      "Epoch # 1252, Training Loss = [[1.07425876]], Testing Loss = [[4.27933639]]\n",
      "Epoch # 1253, Training Loss = [[1.07106175]], Testing Loss = [[4.27218694]]\n",
      "Epoch # 1254, Training Loss = [[1.06787646]], Testing Loss = [[4.26505355]]\n",
      "Epoch # 1255, Training Loss = [[1.06470284]], Testing Loss = [[4.25793619]]\n",
      "Epoch # 1256, Training Loss = [[1.06154085]], Testing Loss = [[4.25083481]]\n",
      "Epoch # 1257, Training Loss = [[1.05839045]], Testing Loss = [[4.24374935]]\n",
      "Epoch # 1258, Training Loss = [[1.05525159]], Testing Loss = [[4.23667977]]\n",
      "Epoch # 1259, Training Loss = [[1.05212422]], Testing Loss = [[4.22962602]]\n",
      "Epoch # 1260, Training Loss = [[1.0490083]], Testing Loss = [[4.22258806]]\n",
      "Epoch # 1261, Training Loss = [[1.04590378]], Testing Loss = [[4.21556584]]\n",
      "Epoch # 1262, Training Loss = [[1.04281061]], Testing Loss = [[4.20855931]]\n",
      "Epoch # 1263, Training Loss = [[1.03972877]], Testing Loss = [[4.20156843]]\n",
      "Epoch # 1264, Training Loss = [[1.03665819]], Testing Loss = [[4.19459314]]\n",
      "Epoch # 1265, Training Loss = [[1.03359883]], Testing Loss = [[4.1876334]]\n",
      "Epoch # 1266, Training Loss = [[1.03055065]], Testing Loss = [[4.18068917]]\n",
      "Epoch # 1267, Training Loss = [[1.02751362]], Testing Loss = [[4.1737604]]\n",
      "Epoch # 1268, Training Loss = [[1.02448767]], Testing Loss = [[4.16684705]]\n",
      "Epoch # 1269, Training Loss = [[1.02147277]], Testing Loss = [[4.15994906]]\n",
      "Epoch # 1270, Training Loss = [[1.01846888]], Testing Loss = [[4.15306639]]\n",
      "Epoch # 1271, Training Loss = [[1.01547595]], Testing Loss = [[4.146199]]\n",
      "Epoch # 1272, Training Loss = [[1.01249393]], Testing Loss = [[4.13934685]]\n",
      "Epoch # 1273, Training Loss = [[1.0095228]], Testing Loss = [[4.13250988]]\n",
      "Epoch # 1274, Training Loss = [[1.0065625]], Testing Loss = [[4.12568805]]\n",
      "Epoch # 1275, Training Loss = [[1.00361298]], Testing Loss = [[4.11888132]]\n",
      "Epoch # 1276, Training Loss = [[1.00067422]], Testing Loss = [[4.11208964]]\n",
      "Epoch # 1277, Training Loss = [[0.99774616]], Testing Loss = [[4.10531297]]\n",
      "Epoch # 1278, Training Loss = [[0.99482877]], Testing Loss = [[4.09855126]]\n",
      "Epoch # 1279, Training Loss = [[0.991922]], Testing Loss = [[4.09180447]]\n",
      "Epoch # 1280, Training Loss = [[0.98902581]], Testing Loss = [[4.08507256]]\n",
      "Epoch # 1281, Training Loss = [[0.98614015]], Testing Loss = [[4.07835548]]\n",
      "Epoch # 1282, Training Loss = [[0.983265]], Testing Loss = [[4.07165319]]\n",
      "Epoch # 1283, Training Loss = [[0.9804003]], Testing Loss = [[4.06496565]]\n",
      "Epoch # 1284, Training Loss = [[0.97754602]], Testing Loss = [[4.0582928]]\n",
      "Epoch # 1285, Training Loss = [[0.97470211]], Testing Loss = [[4.05163462]]\n",
      "Epoch # 1286, Training Loss = [[0.97186853]], Testing Loss = [[4.04499105]]\n",
      "Epoch # 1287, Training Loss = [[0.96904525]], Testing Loss = [[4.03836205]]\n",
      "Epoch # 1288, Training Loss = [[0.96623222]], Testing Loss = [[4.03174758]]\n",
      "Epoch # 1289, Training Loss = [[0.9634294]], Testing Loss = [[4.0251476]]\n",
      "Epoch # 1290, Training Loss = [[0.96063676]], Testing Loss = [[4.01856207]]\n",
      "Epoch # 1291, Training Loss = [[0.95785425]], Testing Loss = [[4.01199093]]\n",
      "Epoch # 1292, Training Loss = [[0.95508183]], Testing Loss = [[4.00543416]]\n",
      "Epoch # 1293, Training Loss = [[0.95231946]], Testing Loss = [[3.99889171]]\n",
      "Epoch # 1294, Training Loss = [[0.94956711]], Testing Loss = [[3.99236353]]\n",
      "Epoch # 1295, Training Loss = [[0.94682473]], Testing Loss = [[3.98584959]]\n",
      "Epoch # 1296, Training Loss = [[0.94409229]], Testing Loss = [[3.97934984]]\n",
      "Epoch # 1297, Training Loss = [[0.94136974]], Testing Loss = [[3.97286424]]\n",
      "Epoch # 1298, Training Loss = [[0.93865705]], Testing Loss = [[3.96639275]]\n",
      "Epoch # 1299, Training Loss = [[0.93595418]], Testing Loss = [[3.95993534]]\n",
      "Epoch # 1300, Training Loss = [[0.93326109]], Testing Loss = [[3.95349195]]\n",
      "Epoch # 1301, Training Loss = [[0.93057775]], Testing Loss = [[3.94706255]]\n",
      "Epoch # 1302, Training Loss = [[0.9279041]], Testing Loss = [[3.9406471]]\n",
      "Epoch # 1303, Training Loss = [[0.92524013]], Testing Loss = [[3.93424556]]\n",
      "Epoch # 1304, Training Loss = [[0.92258578]], Testing Loss = [[3.92785788]]\n",
      "Epoch # 1305, Training Loss = [[0.91994102]], Testing Loss = [[3.92148403]]\n",
      "Epoch # 1306, Training Loss = [[0.91730582]], Testing Loss = [[3.91512397]]\n",
      "Epoch # 1307, Training Loss = [[0.91468013]], Testing Loss = [[3.90877765]]\n",
      "Epoch # 1308, Training Loss = [[0.91206392]], Testing Loss = [[3.90244504]]\n",
      "Epoch # 1309, Training Loss = [[0.90945715]], Testing Loss = [[3.89612609]]\n",
      "Epoch # 1310, Training Loss = [[0.90685978]], Testing Loss = [[3.88982078]]\n",
      "Epoch # 1311, Training Loss = [[0.90427179]], Testing Loss = [[3.88352905]]\n",
      "Epoch # 1312, Training Loss = [[0.90169312]], Testing Loss = [[3.87725087]]\n",
      "Epoch # 1313, Training Loss = [[0.89912375]], Testing Loss = [[3.8709862]]\n",
      "Epoch # 1314, Training Loss = [[0.89656363]], Testing Loss = [[3.864735]]\n",
      "Epoch # 1315, Training Loss = [[0.89401274]], Testing Loss = [[3.85849724]]\n",
      "Epoch # 1316, Training Loss = [[0.89147104]], Testing Loss = [[3.85227286]]\n",
      "Epoch # 1317, Training Loss = [[0.88893848]], Testing Loss = [[3.84606185]]\n",
      "Epoch # 1318, Training Loss = [[0.88641504]], Testing Loss = [[3.83986415]]\n",
      "Epoch # 1319, Training Loss = [[0.88390067]], Testing Loss = [[3.83367972]]\n",
      "Epoch # 1320, Training Loss = [[0.88139535]], Testing Loss = [[3.82750854]]\n",
      "Epoch # 1321, Training Loss = [[0.87889904]], Testing Loss = [[3.82135056]]\n",
      "Epoch # 1322, Training Loss = [[0.8764117]], Testing Loss = [[3.81520574]]\n",
      "Epoch # 1323, Training Loss = [[0.8739333]], Testing Loss = [[3.80907405]]\n",
      "Epoch # 1324, Training Loss = [[0.8714638]], Testing Loss = [[3.80295545]]\n",
      "Epoch # 1325, Training Loss = [[0.86900317]], Testing Loss = [[3.7968499]]\n",
      "Epoch # 1326, Training Loss = [[0.86655137]], Testing Loss = [[3.79075737]]\n",
      "Epoch # 1327, Training Loss = [[0.86410837]], Testing Loss = [[3.78467781]]\n",
      "Epoch # 1328, Training Loss = [[0.86167413]], Testing Loss = [[3.77861119]]\n",
      "Epoch # 1329, Training Loss = [[0.85924863]], Testing Loss = [[3.77255747]]\n",
      "Epoch # 1330, Training Loss = [[0.85683182]], Testing Loss = [[3.76651662]]\n",
      "Epoch # 1331, Training Loss = [[0.85442367]], Testing Loss = [[3.7604886]]\n",
      "Epoch # 1332, Training Loss = [[0.85202415]], Testing Loss = [[3.75447338]]\n",
      "Epoch # 1333, Training Loss = [[0.84963323]], Testing Loss = [[3.74847091]]\n",
      "Epoch # 1334, Training Loss = [[0.84725086]], Testing Loss = [[3.74248116]]\n",
      "Epoch # 1335, Training Loss = [[0.84487703]], Testing Loss = [[3.73650409]]\n",
      "Epoch # 1336, Training Loss = [[0.84251169]], Testing Loss = [[3.73053967]]\n",
      "Epoch # 1337, Training Loss = [[0.84015481]], Testing Loss = [[3.72458786]]\n",
      "Epoch # 1338, Training Loss = [[0.83780636]], Testing Loss = [[3.71864863]]\n",
      "Epoch # 1339, Training Loss = [[0.8354663]], Testing Loss = [[3.71272194]]\n",
      "Epoch # 1340, Training Loss = [[0.83313461]], Testing Loss = [[3.70680776]]\n",
      "Epoch # 1341, Training Loss = [[0.83081125]], Testing Loss = [[3.70090604]]\n",
      "Epoch # 1342, Training Loss = [[0.82849619]], Testing Loss = [[3.69501676]]\n",
      "Epoch # 1343, Training Loss = [[0.82618939]], Testing Loss = [[3.68913988]]\n",
      "Epoch # 1344, Training Loss = [[0.82389082]], Testing Loss = [[3.68327536]]\n",
      "Epoch # 1345, Training Loss = [[0.82160046]], Testing Loss = [[3.67742317]]\n",
      "Epoch # 1346, Training Loss = [[0.81931827]], Testing Loss = [[3.67158327]]\n",
      "Epoch # 1347, Training Loss = [[0.81704421]], Testing Loss = [[3.66575564]]\n",
      "Epoch # 1348, Training Loss = [[0.81477826]], Testing Loss = [[3.65994023]]\n",
      "Epoch # 1349, Training Loss = [[0.81252038]], Testing Loss = [[3.65413701]]\n",
      "Epoch # 1350, Training Loss = [[0.81027055]], Testing Loss = [[3.64834594]]\n",
      "Epoch # 1351, Training Loss = [[0.80802873]], Testing Loss = [[3.642567]]\n",
      "Epoch # 1352, Training Loss = [[0.80579489]], Testing Loss = [[3.63680015]]\n",
      "Epoch # 1353, Training Loss = [[0.803569]], Testing Loss = [[3.63104535]]\n",
      "Epoch # 1354, Training Loss = [[0.80135102]], Testing Loss = [[3.62530257]]\n",
      "Epoch # 1355, Training Loss = [[0.79914094]], Testing Loss = [[3.61957177]]\n",
      "Epoch # 1356, Training Loss = [[0.79693871]], Testing Loss = [[3.61385293]]\n",
      "Epoch # 1357, Training Loss = [[0.79474431]], Testing Loss = [[3.60814601]]\n",
      "Epoch # 1358, Training Loss = [[0.7925577]], Testing Loss = [[3.60245097]]\n",
      "Epoch # 1359, Training Loss = [[0.79037886]], Testing Loss = [[3.59676779]]\n",
      "Epoch # 1360, Training Loss = [[0.78820776]], Testing Loss = [[3.59109643]]\n",
      "Epoch # 1361, Training Loss = [[0.78604436]], Testing Loss = [[3.58543685]]\n",
      "Epoch # 1362, Training Loss = [[0.78388864]], Testing Loss = [[3.57978903]]\n",
      "Epoch # 1363, Training Loss = [[0.78174056]], Testing Loss = [[3.57415292]]\n",
      "Epoch # 1364, Training Loss = [[0.7796001]], Testing Loss = [[3.56852851]]\n",
      "Epoch # 1365, Training Loss = [[0.77746723]], Testing Loss = [[3.56291575]]\n",
      "Epoch # 1366, Training Loss = [[0.77534191]], Testing Loss = [[3.55731461]]\n",
      "Epoch # 1367, Training Loss = [[0.77322412]], Testing Loss = [[3.55172506]]\n",
      "Epoch # 1368, Training Loss = [[0.77111383]], Testing Loss = [[3.54614707]]\n",
      "Epoch # 1369, Training Loss = [[0.76901101]], Testing Loss = [[3.54058061]]\n",
      "Epoch # 1370, Training Loss = [[0.76691563]], Testing Loss = [[3.53502564]]\n",
      "Epoch # 1371, Training Loss = [[0.76482766]], Testing Loss = [[3.52948214]]\n",
      "Epoch # 1372, Training Loss = [[0.76274708]], Testing Loss = [[3.52395006]]\n",
      "Epoch # 1373, Training Loss = [[0.76067384]], Testing Loss = [[3.51842938]]\n",
      "Epoch # 1374, Training Loss = [[0.75860794]], Testing Loss = [[3.51292007]]\n",
      "Epoch # 1375, Training Loss = [[0.75654933]], Testing Loss = [[3.5074221]]\n",
      "Epoch # 1376, Training Loss = [[0.75449799]], Testing Loss = [[3.50193543]]\n",
      "Epoch # 1377, Training Loss = [[0.75245389]], Testing Loss = [[3.49646003]]\n",
      "Epoch # 1378, Training Loss = [[0.75041701]], Testing Loss = [[3.49099588]]\n",
      "Epoch # 1379, Training Loss = [[0.74838731]], Testing Loss = [[3.48554293]]\n",
      "Epoch # 1380, Training Loss = [[0.74636476]], Testing Loss = [[3.48010117]]\n",
      "Epoch # 1381, Training Loss = [[0.74434935]], Testing Loss = [[3.47467055]]\n",
      "Epoch # 1382, Training Loss = [[0.74234103]], Testing Loss = [[3.46925105]]\n",
      "Epoch # 1383, Training Loss = [[0.7403398]], Testing Loss = [[3.46384265]]\n",
      "Epoch # 1384, Training Loss = [[0.7383456]], Testing Loss = [[3.45844529]]\n",
      "Epoch # 1385, Training Loss = [[0.73635843]], Testing Loss = [[3.45305897]]\n",
      "Epoch # 1386, Training Loss = [[0.73437825]], Testing Loss = [[3.44768364]]\n",
      "Epoch # 1387, Training Loss = [[0.73240504]], Testing Loss = [[3.44231927]]\n",
      "Epoch # 1388, Training Loss = [[0.73043876]], Testing Loss = [[3.43696585]]\n",
      "Epoch # 1389, Training Loss = [[0.7284794]], Testing Loss = [[3.43162333]]\n",
      "Epoch # 1390, Training Loss = [[0.72652692]], Testing Loss = [[3.42629168]]\n",
      "Epoch # 1391, Training Loss = [[0.7245813]], Testing Loss = [[3.42097088]]\n",
      "Epoch # 1392, Training Loss = [[0.72264251]], Testing Loss = [[3.4156609]]\n",
      "Epoch # 1393, Training Loss = [[0.72071052]], Testing Loss = [[3.41036171]]\n",
      "Epoch # 1394, Training Loss = [[0.71878532]], Testing Loss = [[3.40507327]]\n",
      "Epoch # 1395, Training Loss = [[0.71686687]], Testing Loss = [[3.39979556]]\n",
      "Epoch # 1396, Training Loss = [[0.71495514]], Testing Loss = [[3.39452855]]\n",
      "Epoch # 1397, Training Loss = [[0.71305011]], Testing Loss = [[3.38927221]]\n",
      "Epoch # 1398, Training Loss = [[0.71115176]], Testing Loss = [[3.38402651]]\n",
      "Epoch # 1399, Training Loss = [[0.70926006]], Testing Loss = [[3.37879142]]\n",
      "Epoch # 1400, Training Loss = [[0.70737498]], Testing Loss = [[3.37356692]]\n",
      "Epoch # 1401, Training Loss = [[0.7054965]], Testing Loss = [[3.36835297]]\n",
      "Epoch # 1402, Training Loss = [[0.70362459]], Testing Loss = [[3.36314954]]\n",
      "Epoch # 1403, Training Loss = [[0.70175923]], Testing Loss = [[3.35795661]]\n",
      "Epoch # 1404, Training Loss = [[0.69990039]], Testing Loss = [[3.35277415]]\n",
      "Epoch # 1405, Training Loss = [[0.69804805]], Testing Loss = [[3.34760213]]\n",
      "Epoch # 1406, Training Loss = [[0.69620218]], Testing Loss = [[3.34244052]]\n",
      "Epoch # 1407, Training Loss = [[0.69436275]], Testing Loss = [[3.3372893]]\n",
      "Epoch # 1408, Training Loss = [[0.69252975]], Testing Loss = [[3.33214842]]\n",
      "Epoch # 1409, Training Loss = [[0.69070314]], Testing Loss = [[3.32701788]]\n",
      "Epoch # 1410, Training Loss = [[0.68888291]], Testing Loss = [[3.32189764]]\n",
      "Epoch # 1411, Training Loss = [[0.68706902]], Testing Loss = [[3.31678766]]\n",
      "Epoch # 1412, Training Loss = [[0.68526145]], Testing Loss = [[3.31168793]]\n",
      "Epoch # 1413, Training Loss = [[0.68346019]], Testing Loss = [[3.30659842]]\n",
      "Epoch # 1414, Training Loss = [[0.6816652]], Testing Loss = [[3.3015191]]\n",
      "Epoch # 1415, Training Loss = [[0.67987646]], Testing Loss = [[3.29644993]]\n",
      "Epoch # 1416, Training Loss = [[0.67809394]], Testing Loss = [[3.2913909]]\n",
      "Epoch # 1417, Training Loss = [[0.67631763]], Testing Loss = [[3.28634198]]\n",
      "Epoch # 1418, Training Loss = [[0.6745475]], Testing Loss = [[3.28130313]]\n",
      "Epoch # 1419, Training Loss = [[0.67278352]], Testing Loss = [[3.27627434]]\n",
      "Epoch # 1420, Training Loss = [[0.67102567]], Testing Loss = [[3.27125557]]\n",
      "Epoch # 1421, Training Loss = [[0.66927393]], Testing Loss = [[3.26624681]]\n",
      "Epoch # 1422, Training Loss = [[0.66752827]], Testing Loss = [[3.26124801]]\n",
      "Epoch # 1423, Training Loss = [[0.66578867]], Testing Loss = [[3.25625916]]\n",
      "Epoch # 1424, Training Loss = [[0.66405511]], Testing Loss = [[3.25128022]]\n",
      "Epoch # 1425, Training Loss = [[0.66232757]], Testing Loss = [[3.24631118]]\n",
      "Epoch # 1426, Training Loss = [[0.66060601]], Testing Loss = [[3.241352]]\n",
      "Epoch # 1427, Training Loss = [[0.65889042]], Testing Loss = [[3.23640266]]\n",
      "Epoch # 1428, Training Loss = [[0.65718078]], Testing Loss = [[3.23146313]]\n",
      "Epoch # 1429, Training Loss = [[0.65547706]], Testing Loss = [[3.22653339]]\n",
      "Epoch # 1430, Training Loss = [[0.65377924]], Testing Loss = [[3.22161341]]\n",
      "Epoch # 1431, Training Loss = [[0.65208729]], Testing Loss = [[3.21670317]]\n",
      "Epoch # 1432, Training Loss = [[0.6504012]], Testing Loss = [[3.21180263]]\n",
      "Epoch # 1433, Training Loss = [[0.64872094]], Testing Loss = [[3.20691177]]\n",
      "Epoch # 1434, Training Loss = [[0.64704649]], Testing Loss = [[3.20203057]]\n",
      "Epoch # 1435, Training Loss = [[0.64537782]], Testing Loss = [[3.19715901]]\n",
      "Epoch # 1436, Training Loss = [[0.64371492]], Testing Loss = [[3.19229704]]\n",
      "Epoch # 1437, Training Loss = [[0.64205777]], Testing Loss = [[3.18744466]]\n",
      "Epoch # 1438, Training Loss = [[0.64040633]], Testing Loss = [[3.18260183]]\n",
      "Epoch # 1439, Training Loss = [[0.63876059]], Testing Loss = [[3.17776854]]\n",
      "Epoch # 1440, Training Loss = [[0.63712053]], Testing Loss = [[3.17294474]]\n",
      "Epoch # 1441, Training Loss = [[0.63548612]], Testing Loss = [[3.16813043]]\n",
      "Epoch # 1442, Training Loss = [[0.63385735]], Testing Loss = [[3.16332556]]\n",
      "Epoch # 1443, Training Loss = [[0.63223419]], Testing Loss = [[3.15853013]]\n",
      "Epoch # 1444, Training Loss = [[0.63061661]], Testing Loss = [[3.1537441]]\n",
      "Epoch # 1445, Training Loss = [[0.62900461]], Testing Loss = [[3.14896745]]\n",
      "Epoch # 1446, Training Loss = [[0.62739815]], Testing Loss = [[3.14420016]]\n",
      "Epoch # 1447, Training Loss = [[0.62579722]], Testing Loss = [[3.13944219]]\n",
      "Epoch # 1448, Training Loss = [[0.62420179]], Testing Loss = [[3.13469353]]\n",
      "Epoch # 1449, Training Loss = [[0.62261185]], Testing Loss = [[3.12995415]]\n",
      "Epoch # 1450, Training Loss = [[0.62102737]], Testing Loss = [[3.12522403]]\n",
      "Epoch # 1451, Training Loss = [[0.61944833]], Testing Loss = [[3.12050313]]\n",
      "Epoch # 1452, Training Loss = [[0.61787472]], Testing Loss = [[3.11579145]]\n",
      "Epoch # 1453, Training Loss = [[0.6163065]], Testing Loss = [[3.11108895]]\n",
      "Epoch # 1454, Training Loss = [[0.61474366]], Testing Loss = [[3.10639561]]\n",
      "Epoch # 1455, Training Loss = [[0.61318618]], Testing Loss = [[3.1017114]]\n",
      "Epoch # 1456, Training Loss = [[0.61163404]], Testing Loss = [[3.09703631]]\n",
      "Epoch # 1457, Training Loss = [[0.61008722]], Testing Loss = [[3.0923703]]\n",
      "Epoch # 1458, Training Loss = [[0.6085457]], Testing Loss = [[3.08771336]]\n",
      "Epoch # 1459, Training Loss = [[0.60700945]], Testing Loss = [[3.08306546]]\n",
      "Epoch # 1460, Training Loss = [[0.60547846]], Testing Loss = [[3.07842657]]\n",
      "Epoch # 1461, Training Loss = [[0.60395271]], Testing Loss = [[3.07379668]]\n",
      "Epoch # 1462, Training Loss = [[0.60243217]], Testing Loss = [[3.06917576]]\n",
      "Epoch # 1463, Training Loss = [[0.60091683]], Testing Loss = [[3.06456378]]\n",
      "Epoch # 1464, Training Loss = [[0.59940667]], Testing Loss = [[3.05996073]]\n",
      "Epoch # 1465, Training Loss = [[0.59790167]], Testing Loss = [[3.05536657]]\n",
      "Epoch # 1466, Training Loss = [[0.5964018]], Testing Loss = [[3.0507813]]\n",
      "Epoch # 1467, Training Loss = [[0.59490706]], Testing Loss = [[3.04620487]]\n",
      "Epoch # 1468, Training Loss = [[0.59341741]], Testing Loss = [[3.04163728]]\n",
      "Epoch # 1469, Training Loss = [[0.59193284]], Testing Loss = [[3.0370785]]\n",
      "Epoch # 1470, Training Loss = [[0.59045333]], Testing Loss = [[3.0325285]]\n",
      "Epoch # 1471, Training Loss = [[0.58897886]], Testing Loss = [[3.02798726]]\n",
      "Epoch # 1472, Training Loss = [[0.58750941]], Testing Loss = [[3.02345476]]\n",
      "Epoch # 1473, Training Loss = [[0.58604497]], Testing Loss = [[3.01893098]]\n",
      "Epoch # 1474, Training Loss = [[0.58458551]], Testing Loss = [[3.01441589]]\n",
      "Epoch # 1475, Training Loss = [[0.58313101]], Testing Loss = [[3.00990948]]\n",
      "Epoch # 1476, Training Loss = [[0.58168146]], Testing Loss = [[3.00541171]]\n",
      "Epoch # 1477, Training Loss = [[0.58023683]], Testing Loss = [[3.00092257]]\n",
      "Epoch # 1478, Training Loss = [[0.57879711]], Testing Loss = [[2.99644204]]\n",
      "Epoch # 1479, Training Loss = [[0.57736228]], Testing Loss = [[2.99197009]]\n",
      "Epoch # 1480, Training Loss = [[0.57593232]], Testing Loss = [[2.9875067]]\n",
      "Epoch # 1481, Training Loss = [[0.57450722]], Testing Loss = [[2.98305185]]\n",
      "Epoch # 1482, Training Loss = [[0.57308694]], Testing Loss = [[2.97860551]]\n",
      "Epoch # 1483, Training Loss = [[0.57167148]], Testing Loss = [[2.97416767]]\n",
      "Epoch # 1484, Training Loss = [[0.57026082]], Testing Loss = [[2.96973831]]\n",
      "Epoch # 1485, Training Loss = [[0.56885494]], Testing Loss = [[2.96531739]]\n",
      "Epoch # 1486, Training Loss = [[0.56745381]], Testing Loss = [[2.96090491]]\n",
      "Epoch # 1487, Training Loss = [[0.56605743]], Testing Loss = [[2.95650083]]\n",
      "Epoch # 1488, Training Loss = [[0.56466577]], Testing Loss = [[2.95210514]]\n",
      "Epoch # 1489, Training Loss = [[0.56327882]], Testing Loss = [[2.94771781]]\n",
      "Epoch # 1490, Training Loss = [[0.56189655]], Testing Loss = [[2.94333883]]\n",
      "Epoch # 1491, Training Loss = [[0.56051896]], Testing Loss = [[2.93896817]]\n",
      "Epoch # 1492, Training Loss = [[0.55914602]], Testing Loss = [[2.93460581]]\n",
      "Epoch # 1493, Training Loss = [[0.55777771]], Testing Loss = [[2.93025173]]\n",
      "Epoch # 1494, Training Loss = [[0.55641402]], Testing Loss = [[2.92590591]]\n",
      "Epoch # 1495, Training Loss = [[0.55505494]], Testing Loss = [[2.92156833]]\n",
      "Epoch # 1496, Training Loss = [[0.55370043]], Testing Loss = [[2.91723896]]\n",
      "Epoch # 1497, Training Loss = [[0.55235049]], Testing Loss = [[2.9129178]]\n",
      "Epoch # 1498, Training Loss = [[0.55100509]], Testing Loss = [[2.9086048]]\n",
      "Epoch # 1499, Training Loss = [[0.54966423]], Testing Loss = [[2.90429996]]\n",
      "Epoch # 1500, Training Loss = [[0.54832788]], Testing Loss = [[2.90000326]]\n",
      "Epoch # 1501, Training Loss = [[0.54699602]], Testing Loss = [[2.89571466]]\n",
      "Epoch # 1502, Training Loss = [[0.54566865]], Testing Loss = [[2.89143416]]\n",
      "Epoch # 1503, Training Loss = [[0.54434573]], Testing Loss = [[2.88716174]]\n",
      "Epoch # 1504, Training Loss = [[0.54302726]], Testing Loss = [[2.88289736]]\n",
      "Epoch # 1505, Training Loss = [[0.54171322]], Testing Loss = [[2.87864101]]\n",
      "Epoch # 1506, Training Loss = [[0.54040359]], Testing Loss = [[2.87439268]]\n",
      "Epoch # 1507, Training Loss = [[0.53909836]], Testing Loss = [[2.87015234]]\n",
      "Epoch # 1508, Training Loss = [[0.5377975]], Testing Loss = [[2.86591997]]\n",
      "Epoch # 1509, Training Loss = [[0.53650101]], Testing Loss = [[2.86169554]]\n",
      "Epoch # 1510, Training Loss = [[0.53520886]], Testing Loss = [[2.85747905]]\n",
      "Epoch # 1511, Training Loss = [[0.53392104]], Testing Loss = [[2.85327047]]\n",
      "Epoch # 1512, Training Loss = [[0.53263753]], Testing Loss = [[2.84906978]]\n",
      "Epoch # 1513, Training Loss = [[0.53135831]], Testing Loss = [[2.84487696]]\n",
      "Epoch # 1514, Training Loss = [[0.53008338]], Testing Loss = [[2.84069199]]\n",
      "Epoch # 1515, Training Loss = [[0.52881271]], Testing Loss = [[2.83651486]]\n",
      "Epoch # 1516, Training Loss = [[0.52754629]], Testing Loss = [[2.83234553]]\n",
      "Epoch # 1517, Training Loss = [[0.52628409]], Testing Loss = [[2.828184]]\n",
      "Epoch # 1518, Training Loss = [[0.52502612]], Testing Loss = [[2.82403024]]\n",
      "Epoch # 1519, Training Loss = [[0.52377234]], Testing Loss = [[2.81988423]]\n",
      "Epoch # 1520, Training Loss = [[0.52252274]], Testing Loss = [[2.81574596]]\n",
      "Epoch # 1521, Training Loss = [[0.52127732]], Testing Loss = [[2.8116154]]\n",
      "Epoch # 1522, Training Loss = [[0.52003604]], Testing Loss = [[2.80749253]]\n",
      "Epoch # 1523, Training Loss = [[0.51879891]], Testing Loss = [[2.80337735]]\n",
      "Epoch # 1524, Training Loss = [[0.51756589]], Testing Loss = [[2.79926982]]\n",
      "Epoch # 1525, Training Loss = [[0.51633697]], Testing Loss = [[2.79516992]]\n",
      "Epoch # 1526, Training Loss = [[0.51511215]], Testing Loss = [[2.79107765]]\n",
      "Epoch # 1527, Training Loss = [[0.5138914]], Testing Loss = [[2.78699297]]\n",
      "Epoch # 1528, Training Loss = [[0.51267471]], Testing Loss = [[2.78291588]]\n",
      "Epoch # 1529, Training Loss = [[0.51146206]], Testing Loss = [[2.77884635]]\n",
      "Epoch # 1530, Training Loss = [[0.51025344]], Testing Loss = [[2.77478436]]\n",
      "Epoch # 1531, Training Loss = [[0.50904884]], Testing Loss = [[2.7707299]]\n",
      "Epoch # 1532, Training Loss = [[0.50784823]], Testing Loss = [[2.76668294]]\n",
      "Epoch # 1533, Training Loss = [[0.5066516]], Testing Loss = [[2.76264347]]\n",
      "Epoch # 1534, Training Loss = [[0.50545895]], Testing Loss = [[2.75861146]]\n",
      "Epoch # 1535, Training Loss = [[0.50427024]], Testing Loss = [[2.75458691]]\n",
      "Epoch # 1536, Training Loss = [[0.50308547]], Testing Loss = [[2.75056979]]\n",
      "Epoch # 1537, Training Loss = [[0.50190463]], Testing Loss = [[2.74656009]]\n",
      "Epoch # 1538, Training Loss = [[0.5007277]], Testing Loss = [[2.74255778]]\n",
      "Epoch # 1539, Training Loss = [[0.49955465]], Testing Loss = [[2.73856284]]\n",
      "Epoch # 1540, Training Loss = [[0.49838549]], Testing Loss = [[2.73457527]]\n",
      "Epoch # 1541, Training Loss = [[0.49722019]], Testing Loss = [[2.73059503]]\n",
      "Epoch # 1542, Training Loss = [[0.49605874]], Testing Loss = [[2.72662212]]\n",
      "Epoch # 1543, Training Loss = [[0.49490113]], Testing Loss = [[2.72265651]]\n",
      "Epoch # 1544, Training Loss = [[0.49374734]], Testing Loss = [[2.71869819]]\n",
      "Epoch # 1545, Training Loss = [[0.49259735]], Testing Loss = [[2.71474714]]\n",
      "Epoch # 1546, Training Loss = [[0.49145116]], Testing Loss = [[2.71080334]]\n",
      "Epoch # 1547, Training Loss = [[0.49030874]], Testing Loss = [[2.70686677]]\n",
      "Epoch # 1548, Training Loss = [[0.48917009]], Testing Loss = [[2.70293742]]\n",
      "Epoch # 1549, Training Loss = [[0.48803519]], Testing Loss = [[2.69901527]]\n",
      "Epoch # 1550, Training Loss = [[0.48690402]], Testing Loss = [[2.69510029]]\n",
      "Epoch # 1551, Training Loss = [[0.48577658]], Testing Loss = [[2.69119248]]\n",
      "Epoch # 1552, Training Loss = [[0.48465284]], Testing Loss = [[2.68729181]]\n",
      "Epoch # 1553, Training Loss = [[0.4835328]], Testing Loss = [[2.68339827]]\n",
      "Epoch # 1554, Training Loss = [[0.48241644]], Testing Loss = [[2.67951185]]\n",
      "Epoch # 1555, Training Loss = [[0.48130374]], Testing Loss = [[2.67563251]]\n",
      "Epoch # 1556, Training Loss = [[0.48019469]], Testing Loss = [[2.67176025]]\n",
      "Epoch # 1557, Training Loss = [[0.47908929]], Testing Loss = [[2.66789505]]\n",
      "Epoch # 1558, Training Loss = [[0.4779875]], Testing Loss = [[2.66403689]]\n",
      "Epoch # 1559, Training Loss = [[0.47688933]], Testing Loss = [[2.66018575]]\n",
      "Epoch # 1560, Training Loss = [[0.47579476]], Testing Loss = [[2.65634162]]\n",
      "Epoch # 1561, Training Loss = [[0.47470377]], Testing Loss = [[2.65250448]]\n",
      "Epoch # 1562, Training Loss = [[0.47361636]], Testing Loss = [[2.64867432]]\n",
      "Epoch # 1563, Training Loss = [[0.4725325]], Testing Loss = [[2.64485111]]\n",
      "Epoch # 1564, Training Loss = [[0.47145218]], Testing Loss = [[2.64103483]]\n",
      "Epoch # 1565, Training Loss = [[0.4703754]], Testing Loss = [[2.63722549]]\n",
      "Epoch # 1566, Training Loss = [[0.46930213]], Testing Loss = [[2.63342304]]\n",
      "Epoch # 1567, Training Loss = [[0.46823237]], Testing Loss = [[2.62962749]]\n",
      "Epoch # 1568, Training Loss = [[0.4671661]], Testing Loss = [[2.62583881]]\n",
      "Epoch # 1569, Training Loss = [[0.46610331]], Testing Loss = [[2.62205699]]\n",
      "Epoch # 1570, Training Loss = [[0.46504398]], Testing Loss = [[2.618282]]\n",
      "Epoch # 1571, Training Loss = [[0.46398811]], Testing Loss = [[2.61451384]]\n",
      "Epoch # 1572, Training Loss = [[0.46293568]], Testing Loss = [[2.61075248]]\n",
      "Epoch # 1573, Training Loss = [[0.46188667]], Testing Loss = [[2.60699792]]\n",
      "Epoch # 1574, Training Loss = [[0.46084108]], Testing Loss = [[2.60325012]]\n",
      "Epoch # 1575, Training Loss = [[0.45979888]], Testing Loss = [[2.59950909]]\n",
      "Epoch # 1576, Training Loss = [[0.45876008]], Testing Loss = [[2.59577479]]\n",
      "Epoch # 1577, Training Loss = [[0.45772465]], Testing Loss = [[2.59204723]]\n",
      "Epoch # 1578, Training Loss = [[0.45669259]], Testing Loss = [[2.58832637]]\n",
      "Epoch # 1579, Training Loss = [[0.45566387]], Testing Loss = [[2.5846122]]\n",
      "Epoch # 1580, Training Loss = [[0.4546385]], Testing Loss = [[2.58090471]]\n",
      "Epoch # 1581, Training Loss = [[0.45361645]], Testing Loss = [[2.57720388]]\n",
      "Epoch # 1582, Training Loss = [[0.45259771]], Testing Loss = [[2.5735097]]\n",
      "Epoch # 1583, Training Loss = [[0.45158228]], Testing Loss = [[2.56982214]]\n",
      "Epoch # 1584, Training Loss = [[0.45057013]], Testing Loss = [[2.5661412]]\n",
      "Epoch # 1585, Training Loss = [[0.44956126]], Testing Loss = [[2.56246686]]\n",
      "Epoch # 1586, Training Loss = [[0.44855566]], Testing Loss = [[2.5587991]]\n",
      "Epoch # 1587, Training Loss = [[0.4475533]], Testing Loss = [[2.5551379]]\n",
      "Epoch # 1588, Training Loss = [[0.44655419]], Testing Loss = [[2.55148325]]\n",
      "Epoch # 1589, Training Loss = [[0.4455583]], Testing Loss = [[2.54783514]]\n",
      "Epoch # 1590, Training Loss = [[0.44456563]], Testing Loss = [[2.54419355]]\n",
      "Epoch # 1591, Training Loss = [[0.44357617]], Testing Loss = [[2.54055846]]\n",
      "Epoch # 1592, Training Loss = [[0.4425899]], Testing Loss = [[2.53692986]]\n",
      "Epoch # 1593, Training Loss = [[0.4416068]], Testing Loss = [[2.53330774]]\n",
      "Epoch # 1594, Training Loss = [[0.44062688]], Testing Loss = [[2.52969207]]\n",
      "Epoch # 1595, Training Loss = [[0.43965011]], Testing Loss = [[2.52608285]]\n",
      "Epoch # 1596, Training Loss = [[0.43867649]], Testing Loss = [[2.52248005]]\n",
      "Epoch # 1597, Training Loss = [[0.437706]], Testing Loss = [[2.51888366]]\n",
      "Epoch # 1598, Training Loss = [[0.43673863]], Testing Loss = [[2.51529367]]\n",
      "Epoch # 1599, Training Loss = [[0.43577437]], Testing Loss = [[2.51171006]]\n",
      "Epoch # 1600, Training Loss = [[0.43481321]], Testing Loss = [[2.50813282]]\n",
      "Epoch # 1601, Training Loss = [[0.43385513]], Testing Loss = [[2.50456193]]\n",
      "Epoch # 1602, Training Loss = [[0.43290014]], Testing Loss = [[2.50099738]]\n",
      "Epoch # 1603, Training Loss = [[0.4319482]], Testing Loss = [[2.49743915]]\n",
      "Epoch # 1604, Training Loss = [[0.43099932]], Testing Loss = [[2.49388722]]\n",
      "Epoch # 1605, Training Loss = [[0.43005348]], Testing Loss = [[2.49034159]]\n",
      "Epoch # 1606, Training Loss = [[0.42911067]], Testing Loss = [[2.48680224]]\n",
      "Epoch # 1607, Training Loss = [[0.42817088]], Testing Loss = [[2.48326915]]\n",
      "Epoch # 1608, Training Loss = [[0.4272341]], Testing Loss = [[2.4797423]]\n",
      "Epoch # 1609, Training Loss = [[0.42630031]], Testing Loss = [[2.47622169]]\n",
      "Epoch # 1610, Training Loss = [[0.42536951]], Testing Loss = [[2.4727073]]\n",
      "Epoch # 1611, Training Loss = [[0.42444168]], Testing Loss = [[2.46919911]]\n",
      "Epoch # 1612, Training Loss = [[0.42351682]], Testing Loss = [[2.46569711]]\n",
      "Epoch # 1613, Training Loss = [[0.42259491]], Testing Loss = [[2.46220129]]\n",
      "Epoch # 1614, Training Loss = [[0.42167594]], Testing Loss = [[2.45871163]]\n",
      "Epoch # 1615, Training Loss = [[0.4207599]], Testing Loss = [[2.45522811]]\n",
      "Epoch # 1616, Training Loss = [[0.41984678]], Testing Loss = [[2.45175073]]\n",
      "Epoch # 1617, Training Loss = [[0.41893657]], Testing Loss = [[2.44827946]]\n",
      "Epoch # 1618, Training Loss = [[0.41802926]], Testing Loss = [[2.4448143]]\n",
      "Epoch # 1619, Training Loss = [[0.41712484]], Testing Loss = [[2.44135522]]\n",
      "Epoch # 1620, Training Loss = [[0.41622329]], Testing Loss = [[2.43790223]]\n",
      "Epoch # 1621, Training Loss = [[0.41532461]], Testing Loss = [[2.43445529]]\n",
      "Epoch # 1622, Training Loss = [[0.41442878]], Testing Loss = [[2.4310144]]\n",
      "Epoch # 1623, Training Loss = [[0.4135358]], Testing Loss = [[2.42757954]]\n",
      "Epoch # 1624, Training Loss = [[0.41264566]], Testing Loss = [[2.4241507]]\n",
      "Epoch # 1625, Training Loss = [[0.41175833]], Testing Loss = [[2.42072787]]\n",
      "Epoch # 1626, Training Loss = [[0.41087383]], Testing Loss = [[2.41731103]]\n",
      "Epoch # 1627, Training Loss = [[0.40999212]], Testing Loss = [[2.41390016]]\n",
      "Epoch # 1628, Training Loss = [[0.40911321]], Testing Loss = [[2.41049526]]\n",
      "Epoch # 1629, Training Loss = [[0.40823708]], Testing Loss = [[2.40709631]]\n",
      "Epoch # 1630, Training Loss = [[0.40736373]], Testing Loss = [[2.40370329]]\n",
      "Epoch # 1631, Training Loss = [[0.40649313]], Testing Loss = [[2.4003162]]\n",
      "Epoch # 1632, Training Loss = [[0.40562529]], Testing Loss = [[2.39693501]]\n",
      "Epoch # 1633, Training Loss = [[0.40476019]], Testing Loss = [[2.39355972]]\n",
      "Epoch # 1634, Training Loss = [[0.40389782]], Testing Loss = [[2.39019031]]\n",
      "Epoch # 1635, Training Loss = [[0.40303818]], Testing Loss = [[2.38682677]]\n",
      "Epoch # 1636, Training Loss = [[0.40218124]], Testing Loss = [[2.38346908]]\n",
      "Epoch # 1637, Training Loss = [[0.40132701]], Testing Loss = [[2.38011724]]\n",
      "Epoch # 1638, Training Loss = [[0.40047547]], Testing Loss = [[2.37677122]]\n",
      "Epoch # 1639, Training Loss = [[0.39962661]], Testing Loss = [[2.37343102]]\n",
      "Epoch # 1640, Training Loss = [[0.39878042]], Testing Loss = [[2.37009661]]\n",
      "Epoch # 1641, Training Loss = [[0.3979369]], Testing Loss = [[2.366768]]\n",
      "Epoch # 1642, Training Loss = [[0.39709603]], Testing Loss = [[2.36344516]]\n",
      "Epoch # 1643, Training Loss = [[0.3962578]], Testing Loss = [[2.36012808]]\n",
      "Epoch # 1644, Training Loss = [[0.3954222]], Testing Loss = [[2.35681674]]\n",
      "Epoch # 1645, Training Loss = [[0.39458923]], Testing Loss = [[2.35351115]]\n",
      "Epoch # 1646, Training Loss = [[0.39375887]], Testing Loss = [[2.35021127]]\n",
      "Epoch # 1647, Training Loss = [[0.39293112]], Testing Loss = [[2.3469171]]\n",
      "Epoch # 1648, Training Loss = [[0.39210596]], Testing Loss = [[2.34362863]]\n",
      "Epoch # 1649, Training Loss = [[0.39128338]], Testing Loss = [[2.34034585]]\n",
      "Epoch # 1650, Training Loss = [[0.39046339]], Testing Loss = [[2.33706873]]\n",
      "Epoch # 1651, Training Loss = [[0.38964595]], Testing Loss = [[2.33379727]]\n",
      "Epoch # 1652, Training Loss = [[0.38883108]], Testing Loss = [[2.33053145]]\n",
      "Epoch # 1653, Training Loss = [[0.38801875]], Testing Loss = [[2.32727126]]\n",
      "Epoch # 1654, Training Loss = [[0.38720896]], Testing Loss = [[2.3240167]]\n",
      "Epoch # 1655, Training Loss = [[0.3864017]], Testing Loss = [[2.32076774]]\n",
      "Epoch # 1656, Training Loss = [[0.38559696]], Testing Loss = [[2.31752437]]\n",
      "Epoch # 1657, Training Loss = [[0.38479472]], Testing Loss = [[2.31428658]]\n",
      "Epoch # 1658, Training Loss = [[0.38399499]], Testing Loss = [[2.31105436]]\n",
      "Epoch # 1659, Training Loss = [[0.38319776]], Testing Loss = [[2.30782769]]\n",
      "Epoch # 1660, Training Loss = [[0.382403]], Testing Loss = [[2.30460657]]\n",
      "Epoch # 1661, Training Loss = [[0.38161072]], Testing Loss = [[2.30139098]]\n",
      "Epoch # 1662, Training Loss = [[0.3808209]], Testing Loss = [[2.29818091]]\n",
      "Epoch # 1663, Training Loss = [[0.38003354]], Testing Loss = [[2.29497634]]\n",
      "Epoch # 1664, Training Loss = [[0.37924863]], Testing Loss = [[2.29177726]]\n",
      "Epoch # 1665, Training Loss = [[0.37846615]], Testing Loss = [[2.28858367]]\n",
      "Epoch # 1666, Training Loss = [[0.37768611]], Testing Loss = [[2.28539554]]\n",
      "Epoch # 1667, Training Loss = [[0.37690848]], Testing Loss = [[2.28221287]]\n",
      "Epoch # 1668, Training Loss = [[0.37613327]], Testing Loss = [[2.27903564]]\n",
      "Epoch # 1669, Training Loss = [[0.37536045]], Testing Loss = [[2.27586384]]\n",
      "Epoch # 1670, Training Loss = [[0.37459004]], Testing Loss = [[2.27269747]]\n",
      "Epoch # 1671, Training Loss = [[0.373822]], Testing Loss = [[2.2695365]]\n",
      "Epoch # 1672, Training Loss = [[0.37305634]], Testing Loss = [[2.26638092]]\n",
      "Epoch # 1673, Training Loss = [[0.37229305]], Testing Loss = [[2.26323073]]\n",
      "Epoch # 1674, Training Loss = [[0.37153212]], Testing Loss = [[2.26008591]]\n",
      "Epoch # 1675, Training Loss = [[0.37077354]], Testing Loss = [[2.25694645]]\n",
      "Epoch # 1676, Training Loss = [[0.37001731]], Testing Loss = [[2.25381234]]\n",
      "Epoch # 1677, Training Loss = [[0.3692634]], Testing Loss = [[2.25068356]]\n",
      "Epoch # 1678, Training Loss = [[0.36851182]], Testing Loss = [[2.2475601]]\n",
      "Epoch # 1679, Training Loss = [[0.36776256]], Testing Loss = [[2.24444196]]\n",
      "Epoch # 1680, Training Loss = [[0.3670156]], Testing Loss = [[2.24132911]]\n",
      "Epoch # 1681, Training Loss = [[0.36627095]], Testing Loss = [[2.23822155]]\n",
      "Epoch # 1682, Training Loss = [[0.36552858]], Testing Loss = [[2.23511927]]\n",
      "Epoch # 1683, Training Loss = [[0.3647885]], Testing Loss = [[2.23202225]]\n",
      "Epoch # 1684, Training Loss = [[0.3640507]], Testing Loss = [[2.22893049]]\n",
      "Epoch # 1685, Training Loss = [[0.36331516]], Testing Loss = [[2.22584397]]\n",
      "Epoch # 1686, Training Loss = [[0.36258187]], Testing Loss = [[2.22276267]]\n",
      "Epoch # 1687, Training Loss = [[0.36185084]], Testing Loss = [[2.21968659]]\n",
      "Epoch # 1688, Training Loss = [[0.36112205]], Testing Loss = [[2.21661572]]\n",
      "Epoch # 1689, Training Loss = [[0.3603955]], Testing Loss = [[2.21355005]]\n",
      "Epoch # 1690, Training Loss = [[0.35967117]], Testing Loss = [[2.21048956]]\n",
      "Epoch # 1691, Training Loss = [[0.35894905]], Testing Loss = [[2.20743423]]\n",
      "Epoch # 1692, Training Loss = [[0.35822915]], Testing Loss = [[2.20438407]]\n",
      "Epoch # 1693, Training Loss = [[0.35751145]], Testing Loss = [[2.20133906]]\n",
      "Epoch # 1694, Training Loss = [[0.35679594]], Testing Loss = [[2.19829919]]\n",
      "Epoch # 1695, Training Loss = [[0.35608262]], Testing Loss = [[2.19526444]]\n",
      "Epoch # 1696, Training Loss = [[0.35537147]], Testing Loss = [[2.19223481]]\n",
      "Epoch # 1697, Training Loss = [[0.3546625]], Testing Loss = [[2.18921028]]\n",
      "Epoch # 1698, Training Loss = [[0.35395568]], Testing Loss = [[2.18619084]]\n",
      "Epoch # 1699, Training Loss = [[0.35325102]], Testing Loss = [[2.18317649]]\n",
      "Epoch # 1700, Training Loss = [[0.35254851]], Testing Loss = [[2.18016721]]\n",
      "Epoch # 1701, Training Loss = [[0.35184814]], Testing Loss = [[2.17716298]]\n",
      "Epoch # 1702, Training Loss = [[0.35114989]], Testing Loss = [[2.1741638]]\n",
      "Epoch # 1703, Training Loss = [[0.35045377]], Testing Loss = [[2.17116966]]\n",
      "Epoch # 1704, Training Loss = [[0.34975977]], Testing Loss = [[2.16818055]]\n",
      "Epoch # 1705, Training Loss = [[0.34906787]], Testing Loss = [[2.16519645]]\n",
      "Epoch # 1706, Training Loss = [[0.34837808]], Testing Loss = [[2.16221735]]\n",
      "Epoch # 1707, Training Loss = [[0.34769037]], Testing Loss = [[2.15924325]]\n",
      "Epoch # 1708, Training Loss = [[0.34700475]], Testing Loss = [[2.15627413]]\n",
      "Epoch # 1709, Training Loss = [[0.34632121]], Testing Loss = [[2.15330998]]\n",
      "Epoch # 1710, Training Loss = [[0.34563974]], Testing Loss = [[2.1503508]]\n",
      "Epoch # 1711, Training Loss = [[0.34496034]], Testing Loss = [[2.14739656]]\n",
      "Epoch # 1712, Training Loss = [[0.34428298]], Testing Loss = [[2.14444726]]\n",
      "Epoch # 1713, Training Loss = [[0.34360768]], Testing Loss = [[2.14150289]]\n",
      "Epoch # 1714, Training Loss = [[0.34293441]], Testing Loss = [[2.13856344]]\n",
      "Epoch # 1715, Training Loss = [[0.34226318]], Testing Loss = [[2.13562889]]\n",
      "Epoch # 1716, Training Loss = [[0.34159398]], Testing Loss = [[2.13269924]]\n",
      "Epoch # 1717, Training Loss = [[0.34092679]], Testing Loss = [[2.12977448]]\n",
      "Epoch # 1718, Training Loss = [[0.34026162]], Testing Loss = [[2.12685459]]\n",
      "Epoch # 1719, Training Loss = [[0.33959844]], Testing Loss = [[2.12393956]]\n",
      "Epoch # 1720, Training Loss = [[0.33893727]], Testing Loss = [[2.12102939]]\n",
      "Epoch # 1721, Training Loss = [[0.33827809]], Testing Loss = [[2.11812407]]\n",
      "Epoch # 1722, Training Loss = [[0.33762088]], Testing Loss = [[2.11522357]]\n",
      "Epoch # 1723, Training Loss = [[0.33696566]], Testing Loss = [[2.1123279]]\n",
      "Epoch # 1724, Training Loss = [[0.3363124]], Testing Loss = [[2.10943704]]\n",
      "Epoch # 1725, Training Loss = [[0.3356611]], Testing Loss = [[2.10655098]]\n",
      "Epoch # 1726, Training Loss = [[0.33501176]], Testing Loss = [[2.10366972]]\n",
      "Epoch # 1727, Training Loss = [[0.33436436]], Testing Loss = [[2.10079323]]\n",
      "Epoch # 1728, Training Loss = [[0.33371891]], Testing Loss = [[2.09792152]]\n",
      "Epoch # 1729, Training Loss = [[0.33307539]], Testing Loss = [[2.09505457]]\n",
      "Epoch # 1730, Training Loss = [[0.33243379]], Testing Loss = [[2.09219236]]\n",
      "Epoch # 1731, Training Loss = [[0.33179411]], Testing Loss = [[2.0893349]]\n",
      "Epoch # 1732, Training Loss = [[0.33115635]], Testing Loss = [[2.08648217]]\n",
      "Epoch # 1733, Training Loss = [[0.33052049]], Testing Loss = [[2.08363416]]\n",
      "Epoch # 1734, Training Loss = [[0.32988653]], Testing Loss = [[2.08079086]]\n",
      "Epoch # 1735, Training Loss = [[0.32925447]], Testing Loss = [[2.07795225]]\n",
      "Epoch # 1736, Training Loss = [[0.32862429]], Testing Loss = [[2.07511834]]\n",
      "Epoch # 1737, Training Loss = [[0.32799598]], Testing Loss = [[2.07228911]]\n",
      "Epoch # 1738, Training Loss = [[0.32736955]], Testing Loss = [[2.06946454]]\n",
      "Epoch # 1739, Training Loss = [[0.32674499]], Testing Loss = [[2.06664464]]\n",
      "Epoch # 1740, Training Loss = [[0.32612228]], Testing Loss = [[2.06382939]]\n",
      "Epoch # 1741, Training Loss = [[0.32550142]], Testing Loss = [[2.06101877]]\n",
      "Epoch # 1742, Training Loss = [[0.32488242]], Testing Loss = [[2.05821279]]\n",
      "Epoch # 1743, Training Loss = [[0.32426524]], Testing Loss = [[2.05541143]]\n",
      "Epoch # 1744, Training Loss = [[0.32364991]], Testing Loss = [[2.05261467]]\n",
      "Epoch # 1745, Training Loss = [[0.32303639]], Testing Loss = [[2.04982252]]\n",
      "Epoch # 1746, Training Loss = [[0.3224247]], Testing Loss = [[2.04703496]]\n",
      "Epoch # 1747, Training Loss = [[0.32181482]], Testing Loss = [[2.04425198]]\n",
      "Epoch # 1748, Training Loss = [[0.32120674]], Testing Loss = [[2.04147357]]\n",
      "Epoch # 1749, Training Loss = [[0.32060047]], Testing Loss = [[2.03869972]]\n",
      "Epoch # 1750, Training Loss = [[0.31999599]], Testing Loss = [[2.03593043]]\n",
      "Epoch # 1751, Training Loss = [[0.31939329]], Testing Loss = [[2.03316567]]\n",
      "Epoch # 1752, Training Loss = [[0.31879238]], Testing Loss = [[2.03040545]]\n",
      "Epoch # 1753, Training Loss = [[0.31819324]], Testing Loss = [[2.02764976]]\n",
      "Epoch # 1754, Training Loss = [[0.31759587]], Testing Loss = [[2.02489857]]\n",
      "Epoch # 1755, Training Loss = [[0.31700026]], Testing Loss = [[2.02215189]]\n",
      "Epoch # 1756, Training Loss = [[0.3164064]], Testing Loss = [[2.01940971]]\n",
      "Epoch # 1757, Training Loss = [[0.3158143]], Testing Loss = [[2.01667201]]\n",
      "Epoch # 1758, Training Loss = [[0.31522393]], Testing Loss = [[2.01393879]]\n",
      "Epoch # 1759, Training Loss = [[0.31463531]], Testing Loss = [[2.01121003]]\n",
      "Epoch # 1760, Training Loss = [[0.31404841]], Testing Loss = [[2.00848573]]\n",
      "Epoch # 1761, Training Loss = [[0.31346324]], Testing Loss = [[2.00576588]]\n",
      "Epoch # 1762, Training Loss = [[0.31287979]], Testing Loss = [[2.00305046]]\n",
      "Epoch # 1763, Training Loss = [[0.31229805]], Testing Loss = [[2.00033948]]\n",
      "Epoch # 1764, Training Loss = [[0.31171802]], Testing Loss = [[1.99763291]]\n",
      "Epoch # 1765, Training Loss = [[0.31113968]], Testing Loss = [[1.99493076]]\n",
      "Epoch # 1766, Training Loss = [[0.31056305]], Testing Loss = [[1.992233]]\n",
      "Epoch # 1767, Training Loss = [[0.30998809]], Testing Loss = [[1.98953964]]\n",
      "Epoch # 1768, Training Loss = [[0.30941483]], Testing Loss = [[1.98685066]]\n",
      "Epoch # 1769, Training Loss = [[0.30884323]], Testing Loss = [[1.98416605]]\n",
      "Epoch # 1770, Training Loss = [[0.30827331]], Testing Loss = [[1.98148581]]\n",
      "Epoch # 1771, Training Loss = [[0.30770505]], Testing Loss = [[1.97880993]]\n",
      "Epoch # 1772, Training Loss = [[0.30713845]], Testing Loss = [[1.97613839]]\n",
      "Epoch # 1773, Training Loss = [[0.30657351]], Testing Loss = [[1.97347118]]\n",
      "Epoch # 1774, Training Loss = [[0.30601021]], Testing Loss = [[1.97080831]]\n",
      "Epoch # 1775, Training Loss = [[0.30544855]], Testing Loss = [[1.96814976]]\n",
      "Epoch # 1776, Training Loss = [[0.30488853]], Testing Loss = [[1.96549551]]\n",
      "Epoch # 1777, Training Loss = [[0.30433013]], Testing Loss = [[1.96284557]]\n",
      "Epoch # 1778, Training Loss = [[0.30377336]], Testing Loss = [[1.96019992]]\n",
      "Epoch # 1779, Training Loss = [[0.30321821]], Testing Loss = [[1.95755855]]\n",
      "Epoch # 1780, Training Loss = [[0.30266467]], Testing Loss = [[1.95492146]]\n",
      "Epoch # 1781, Training Loss = [[0.30211274]], Testing Loss = [[1.95228863]]\n",
      "Epoch # 1782, Training Loss = [[0.30156241]], Testing Loss = [[1.94966006]]\n",
      "Epoch # 1783, Training Loss = [[0.30101367]], Testing Loss = [[1.94703574]]\n",
      "Epoch # 1784, Training Loss = [[0.30046652]], Testing Loss = [[1.94441566]]\n",
      "Epoch # 1785, Training Loss = [[0.29992096]], Testing Loss = [[1.9417998]]\n",
      "Epoch # 1786, Training Loss = [[0.29937697]], Testing Loss = [[1.93918817]]\n",
      "Epoch # 1787, Training Loss = [[0.29883456]], Testing Loss = [[1.93658076]]\n",
      "Epoch # 1788, Training Loss = [[0.29829372]], Testing Loss = [[1.93397754]]\n",
      "Epoch # 1789, Training Loss = [[0.29775444]], Testing Loss = [[1.93137853]]\n",
      "Epoch # 1790, Training Loss = [[0.29721671]], Testing Loss = [[1.9287837]]\n",
      "Epoch # 1791, Training Loss = [[0.29668054]], Testing Loss = [[1.92619305]]\n",
      "Epoch # 1792, Training Loss = [[0.29614591]], Testing Loss = [[1.92360657]]\n",
      "Epoch # 1793, Training Loss = [[0.29561282]], Testing Loss = [[1.92102425]]\n",
      "Epoch # 1794, Training Loss = [[0.29508126]], Testing Loss = [[1.91844608]]\n",
      "Epoch # 1795, Training Loss = [[0.29455124]], Testing Loss = [[1.91587206]]\n",
      "Epoch # 1796, Training Loss = [[0.29402274]], Testing Loss = [[1.91330217]]\n",
      "Epoch # 1797, Training Loss = [[0.29349575]], Testing Loss = [[1.91073641]]\n",
      "Epoch # 1798, Training Loss = [[0.29297029]], Testing Loss = [[1.90817477]]\n",
      "Epoch # 1799, Training Loss = [[0.29244632]], Testing Loss = [[1.90561724]]\n",
      "Epoch # 1800, Training Loss = [[0.29192387]], Testing Loss = [[1.90306381]]\n",
      "Epoch # 1801, Training Loss = [[0.2914029]], Testing Loss = [[1.90051448]]\n",
      "Epoch # 1802, Training Loss = [[0.29088344]], Testing Loss = [[1.89796923]]\n",
      "Epoch # 1803, Training Loss = [[0.29036545]], Testing Loss = [[1.89542806]]\n",
      "Epoch # 1804, Training Loss = [[0.28984895]], Testing Loss = [[1.89289095]]\n",
      "Epoch # 1805, Training Loss = [[0.28933393]], Testing Loss = [[1.89035791]]\n",
      "Epoch # 1806, Training Loss = [[0.28882038]], Testing Loss = [[1.88782892]]\n",
      "Epoch # 1807, Training Loss = [[0.28830829]], Testing Loss = [[1.88530397]]\n",
      "Epoch # 1808, Training Loss = [[0.28779767]], Testing Loss = [[1.88278306]]\n",
      "Epoch # 1809, Training Loss = [[0.2872885]], Testing Loss = [[1.88026618]]\n",
      "Epoch # 1810, Training Loss = [[0.28678079]], Testing Loss = [[1.87775331]]\n",
      "Epoch # 1811, Training Loss = [[0.28627451]], Testing Loss = [[1.87524446]]\n",
      "Epoch # 1812, Training Loss = [[0.28576968]], Testing Loss = [[1.87273961]]\n",
      "Epoch # 1813, Training Loss = [[0.28526629]], Testing Loss = [[1.87023875]]\n",
      "Epoch # 1814, Training Loss = [[0.28476433]], Testing Loss = [[1.86774188]]\n",
      "Epoch # 1815, Training Loss = [[0.28426379]], Testing Loss = [[1.86524899]]\n",
      "Epoch # 1816, Training Loss = [[0.28376467]], Testing Loss = [[1.86276007]]\n",
      "Epoch # 1817, Training Loss = [[0.28326697]], Testing Loss = [[1.86027512]]\n",
      "Epoch # 1818, Training Loss = [[0.28277069]], Testing Loss = [[1.85779412]]\n",
      "Epoch # 1819, Training Loss = [[0.2822758]], Testing Loss = [[1.85531706]]\n",
      "Epoch # 1820, Training Loss = [[0.28178232]], Testing Loss = [[1.85284394]]\n",
      "Epoch # 1821, Training Loss = [[0.28129024]], Testing Loss = [[1.85037476]]\n",
      "Epoch # 1822, Training Loss = [[0.28079954]], Testing Loss = [[1.8479095]]\n",
      "Epoch # 1823, Training Loss = [[0.28031024]], Testing Loss = [[1.84544815]]\n",
      "Epoch # 1824, Training Loss = [[0.27982231]], Testing Loss = [[1.84299071]]\n",
      "Epoch # 1825, Training Loss = [[0.27933576]], Testing Loss = [[1.84053716]]\n",
      "Epoch # 1826, Training Loss = [[0.27885059]], Testing Loss = [[1.83808751]]\n",
      "Epoch # 1827, Training Loss = [[0.27836678]], Testing Loss = [[1.83564175]]\n",
      "Epoch # 1828, Training Loss = [[0.27788434]], Testing Loss = [[1.83319986]]\n",
      "Epoch # 1829, Training Loss = [[0.27740325]], Testing Loss = [[1.83076183]]\n",
      "Epoch # 1830, Training Loss = [[0.27692352]], Testing Loss = [[1.82832767]]\n",
      "Epoch # 1831, Training Loss = [[0.27644513]], Testing Loss = [[1.82589736]]\n",
      "Epoch # 1832, Training Loss = [[0.27596809]], Testing Loss = [[1.8234709]]\n",
      "Epoch # 1833, Training Loss = [[0.27549239]], Testing Loss = [[1.82104827]]\n",
      "Epoch # 1834, Training Loss = [[0.27501803]], Testing Loss = [[1.81862947]]\n",
      "Epoch # 1835, Training Loss = [[0.27454499]], Testing Loss = [[1.81621449]]\n",
      "Epoch # 1836, Training Loss = [[0.27407328]], Testing Loss = [[1.81380333]]\n",
      "Epoch # 1837, Training Loss = [[0.27360289]], Testing Loss = [[1.81139597]]\n",
      "Epoch # 1838, Training Loss = [[0.27313382]], Testing Loss = [[1.80899242]]\n",
      "Epoch # 1839, Training Loss = [[0.27266606]], Testing Loss = [[1.80659265]]\n",
      "Epoch # 1840, Training Loss = [[0.27219961]], Testing Loss = [[1.80419667]]\n",
      "Epoch # 1841, Training Loss = [[0.27173445]], Testing Loss = [[1.80180447]]\n",
      "Epoch # 1842, Training Loss = [[0.2712706]], Testing Loss = [[1.79941603]]\n",
      "Epoch # 1843, Training Loss = [[0.27080804]], Testing Loss = [[1.79703135]]\n",
      "Epoch # 1844, Training Loss = [[0.27034678]], Testing Loss = [[1.79465043]]\n",
      "Epoch # 1845, Training Loss = [[0.26988679]], Testing Loss = [[1.79227326]]\n",
      "Epoch # 1846, Training Loss = [[0.26942809]], Testing Loss = [[1.78989982]]\n",
      "Epoch # 1847, Training Loss = [[0.26897066]], Testing Loss = [[1.78753012]]\n",
      "Epoch # 1848, Training Loss = [[0.26851451]], Testing Loss = [[1.78516414]]\n",
      "Epoch # 1849, Training Loss = [[0.26805962]], Testing Loss = [[1.78280188]]\n",
      "Epoch # 1850, Training Loss = [[0.267606]], Testing Loss = [[1.78044333]]\n",
      "Epoch # 1851, Training Loss = [[0.26715363]], Testing Loss = [[1.77808848]]\n",
      "Epoch # 1852, Training Loss = [[0.26670252]], Testing Loss = [[1.77573732]]\n",
      "Epoch # 1853, Training Loss = [[0.26625266]], Testing Loss = [[1.77338985]]\n",
      "Epoch # 1854, Training Loss = [[0.26580404]], Testing Loss = [[1.77104607]]\n",
      "Epoch # 1855, Training Loss = [[0.26535667]], Testing Loss = [[1.76870595]]\n",
      "Epoch # 1856, Training Loss = [[0.26491053]], Testing Loss = [[1.7663695]]\n",
      "Epoch # 1857, Training Loss = [[0.26446563]], Testing Loss = [[1.76403671]]\n",
      "Epoch # 1858, Training Loss = [[0.26402196]], Testing Loss = [[1.76170758]]\n",
      "Epoch # 1859, Training Loss = [[0.26357951]], Testing Loss = [[1.75938208]]\n",
      "Epoch # 1860, Training Loss = [[0.26313828]], Testing Loss = [[1.75706022]]\n",
      "Epoch # 1861, Training Loss = [[0.26269826]], Testing Loss = [[1.754742]]\n",
      "Epoch # 1862, Training Loss = [[0.26225946]], Testing Loss = [[1.75242739]]\n",
      "Epoch # 1863, Training Loss = [[0.26182187]], Testing Loss = [[1.75011641]]\n",
      "Epoch # 1864, Training Loss = [[0.26138548]], Testing Loss = [[1.74780903]]\n",
      "Epoch # 1865, Training Loss = [[0.26095029]], Testing Loss = [[1.74550525]]\n",
      "Epoch # 1866, Training Loss = [[0.2605163]], Testing Loss = [[1.74320507]]\n",
      "Epoch # 1867, Training Loss = [[0.26008349]], Testing Loss = [[1.74090847]]\n",
      "Epoch # 1868, Training Loss = [[0.25965188]], Testing Loss = [[1.73861546]]\n",
      "Epoch # 1869, Training Loss = [[0.25922145]], Testing Loss = [[1.73632601]]\n",
      "Epoch # 1870, Training Loss = [[0.25879219]], Testing Loss = [[1.73404014]]\n",
      "Epoch # 1871, Training Loss = [[0.25836412]], Testing Loss = [[1.73175782]]\n",
      "Epoch # 1872, Training Loss = [[0.25793721]], Testing Loss = [[1.72947906]]\n",
      "Epoch # 1873, Training Loss = [[0.25751147]], Testing Loss = [[1.72720385]]\n",
      "Epoch # 1874, Training Loss = [[0.2570869]], Testing Loss = [[1.72493217]]\n",
      "Epoch # 1875, Training Loss = [[0.25666348]], Testing Loss = [[1.72266402]]\n",
      "Epoch # 1876, Training Loss = [[0.25624122]], Testing Loss = [[1.7203994]]\n",
      "Epoch # 1877, Training Loss = [[0.25582011]], Testing Loss = [[1.7181383]]\n",
      "Epoch # 1878, Training Loss = [[0.25540015]], Testing Loss = [[1.71588071]]\n",
      "Epoch # 1879, Training Loss = [[0.25498134]], Testing Loss = [[1.71362662]]\n",
      "Epoch # 1880, Training Loss = [[0.25456366]], Testing Loss = [[1.71137604]]\n",
      "Epoch # 1881, Training Loss = [[0.25414712]], Testing Loss = [[1.70912894]]\n",
      "Epoch # 1882, Training Loss = [[0.25373171]], Testing Loss = [[1.70688533]]\n",
      "Epoch # 1883, Training Loss = [[0.25331743]], Testing Loss = [[1.70464519]]\n",
      "Epoch # 1884, Training Loss = [[0.25290427]], Testing Loss = [[1.70240853]]\n",
      "Epoch # 1885, Training Loss = [[0.25249224]], Testing Loss = [[1.70017533]]\n",
      "Epoch # 1886, Training Loss = [[0.25208132]], Testing Loss = [[1.69794559]]\n",
      "Epoch # 1887, Training Loss = [[0.25167151]], Testing Loss = [[1.69571929]]\n",
      "Epoch # 1888, Training Loss = [[0.25126281]], Testing Loss = [[1.69349645]]\n",
      "Epoch # 1889, Training Loss = [[0.25085522]], Testing Loss = [[1.69127704]]\n",
      "Epoch # 1890, Training Loss = [[0.25044873]], Testing Loss = [[1.68906106]]\n",
      "Epoch # 1891, Training Loss = [[0.25004334]], Testing Loss = [[1.6868485]]\n",
      "Epoch # 1892, Training Loss = [[0.24963904]], Testing Loss = [[1.68463937]]\n",
      "Epoch # 1893, Training Loss = [[0.24923584]], Testing Loss = [[1.68243364]]\n",
      "Epoch # 1894, Training Loss = [[0.24883372]], Testing Loss = [[1.68023132]]\n",
      "Epoch # 1895, Training Loss = [[0.24843268]], Testing Loss = [[1.6780324]]\n",
      "Epoch # 1896, Training Loss = [[0.24803272]], Testing Loss = [[1.67583687]]\n",
      "Epoch # 1897, Training Loss = [[0.24763384]], Testing Loss = [[1.67364472]]\n",
      "Epoch # 1898, Training Loss = [[0.24723604]], Testing Loss = [[1.67145595]]\n",
      "Epoch # 1899, Training Loss = [[0.2468393]], Testing Loss = [[1.66927056]]\n",
      "Epoch # 1900, Training Loss = [[0.24644362]], Testing Loss = [[1.66708853]]\n",
      "Epoch # 1901, Training Loss = [[0.24604901]], Testing Loss = [[1.66490986]]\n",
      "Epoch # 1902, Training Loss = [[0.24565545]], Testing Loss = [[1.66273454]]\n",
      "Epoch # 1903, Training Loss = [[0.24526295]], Testing Loss = [[1.66056257]]\n",
      "Epoch # 1904, Training Loss = [[0.24487151]], Testing Loss = [[1.65839393]]\n",
      "Epoch # 1905, Training Loss = [[0.2444811]], Testing Loss = [[1.65622864]]\n",
      "Epoch # 1906, Training Loss = [[0.24409174]], Testing Loss = [[1.65406667]]\n",
      "Epoch # 1907, Training Loss = [[0.24370343]], Testing Loss = [[1.65190802]]\n",
      "Epoch # 1908, Training Loss = [[0.24331615]], Testing Loss = [[1.64975268]]\n",
      "Epoch # 1909, Training Loss = [[0.2429299]], Testing Loss = [[1.64760065]]\n",
      "Epoch # 1910, Training Loss = [[0.24254468]], Testing Loss = [[1.64545193]]\n",
      "Epoch # 1911, Training Loss = [[0.24216049]], Testing Loss = [[1.6433065]]\n",
      "Epoch # 1912, Training Loss = [[0.24177732]], Testing Loss = [[1.64116436]]\n",
      "Epoch # 1913, Training Loss = [[0.24139517]], Testing Loss = [[1.6390255]]\n",
      "Epoch # 1914, Training Loss = [[0.24101404]], Testing Loss = [[1.63688992]]\n",
      "Epoch # 1915, Training Loss = [[0.24063392]], Testing Loss = [[1.63475762]]\n",
      "Epoch # 1916, Training Loss = [[0.24025481]], Testing Loss = [[1.63262857]]\n",
      "Epoch # 1917, Training Loss = [[0.2398767]], Testing Loss = [[1.63050279]]\n",
      "Epoch # 1918, Training Loss = [[0.2394996]], Testing Loss = [[1.62838025]]\n",
      "Epoch # 1919, Training Loss = [[0.2391235]], Testing Loss = [[1.62626096]]\n",
      "Epoch # 1920, Training Loss = [[0.23874839]], Testing Loss = [[1.62414492]]\n",
      "Epoch # 1921, Training Loss = [[0.23837428]], Testing Loss = [[1.6220321]]\n",
      "Epoch # 1922, Training Loss = [[0.23800115]], Testing Loss = [[1.61992252]]\n",
      "Epoch # 1923, Training Loss = [[0.23762901]], Testing Loss = [[1.61781615]]\n",
      "Epoch # 1924, Training Loss = [[0.23725786]], Testing Loss = [[1.61571301]]\n",
      "Epoch # 1925, Training Loss = [[0.23688768]], Testing Loss = [[1.61361307]]\n",
      "Epoch # 1926, Training Loss = [[0.23651848]], Testing Loss = [[1.61151633]]\n",
      "Epoch # 1927, Training Loss = [[0.23615025]], Testing Loss = [[1.60942279]]\n",
      "Epoch # 1928, Training Loss = [[0.23578299]], Testing Loss = [[1.60733245]]\n",
      "Epoch # 1929, Training Loss = [[0.2354167]], Testing Loss = [[1.60524529]]\n",
      "Epoch # 1930, Training Loss = [[0.23505137]], Testing Loss = [[1.6031613]]\n",
      "Epoch # 1931, Training Loss = [[0.234687]], Testing Loss = [[1.6010805]]\n",
      "Epoch # 1932, Training Loss = [[0.23432359]], Testing Loss = [[1.59900285]]\n",
      "Epoch # 1933, Training Loss = [[0.23396113]], Testing Loss = [[1.59692838]]\n",
      "Epoch # 1934, Training Loss = [[0.23359963]], Testing Loss = [[1.59485705]]\n",
      "Epoch # 1935, Training Loss = [[0.23323906]], Testing Loss = [[1.59278888]]\n",
      "Epoch # 1936, Training Loss = [[0.23287945]], Testing Loss = [[1.59072385]]\n",
      "Epoch # 1937, Training Loss = [[0.23252077]], Testing Loss = [[1.58866196]]\n",
      "Epoch # 1938, Training Loss = [[0.23216304]], Testing Loss = [[1.5866032]]\n",
      "Epoch # 1939, Training Loss = [[0.23180623]], Testing Loss = [[1.58454756]]\n",
      "Epoch # 1940, Training Loss = [[0.23145036]], Testing Loss = [[1.58249505]]\n",
      "Epoch # 1941, Training Loss = [[0.23109542]], Testing Loss = [[1.58044565]]\n",
      "Epoch # 1942, Training Loss = [[0.2307414]], Testing Loss = [[1.57839936]]\n",
      "Epoch # 1943, Training Loss = [[0.23038831]], Testing Loss = [[1.57635618]]\n",
      "Epoch # 1944, Training Loss = [[0.23003614]], Testing Loss = [[1.57431609]]\n",
      "Epoch # 1945, Training Loss = [[0.22968488]], Testing Loss = [[1.57227909]]\n",
      "Epoch # 1946, Training Loss = [[0.22933453]], Testing Loss = [[1.57024518]]\n",
      "Epoch # 1947, Training Loss = [[0.2289851]], Testing Loss = [[1.56821435]]\n",
      "Epoch # 1948, Training Loss = [[0.22863657]], Testing Loss = [[1.56618659]]\n",
      "Epoch # 1949, Training Loss = [[0.22828895]], Testing Loss = [[1.5641619]]\n",
      "Epoch # 1950, Training Loss = [[0.22794223]], Testing Loss = [[1.56214028]]\n",
      "Epoch # 1951, Training Loss = [[0.2275964]], Testing Loss = [[1.56012171]]\n",
      "Epoch # 1952, Training Loss = [[0.22725148]], Testing Loss = [[1.55810619]]\n",
      "Epoch # 1953, Training Loss = [[0.22690744]], Testing Loss = [[1.55609372]]\n",
      "Epoch # 1954, Training Loss = [[0.2265643]], Testing Loss = [[1.55408429]]\n",
      "Epoch # 1955, Training Loss = [[0.22622204]], Testing Loss = [[1.55207789]]\n",
      "Epoch # 1956, Training Loss = [[0.22588066]], Testing Loss = [[1.55007452]]\n",
      "Epoch # 1957, Training Loss = [[0.22554017]], Testing Loss = [[1.54807418]]\n",
      "Epoch # 1958, Training Loss = [[0.22520055]], Testing Loss = [[1.54607685]]\n",
      "Epoch # 1959, Training Loss = [[0.22486181]], Testing Loss = [[1.54408254]]\n",
      "Epoch # 1960, Training Loss = [[0.22452394]], Testing Loss = [[1.54209123]]\n",
      "Epoch # 1961, Training Loss = [[0.22418694]], Testing Loss = [[1.54010293]]\n",
      "Epoch # 1962, Training Loss = [[0.22385081]], Testing Loss = [[1.53811762]]\n",
      "Epoch # 1963, Training Loss = [[0.22351554]], Testing Loss = [[1.5361353]]\n",
      "Epoch # 1964, Training Loss = [[0.22318113]], Testing Loss = [[1.53415596]]\n",
      "Epoch # 1965, Training Loss = [[0.22284758]], Testing Loss = [[1.5321796]]\n",
      "Epoch # 1966, Training Loss = [[0.22251488]], Testing Loss = [[1.53020622]]\n",
      "Epoch # 1967, Training Loss = [[0.22218304]], Testing Loss = [[1.52823581]]\n",
      "Epoch # 1968, Training Loss = [[0.22185205]], Testing Loss = [[1.52626835]]\n",
      "Epoch # 1969, Training Loss = [[0.2215219]], Testing Loss = [[1.52430386]]\n",
      "Epoch # 1970, Training Loss = [[0.2211926]], Testing Loss = [[1.52234232]]\n",
      "Epoch # 1971, Training Loss = [[0.22086414]], Testing Loss = [[1.52038372]]\n",
      "Epoch # 1972, Training Loss = [[0.22053651]], Testing Loss = [[1.51842807]]\n",
      "Epoch # 1973, Training Loss = [[0.22020973]], Testing Loss = [[1.51647535]]\n",
      "Epoch # 1974, Training Loss = [[0.21988377]], Testing Loss = [[1.51452556]]\n",
      "Epoch # 1975, Training Loss = [[0.21955865]], Testing Loss = [[1.5125787]]\n",
      "Epoch # 1976, Training Loss = [[0.21923435]], Testing Loss = [[1.51063475]]\n",
      "Epoch # 1977, Training Loss = [[0.21891088]], Testing Loss = [[1.50869372]]\n",
      "Epoch # 1978, Training Loss = [[0.21858823]], Testing Loss = [[1.5067556]]\n",
      "Epoch # 1979, Training Loss = [[0.2182664]], Testing Loss = [[1.50482038]]\n",
      "Epoch # 1980, Training Loss = [[0.21794539]], Testing Loss = [[1.50288806]]\n",
      "Epoch # 1981, Training Loss = [[0.21762519]], Testing Loss = [[1.50095864]]\n",
      "Epoch # 1982, Training Loss = [[0.2173058]], Testing Loss = [[1.4990321]]\n",
      "Epoch # 1983, Training Loss = [[0.21698722]], Testing Loss = [[1.49710845]]\n",
      "Epoch # 1984, Training Loss = [[0.21666945]], Testing Loss = [[1.49518767]]\n",
      "Epoch # 1985, Training Loss = [[0.21635249]], Testing Loss = [[1.49326976]]\n",
      "Epoch # 1986, Training Loss = [[0.21603632]], Testing Loss = [[1.49135473]]\n",
      "Epoch # 1987, Training Loss = [[0.21572095]], Testing Loss = [[1.48944255]]\n",
      "Epoch # 1988, Training Loss = [[0.21540638]], Testing Loss = [[1.48753323]]\n",
      "Epoch # 1989, Training Loss = [[0.2150926]], Testing Loss = [[1.48562676]]\n",
      "Epoch # 1990, Training Loss = [[0.21477961]], Testing Loss = [[1.48372314]]\n",
      "Epoch # 1991, Training Loss = [[0.21446741]], Testing Loss = [[1.48182236]]\n",
      "Epoch # 1992, Training Loss = [[0.214156]], Testing Loss = [[1.47992441]]\n",
      "Epoch # 1993, Training Loss = [[0.21384537]], Testing Loss = [[1.47802929]]\n",
      "Epoch # 1994, Training Loss = [[0.21353552]], Testing Loss = [[1.47613701]]\n",
      "Epoch # 1995, Training Loss = [[0.21322645]], Testing Loss = [[1.47424754]]\n",
      "Epoch # 1996, Training Loss = [[0.21291815]], Testing Loss = [[1.47236088]]\n",
      "Epoch # 1997, Training Loss = [[0.21261063]], Testing Loss = [[1.47047704]]\n",
      "Epoch # 1998, Training Loss = [[0.21230387]], Testing Loss = [[1.468596]]\n",
      "Epoch # 1999, Training Loss = [[0.21199789]], Testing Loss = [[1.46671777]]\n",
      "Epoch # 2000, Training Loss = [[0.21169267]], Testing Loss = [[1.46484232]]\n",
      "Epoch # 2001, Training Loss = [[0.21138821]], Testing Loss = [[1.46296967]]\n",
      "Epoch # 2002, Training Loss = [[0.21108452]], Testing Loss = [[1.4610998]]\n",
      "Epoch # 2003, Training Loss = [[0.21078158]], Testing Loss = [[1.45923272]]\n",
      "Epoch # 2004, Training Loss = [[0.2104794]], Testing Loss = [[1.45736841]]\n",
      "Epoch # 2005, Training Loss = [[0.21017797]], Testing Loss = [[1.45550686]]\n",
      "Epoch # 2006, Training Loss = [[0.20987729]], Testing Loss = [[1.45364809]]\n",
      "Epoch # 2007, Training Loss = [[0.20957737]], Testing Loss = [[1.45179207]]\n",
      "Epoch # 2008, Training Loss = [[0.20927818]], Testing Loss = [[1.44993881]]\n",
      "Epoch # 2009, Training Loss = [[0.20897974]], Testing Loss = [[1.4480883]]\n",
      "Epoch # 2010, Training Loss = [[0.20868205]], Testing Loss = [[1.44624054]]\n",
      "Epoch # 2011, Training Loss = [[0.20838509]], Testing Loss = [[1.44439551]]\n",
      "Epoch # 2012, Training Loss = [[0.20808887]], Testing Loss = [[1.44255322]]\n",
      "Epoch # 2013, Training Loss = [[0.20779338]], Testing Loss = [[1.44071366]]\n",
      "Epoch # 2014, Training Loss = [[0.20749863]], Testing Loss = [[1.43887683]]\n",
      "Epoch # 2015, Training Loss = [[0.2072046]], Testing Loss = [[1.43704272]]\n",
      "Epoch # 2016, Training Loss = [[0.2069113]], Testing Loss = [[1.43521133]]\n",
      "Epoch # 2017, Training Loss = [[0.20661873]], Testing Loss = [[1.43338264]]\n",
      "Epoch # 2018, Training Loss = [[0.20632688]], Testing Loss = [[1.43155666]]\n",
      "Epoch # 2019, Training Loss = [[0.20603575]], Testing Loss = [[1.42973339]]\n",
      "Epoch # 2020, Training Loss = [[0.20574534]], Testing Loss = [[1.42791281]]\n",
      "Epoch # 2021, Training Loss = [[0.20545564]], Testing Loss = [[1.42609492]]\n",
      "Epoch # 2022, Training Loss = [[0.20516665]], Testing Loss = [[1.42427972]]\n",
      "Epoch # 2023, Training Loss = [[0.20487838]], Testing Loss = [[1.4224672]]\n",
      "Epoch # 2024, Training Loss = [[0.20459082]], Testing Loss = [[1.42065736]]\n",
      "Epoch # 2025, Training Loss = [[0.20430396]], Testing Loss = [[1.4188502]]\n",
      "Epoch # 2026, Training Loss = [[0.20401781]], Testing Loss = [[1.4170457]]\n",
      "Epoch # 2027, Training Loss = [[0.20373236]], Testing Loss = [[1.41524386]]\n",
      "Epoch # 2028, Training Loss = [[0.20344761]], Testing Loss = [[1.41344469]]\n",
      "Epoch # 2029, Training Loss = [[0.20316356]], Testing Loss = [[1.41164816]]\n",
      "Epoch # 2030, Training Loss = [[0.2028802]], Testing Loss = [[1.40985429]]\n",
      "Epoch # 2031, Training Loss = [[0.20259754]], Testing Loss = [[1.40806306]]\n",
      "Epoch # 2032, Training Loss = [[0.20231556]], Testing Loss = [[1.40627447]]\n",
      "Epoch # 2033, Training Loss = [[0.20203428]], Testing Loss = [[1.40448852]]\n",
      "Epoch # 2034, Training Loss = [[0.20175368]], Testing Loss = [[1.40270519]]\n",
      "Epoch # 2035, Training Loss = [[0.20147377]], Testing Loss = [[1.4009245]]\n",
      "Epoch # 2036, Training Loss = [[0.20119454]], Testing Loss = [[1.39914642]]\n",
      "Epoch # 2037, Training Loss = [[0.20091599]], Testing Loss = [[1.39737096]]\n",
      "Epoch # 2038, Training Loss = [[0.20063812]], Testing Loss = [[1.39559811]]\n",
      "Epoch # 2039, Training Loss = [[0.20036092]], Testing Loss = [[1.39382787]]\n",
      "Epoch # 2040, Training Loss = [[0.2000844]], Testing Loss = [[1.39206024]]\n",
      "Epoch # 2041, Training Loss = [[0.19980855]], Testing Loss = [[1.3902952]]\n",
      "Epoch # 2042, Training Loss = [[0.19953337]], Testing Loss = [[1.38853275]]\n",
      "Epoch # 2043, Training Loss = [[0.19925886]], Testing Loss = [[1.3867729]]\n",
      "Epoch # 2044, Training Loss = [[0.19898501]], Testing Loss = [[1.38501563]]\n",
      "Epoch # 2045, Training Loss = [[0.19871182]], Testing Loss = [[1.38326093]]\n",
      "Epoch # 2046, Training Loss = [[0.1984393]], Testing Loss = [[1.38150882]]\n",
      "Epoch # 2047, Training Loss = [[0.19816743]], Testing Loss = [[1.37975927]]\n",
      "Epoch # 2048, Training Loss = [[0.19789623]], Testing Loss = [[1.3780123]]\n",
      "Epoch # 2049, Training Loss = [[0.19762567]], Testing Loss = [[1.37626788]]\n",
      "Epoch # 2050, Training Loss = [[0.19735577]], Testing Loss = [[1.37452602]]\n",
      "Epoch # 2051, Training Loss = [[0.19708652]], Testing Loss = [[1.37278671]]\n",
      "Epoch # 2052, Training Loss = [[0.19681792]], Testing Loss = [[1.37104996]]\n",
      "Epoch # 2053, Training Loss = [[0.19654997]], Testing Loss = [[1.36931574]]\n",
      "Epoch # 2054, Training Loss = [[0.19628266]], Testing Loss = [[1.36758407]]\n",
      "Epoch # 2055, Training Loss = [[0.19601599]], Testing Loss = [[1.36585493]]\n",
      "Epoch # 2056, Training Loss = [[0.19574997]], Testing Loss = [[1.36412832]]\n",
      "Epoch # 2057, Training Loss = [[0.19548458]], Testing Loss = [[1.36240424]]\n",
      "Epoch # 2058, Training Loss = [[0.19521983]], Testing Loss = [[1.36068268]]\n",
      "Epoch # 2059, Training Loss = [[0.19495571]], Testing Loss = [[1.35896364]]\n",
      "Epoch # 2060, Training Loss = [[0.19469223]], Testing Loss = [[1.35724711]]\n",
      "Epoch # 2061, Training Loss = [[0.19442938]], Testing Loss = [[1.35553309]]\n",
      "Epoch # 2062, Training Loss = [[0.19416715]], Testing Loss = [[1.35382158]]\n",
      "Epoch # 2063, Training Loss = [[0.19390556]], Testing Loss = [[1.35211256]]\n",
      "Epoch # 2064, Training Loss = [[0.19364459]], Testing Loss = [[1.35040604]]\n",
      "Epoch # 2065, Training Loss = [[0.19338424]], Testing Loss = [[1.34870202]]\n",
      "Epoch # 2066, Training Loss = [[0.19312451]], Testing Loss = [[1.34700048]]\n",
      "Epoch # 2067, Training Loss = [[0.1928654]], Testing Loss = [[1.34530142]]\n",
      "Epoch # 2068, Training Loss = [[0.19260691]], Testing Loss = [[1.34360484]]\n",
      "Epoch # 2069, Training Loss = [[0.19234903]], Testing Loss = [[1.34191073]]\n",
      "Epoch # 2070, Training Loss = [[0.19209176]], Testing Loss = [[1.3402191]]\n",
      "Epoch # 2071, Training Loss = [[0.19183511]], Testing Loss = [[1.33852993]]\n",
      "Epoch # 2072, Training Loss = [[0.19157907]], Testing Loss = [[1.33684322]]\n",
      "Epoch # 2073, Training Loss = [[0.19132363]], Testing Loss = [[1.33515896]]\n",
      "Epoch # 2074, Training Loss = [[0.1910688]], Testing Loss = [[1.33347717]]\n",
      "Epoch # 2075, Training Loss = [[0.19081458]], Testing Loss = [[1.33179781]]\n",
      "Epoch # 2076, Training Loss = [[0.19056095]], Testing Loss = [[1.33012091]]\n",
      "Epoch # 2077, Training Loss = [[0.19030793]], Testing Loss = [[1.32844644]]\n",
      "Epoch # 2078, Training Loss = [[0.1900555]], Testing Loss = [[1.32677441]]\n",
      "Epoch # 2079, Training Loss = [[0.18980367]], Testing Loss = [[1.32510481]]\n",
      "Epoch # 2080, Training Loss = [[0.18955244]], Testing Loss = [[1.32343764]]\n",
      "Epoch # 2081, Training Loss = [[0.18930179]], Testing Loss = [[1.32177289]]\n",
      "Epoch # 2082, Training Loss = [[0.18905174]], Testing Loss = [[1.32011056]]\n",
      "Epoch # 2083, Training Loss = [[0.18880228]], Testing Loss = [[1.31845065]]\n",
      "Epoch # 2084, Training Loss = [[0.1885534]], Testing Loss = [[1.31679315]]\n",
      "Epoch # 2085, Training Loss = [[0.18830511]], Testing Loss = [[1.31513805]]\n",
      "Epoch # 2086, Training Loss = [[0.1880574]], Testing Loss = [[1.31348536]]\n",
      "Epoch # 2087, Training Loss = [[0.18781028]], Testing Loss = [[1.31183506]]\n",
      "Epoch # 2088, Training Loss = [[0.18756373]], Testing Loss = [[1.31018716]]\n",
      "Epoch # 2089, Training Loss = [[0.18731777]], Testing Loss = [[1.30854165]]\n",
      "Epoch # 2090, Training Loss = [[0.18707237]], Testing Loss = [[1.30689852]]\n",
      "Epoch # 2091, Training Loss = [[0.18682756]], Testing Loss = [[1.30525778]]\n",
      "Epoch # 2092, Training Loss = [[0.18658332]], Testing Loss = [[1.30361942]]\n",
      "Epoch # 2093, Training Loss = [[0.18633964]], Testing Loss = [[1.30198342]]\n",
      "Epoch # 2094, Training Loss = [[0.18609654]], Testing Loss = [[1.3003498]]\n",
      "Epoch # 2095, Training Loss = [[0.185854]], Testing Loss = [[1.29871855]]\n",
      "Epoch # 2096, Training Loss = [[0.18561204]], Testing Loss = [[1.29708965]]\n",
      "Epoch # 2097, Training Loss = [[0.18537063]], Testing Loss = [[1.29546312]]\n",
      "Epoch # 2098, Training Loss = [[0.18512979]], Testing Loss = [[1.29383893]]\n",
      "Epoch # 2099, Training Loss = [[0.1848895]], Testing Loss = [[1.2922171]]\n",
      "Epoch # 2100, Training Loss = [[0.18464978]], Testing Loss = [[1.29059761]]\n",
      "Epoch # 2101, Training Loss = [[0.18441061]], Testing Loss = [[1.28898046]]\n",
      "Epoch # 2102, Training Loss = [[0.184172]], Testing Loss = [[1.28736565]]\n",
      "Epoch # 2103, Training Loss = [[0.18393395]], Testing Loss = [[1.28575318]]\n",
      "Epoch # 2104, Training Loss = [[0.18369644]], Testing Loss = [[1.28414303]]\n",
      "Epoch # 2105, Training Loss = [[0.18345949]], Testing Loss = [[1.28253521]]\n",
      "Epoch # 2106, Training Loss = [[0.18322308]], Testing Loss = [[1.28092971]]\n",
      "Epoch # 2107, Training Loss = [[0.18298722]], Testing Loss = [[1.27932653]]\n",
      "Epoch # 2108, Training Loss = [[0.18275191]], Testing Loss = [[1.27772567]]\n",
      "Epoch # 2109, Training Loss = [[0.18251714]], Testing Loss = [[1.27612711]]\n",
      "Epoch # 2110, Training Loss = [[0.18228291]], Testing Loss = [[1.27453086]]\n",
      "Epoch # 2111, Training Loss = [[0.18204923]], Testing Loss = [[1.27293691]]\n",
      "Epoch # 2112, Training Loss = [[0.18181608]], Testing Loss = [[1.27134526]]\n",
      "Epoch # 2113, Training Loss = [[0.18158347]], Testing Loss = [[1.2697559]]\n",
      "Epoch # 2114, Training Loss = [[0.1813514]], Testing Loss = [[1.26816883]]\n",
      "Epoch # 2115, Training Loss = [[0.18111985]], Testing Loss = [[1.26658405]]\n",
      "Epoch # 2116, Training Loss = [[0.18088885]], Testing Loss = [[1.26500155]]\n",
      "Epoch # 2117, Training Loss = [[0.18065837]], Testing Loss = [[1.26342133]]\n",
      "Epoch # 2118, Training Loss = [[0.18042842]], Testing Loss = [[1.26184339]]\n",
      "Epoch # 2119, Training Loss = [[0.180199]], Testing Loss = [[1.26026772]]\n",
      "Epoch # 2120, Training Loss = [[0.1799701]], Testing Loss = [[1.25869431]]\n",
      "Epoch # 2121, Training Loss = [[0.17974173]], Testing Loss = [[1.25712317]]\n",
      "Epoch # 2122, Training Loss = [[0.17951388]], Testing Loss = [[1.25555428]]\n",
      "Epoch # 2123, Training Loss = [[0.17928655]], Testing Loss = [[1.25398766]]\n",
      "Epoch # 2124, Training Loss = [[0.17905975]], Testing Loss = [[1.25242328]]\n",
      "Epoch # 2125, Training Loss = [[0.17883346]], Testing Loss = [[1.25086115]]\n",
      "Epoch # 2126, Training Loss = [[0.17860768]], Testing Loss = [[1.24930127]]\n",
      "Epoch # 2127, Training Loss = [[0.17838242]], Testing Loss = [[1.24774362]]\n",
      "Epoch # 2128, Training Loss = [[0.17815768]], Testing Loss = [[1.24618822]]\n",
      "Epoch # 2129, Training Loss = [[0.17793344]], Testing Loss = [[1.24463505]]\n",
      "Epoch # 2130, Training Loss = [[0.17770972]], Testing Loss = [[1.2430841]]\n",
      "Epoch # 2131, Training Loss = [[0.17748651]], Testing Loss = [[1.24153538]]\n",
      "Epoch # 2132, Training Loss = [[0.1772638]], Testing Loss = [[1.23998889]]\n",
      "Epoch # 2133, Training Loss = [[0.1770416]], Testing Loss = [[1.23844461]]\n",
      "Epoch # 2134, Training Loss = [[0.1768199]], Testing Loss = [[1.23690254]]\n",
      "Epoch # 2135, Training Loss = [[0.1765987]], Testing Loss = [[1.23536269]]\n",
      "Epoch # 2136, Training Loss = [[0.17637801]], Testing Loss = [[1.23382504]]\n",
      "Epoch # 2137, Training Loss = [[0.17615782]], Testing Loss = [[1.2322896]]\n",
      "Epoch # 2138, Training Loss = [[0.17593812]], Testing Loss = [[1.23075635]]\n",
      "Epoch # 2139, Training Loss = [[0.17571892]], Testing Loss = [[1.2292253]]\n",
      "Epoch # 2140, Training Loss = [[0.17550022]], Testing Loss = [[1.22769645]]\n",
      "Epoch # 2141, Training Loss = [[0.17528201]], Testing Loss = [[1.22616978]]\n",
      "Epoch # 2142, Training Loss = [[0.17506429]], Testing Loss = [[1.22464529]]\n",
      "Epoch # 2143, Training Loss = [[0.17484706]], Testing Loss = [[1.22312299]]\n",
      "Epoch # 2144, Training Loss = [[0.17463032]], Testing Loss = [[1.22160287]]\n",
      "Epoch # 2145, Training Loss = [[0.17441407]], Testing Loss = [[1.22008492]]\n",
      "Epoch # 2146, Training Loss = [[0.1741983]], Testing Loss = [[1.21856913]]\n",
      "Epoch # 2147, Training Loss = [[0.17398302]], Testing Loss = [[1.21705552]]\n",
      "Epoch # 2148, Training Loss = [[0.17376823]], Testing Loss = [[1.21554407]]\n",
      "Epoch # 2149, Training Loss = [[0.17355391]], Testing Loss = [[1.21403478]]\n",
      "Epoch # 2150, Training Loss = [[0.17334008]], Testing Loss = [[1.21252764]]\n",
      "Epoch # 2151, Training Loss = [[0.17312673]], Testing Loss = [[1.21102266]]\n",
      "Epoch # 2152, Training Loss = [[0.17291385]], Testing Loss = [[1.20951982]]\n",
      "Epoch # 2153, Training Loss = [[0.17270145]], Testing Loss = [[1.20801913]]\n",
      "Epoch # 2154, Training Loss = [[0.17248952]], Testing Loss = [[1.20652058]]\n",
      "Epoch # 2155, Training Loss = [[0.17227807]], Testing Loss = [[1.20502417]]\n",
      "Epoch # 2156, Training Loss = [[0.17206709]], Testing Loss = [[1.20352989]]\n",
      "Epoch # 2157, Training Loss = [[0.17185658]], Testing Loss = [[1.20203775]]\n",
      "Epoch # 2158, Training Loss = [[0.17164654]], Testing Loss = [[1.20054773]]\n",
      "Epoch # 2159, Training Loss = [[0.17143697]], Testing Loss = [[1.19905983]]\n",
      "Epoch # 2160, Training Loss = [[0.17122786]], Testing Loss = [[1.19757406]]\n",
      "Epoch # 2161, Training Loss = [[0.17101922]], Testing Loss = [[1.1960904]]\n",
      "Epoch # 2162, Training Loss = [[0.17081104]], Testing Loss = [[1.19460886]]\n",
      "Epoch # 2163, Training Loss = [[0.17060333]], Testing Loss = [[1.19312942]]\n",
      "Epoch # 2164, Training Loss = [[0.17039607]], Testing Loss = [[1.19165209]]\n",
      "Epoch # 2165, Training Loss = [[0.17018928]], Testing Loss = [[1.19017687]]\n",
      "Epoch # 2166, Training Loss = [[0.16998294]], Testing Loss = [[1.18870374]]\n",
      "Epoch # 2167, Training Loss = [[0.16977706]], Testing Loss = [[1.18723271]]\n",
      "Epoch # 2168, Training Loss = [[0.16957164]], Testing Loss = [[1.18576377]]\n",
      "Epoch # 2169, Training Loss = [[0.16936667]], Testing Loss = [[1.18429692]]\n",
      "Epoch # 2170, Training Loss = [[0.16916216]], Testing Loss = [[1.18283215]]\n",
      "Epoch # 2171, Training Loss = [[0.16895809]], Testing Loss = [[1.18136947]]\n",
      "Epoch # 2172, Training Loss = [[0.16875448]], Testing Loss = [[1.17990887]]\n",
      "Epoch # 2173, Training Loss = [[0.16855131]], Testing Loss = [[1.17845034]]\n",
      "Epoch # 2174, Training Loss = [[0.16834859]], Testing Loss = [[1.17699388]]\n",
      "Epoch # 2175, Training Loss = [[0.16814632]], Testing Loss = [[1.17553949]]\n",
      "Epoch # 2176, Training Loss = [[0.16794449]], Testing Loss = [[1.17408717]]\n",
      "Epoch # 2177, Training Loss = [[0.16774311]], Testing Loss = [[1.1726369]]\n",
      "Epoch # 2178, Training Loss = [[0.16754217]], Testing Loss = [[1.1711887]]\n",
      "Epoch # 2179, Training Loss = [[0.16734167]], Testing Loss = [[1.16974255]]\n",
      "Epoch # 2180, Training Loss = [[0.16714161]], Testing Loss = [[1.16829845]]\n",
      "Epoch # 2181, Training Loss = [[0.16694199]], Testing Loss = [[1.1668564]]\n",
      "Epoch # 2182, Training Loss = [[0.16674281]], Testing Loss = [[1.16541639]]\n",
      "Epoch # 2183, Training Loss = [[0.16654406]], Testing Loss = [[1.16397843]]\n",
      "Epoch # 2184, Training Loss = [[0.16634575]], Testing Loss = [[1.1625425]]\n",
      "Epoch # 2185, Training Loss = [[0.16614787]], Testing Loss = [[1.16110861]]\n",
      "Epoch # 2186, Training Loss = [[0.16595042]], Testing Loss = [[1.15967675]]\n",
      "Epoch # 2187, Training Loss = [[0.1657534]], Testing Loss = [[1.15824691]]\n",
      "Epoch # 2188, Training Loss = [[0.16555681]], Testing Loss = [[1.1568191]]\n",
      "Epoch # 2189, Training Loss = [[0.16536066]], Testing Loss = [[1.15539332]]\n",
      "Epoch # 2190, Training Loss = [[0.16516492]], Testing Loss = [[1.15396955]]\n",
      "Epoch # 2191, Training Loss = [[0.16496962]], Testing Loss = [[1.15254779]]\n",
      "Epoch # 2192, Training Loss = [[0.16477474]], Testing Loss = [[1.15112805]]\n",
      "Epoch # 2193, Training Loss = [[0.16458028]], Testing Loss = [[1.14971031]]\n",
      "Epoch # 2194, Training Loss = [[0.16438624]], Testing Loss = [[1.14829458]]\n",
      "Epoch # 2195, Training Loss = [[0.16419263]], Testing Loss = [[1.14688085]]\n",
      "Epoch # 2196, Training Loss = [[0.16399944]], Testing Loss = [[1.14546912]]\n",
      "Epoch # 2197, Training Loss = [[0.16380666]], Testing Loss = [[1.14405938]]\n",
      "Epoch # 2198, Training Loss = [[0.1636143]], Testing Loss = [[1.14265164]]\n",
      "Epoch # 2199, Training Loss = [[0.16342236]], Testing Loss = [[1.14124588]]\n",
      "Epoch # 2200, Training Loss = [[0.16323083]], Testing Loss = [[1.13984211]]\n",
      "Epoch # 2201, Training Loss = [[0.16303972]], Testing Loss = [[1.13844032]]\n",
      "Epoch # 2202, Training Loss = [[0.16284902]], Testing Loss = [[1.13704051]]\n",
      "Epoch # 2203, Training Loss = [[0.16265873]], Testing Loss = [[1.13564268]]\n",
      "Epoch # 2204, Training Loss = [[0.16246885]], Testing Loss = [[1.13424682]]\n",
      "Epoch # 2205, Training Loss = [[0.16227938]], Testing Loss = [[1.13285292]]\n",
      "Epoch # 2206, Training Loss = [[0.16209031]], Testing Loss = [[1.131461]]\n",
      "Epoch # 2207, Training Loss = [[0.16190166]], Testing Loss = [[1.13007103]]\n",
      "Epoch # 2208, Training Loss = [[0.16171341]], Testing Loss = [[1.12868303]]\n",
      "Epoch # 2209, Training Loss = [[0.16152556]], Testing Loss = [[1.12729698]]\n",
      "Epoch # 2210, Training Loss = [[0.16133812]], Testing Loss = [[1.12591288]]\n",
      "Epoch # 2211, Training Loss = [[0.16115107]], Testing Loss = [[1.12453074]]\n",
      "Epoch # 2212, Training Loss = [[0.16096443]], Testing Loss = [[1.12315054]]\n",
      "Epoch # 2213, Training Loss = [[0.16077819]], Testing Loss = [[1.12177228]]\n",
      "Epoch # 2214, Training Loss = [[0.16059235]], Testing Loss = [[1.12039597]]\n",
      "Epoch # 2215, Training Loss = [[0.16040691]], Testing Loss = [[1.11902159]]\n",
      "Epoch # 2216, Training Loss = [[0.16022186]], Testing Loss = [[1.11764915]]\n",
      "Epoch # 2217, Training Loss = [[0.1600372]], Testing Loss = [[1.11627864]]\n",
      "Epoch # 2218, Training Loss = [[0.15985294]], Testing Loss = [[1.11491006]]\n",
      "Epoch # 2219, Training Loss = [[0.15966908]], Testing Loss = [[1.1135434]]\n",
      "Epoch # 2220, Training Loss = [[0.1594856]], Testing Loss = [[1.11217866]]\n",
      "Epoch # 2221, Training Loss = [[0.15930252]], Testing Loss = [[1.11081584]]\n",
      "Epoch # 2222, Training Loss = [[0.15911982]], Testing Loss = [[1.10945494]]\n",
      "Epoch # 2223, Training Loss = [[0.15893752]], Testing Loss = [[1.10809595]]\n",
      "Epoch # 2224, Training Loss = [[0.1587556]], Testing Loss = [[1.10673887]]\n",
      "Epoch # 2225, Training Loss = [[0.15857406]], Testing Loss = [[1.1053837]]\n",
      "Epoch # 2226, Training Loss = [[0.15839292]], Testing Loss = [[1.10403043]]\n",
      "Epoch # 2227, Training Loss = [[0.15821215]], Testing Loss = [[1.10267906]]\n",
      "Epoch # 2228, Training Loss = [[0.15803177]], Testing Loss = [[1.10132959]]\n",
      "Epoch # 2229, Training Loss = [[0.15785177]], Testing Loss = [[1.09998201]]\n",
      "Epoch # 2230, Training Loss = [[0.15767216]], Testing Loss = [[1.09863632]]\n",
      "Epoch # 2231, Training Loss = [[0.15749292]], Testing Loss = [[1.09729252]]\n",
      "Epoch # 2232, Training Loss = [[0.15731406]], Testing Loss = [[1.09595061]]\n",
      "Epoch # 2233, Training Loss = [[0.15713557]], Testing Loss = [[1.09461057]]\n",
      "Epoch # 2234, Training Loss = [[0.15695747]], Testing Loss = [[1.09327242]]\n",
      "Epoch # 2235, Training Loss = [[0.15677974]], Testing Loss = [[1.09193614]]\n",
      "Epoch # 2236, Training Loss = [[0.15660238]], Testing Loss = [[1.09060174]]\n",
      "Epoch # 2237, Training Loss = [[0.1564254]], Testing Loss = [[1.0892692]]\n",
      "Epoch # 2238, Training Loss = [[0.15624879]], Testing Loss = [[1.08793853]]\n",
      "Epoch # 2239, Training Loss = [[0.15607255]], Testing Loss = [[1.08660973]]\n",
      "Epoch # 2240, Training Loss = [[0.15589668]], Testing Loss = [[1.08528279]]\n",
      "Epoch # 2241, Training Loss = [[0.15572118]], Testing Loss = [[1.0839577]]\n",
      "Epoch # 2242, Training Loss = [[0.15554605]], Testing Loss = [[1.08263447]]\n",
      "Epoch # 2243, Training Loss = [[0.15537128]], Testing Loss = [[1.08131309]]\n",
      "Epoch # 2244, Training Loss = [[0.15519688]], Testing Loss = [[1.07999357]]\n",
      "Epoch # 2245, Training Loss = [[0.15502285]], Testing Loss = [[1.07867588]]\n",
      "Epoch # 2246, Training Loss = [[0.15484918]], Testing Loss = [[1.07736004]]\n",
      "Epoch # 2247, Training Loss = [[0.15467587]], Testing Loss = [[1.07604604]]\n",
      "Epoch # 2248, Training Loss = [[0.15450292]], Testing Loss = [[1.07473388]]\n",
      "Epoch # 2249, Training Loss = [[0.15433034]], Testing Loss = [[1.07342355]]\n",
      "Epoch # 2250, Training Loss = [[0.15415811]], Testing Loss = [[1.07211506]]\n",
      "Epoch # 2251, Training Loss = [[0.15398625]], Testing Loss = [[1.07080839]]\n",
      "Epoch # 2252, Training Loss = [[0.15381474]], Testing Loss = [[1.06950355]]\n",
      "Epoch # 2253, Training Loss = [[0.15364359]], Testing Loss = [[1.06820053]]\n",
      "Epoch # 2254, Training Loss = [[0.15347279]], Testing Loss = [[1.06689933]]\n",
      "Epoch # 2255, Training Loss = [[0.15330235]], Testing Loss = [[1.06559995]]\n",
      "Epoch # 2256, Training Loss = [[0.15313227]], Testing Loss = [[1.06430238]]\n",
      "Epoch # 2257, Training Loss = [[0.15296253]], Testing Loss = [[1.06300662]]\n",
      "Epoch # 2258, Training Loss = [[0.15279315]], Testing Loss = [[1.06171267]]\n",
      "Epoch # 2259, Training Loss = [[0.15262412]], Testing Loss = [[1.06042053]]\n",
      "Epoch # 2260, Training Loss = [[0.15245544]], Testing Loss = [[1.05913018]]\n",
      "Epoch # 2261, Training Loss = [[0.15228711]], Testing Loss = [[1.05784164]]\n",
      "Epoch # 2262, Training Loss = [[0.15211912]], Testing Loss = [[1.0565549]]\n",
      "Epoch # 2263, Training Loss = [[0.15195148]], Testing Loss = [[1.05526995]]\n",
      "Epoch # 2264, Training Loss = [[0.15178419]], Testing Loss = [[1.05398679]]\n",
      "Epoch # 2265, Training Loss = [[0.15161725]], Testing Loss = [[1.05270541]]\n",
      "Epoch # 2266, Training Loss = [[0.15145064]], Testing Loss = [[1.05142583]]\n",
      "Epoch # 2267, Training Loss = [[0.15128439]], Testing Loss = [[1.05014802]]\n",
      "Epoch # 2268, Training Loss = [[0.15111847]], Testing Loss = [[1.048872]]\n",
      "Epoch # 2269, Training Loss = [[0.15095289]], Testing Loss = [[1.04759775]]\n",
      "Epoch # 2270, Training Loss = [[0.15078766]], Testing Loss = [[1.04632528]]\n",
      "Epoch # 2271, Training Loss = [[0.15062276]], Testing Loss = [[1.04505458]]\n",
      "Epoch # 2272, Training Loss = [[0.15045821]], Testing Loss = [[1.04378564]]\n",
      "Epoch # 2273, Training Loss = [[0.15029399]], Testing Loss = [[1.04251847]]\n",
      "Epoch # 2274, Training Loss = [[0.15013011]], Testing Loss = [[1.04125307]]\n",
      "Epoch # 2275, Training Loss = [[0.14996656]], Testing Loss = [[1.03998942]]\n",
      "Epoch # 2276, Training Loss = [[0.14980335]], Testing Loss = [[1.03872754]]\n",
      "Epoch # 2277, Training Loss = [[0.14964047]], Testing Loss = [[1.0374674]]\n",
      "Epoch # 2278, Training Loss = [[0.14947792]], Testing Loss = [[1.03620902]]\n",
      "Epoch # 2279, Training Loss = [[0.14931571]], Testing Loss = [[1.03495239]]\n",
      "Epoch # 2280, Training Loss = [[0.14915383]], Testing Loss = [[1.03369751]]\n",
      "Epoch # 2281, Training Loss = [[0.14899227]], Testing Loss = [[1.03244436]]\n",
      "Epoch # 2282, Training Loss = [[0.14883105]], Testing Loss = [[1.03119296]]\n",
      "Epoch # 2283, Training Loss = [[0.14867016]], Testing Loss = [[1.0299433]]\n",
      "Epoch # 2284, Training Loss = [[0.14850959]], Testing Loss = [[1.02869537]]\n",
      "Epoch # 2285, Training Loss = [[0.14834935]], Testing Loss = [[1.02744918]]\n",
      "Epoch # 2286, Training Loss = [[0.14818943]], Testing Loss = [[1.02620471]]\n",
      "Epoch # 2287, Training Loss = [[0.14802984]], Testing Loss = [[1.02496197]]\n",
      "Epoch # 2288, Training Loss = [[0.14787058]], Testing Loss = [[1.02372096]]\n",
      "Epoch # 2289, Training Loss = [[0.14771164]], Testing Loss = [[1.02248166]]\n",
      "Epoch # 2290, Training Loss = [[0.14755301]], Testing Loss = [[1.02124409]]\n",
      "Epoch # 2291, Training Loss = [[0.14739471]], Testing Loss = [[1.02000823]]\n",
      "Epoch # 2292, Training Loss = [[0.14723674]], Testing Loss = [[1.01877409]]\n",
      "Epoch # 2293, Training Loss = [[0.14707908]], Testing Loss = [[1.01754166]]\n",
      "Epoch # 2294, Training Loss = [[0.14692173]], Testing Loss = [[1.01631093]]\n",
      "Epoch # 2295, Training Loss = [[0.14676471]], Testing Loss = [[1.01508192]]\n",
      "Epoch # 2296, Training Loss = [[0.146608]], Testing Loss = [[1.0138546]]\n",
      "Epoch # 2297, Training Loss = [[0.14645161]], Testing Loss = [[1.01262898]]\n",
      "Epoch # 2298, Training Loss = [[0.14629554]], Testing Loss = [[1.01140507]]\n",
      "Epoch # 2299, Training Loss = [[0.14613978]], Testing Loss = [[1.01018284]]\n",
      "Epoch # 2300, Training Loss = [[0.14598433]], Testing Loss = [[1.00896231]]\n",
      "Epoch # 2301, Training Loss = [[0.14582919]], Testing Loss = [[1.00774347]]\n",
      "Epoch # 2302, Training Loss = [[0.14567437]], Testing Loss = [[1.00652632]]\n",
      "Epoch # 2303, Training Loss = [[0.14551986]], Testing Loss = [[1.00531085]]\n",
      "Epoch # 2304, Training Loss = [[0.14536566]], Testing Loss = [[1.00409706]]\n",
      "Epoch # 2305, Training Loss = [[0.14521176]], Testing Loss = [[1.00288495]]\n",
      "Epoch # 2306, Training Loss = [[0.14505818]], Testing Loss = [[1.00167452]]\n",
      "Epoch # 2307, Training Loss = [[0.1449049]], Testing Loss = [[1.00046576]]\n",
      "Epoch # 2308, Training Loss = [[0.14475193]], Testing Loss = [[0.99925867]]\n",
      "Epoch # 2309, Training Loss = [[0.14459927]], Testing Loss = [[0.99805325]]\n",
      "Epoch # 2310, Training Loss = [[0.14444691]], Testing Loss = [[0.9968495]]\n",
      "Epoch # 2311, Training Loss = [[0.14429486]], Testing Loss = [[0.99564741]]\n",
      "Epoch # 2312, Training Loss = [[0.14414311]], Testing Loss = [[0.99444698]]\n",
      "Epoch # 2313, Training Loss = [[0.14399166]], Testing Loss = [[0.99324821]]\n",
      "Epoch # 2314, Training Loss = [[0.14384051]], Testing Loss = [[0.99205109]]\n",
      "Epoch # 2315, Training Loss = [[0.14368966]], Testing Loss = [[0.99085563]]\n",
      "Epoch # 2316, Training Loss = [[0.14353912]], Testing Loss = [[0.98966182]]\n",
      "Epoch # 2317, Training Loss = [[0.14338887]], Testing Loss = [[0.98846966]]\n",
      "Epoch # 2318, Training Loss = [[0.14323893]], Testing Loss = [[0.98727914]]\n",
      "Epoch # 2319, Training Loss = [[0.14308928]], Testing Loss = [[0.98609026]]\n",
      "Epoch # 2320, Training Loss = [[0.14293992]], Testing Loss = [[0.98490303]]\n",
      "Epoch # 2321, Training Loss = [[0.14279087]], Testing Loss = [[0.98371743]]\n",
      "Epoch # 2322, Training Loss = [[0.14264211]], Testing Loss = [[0.98253347]]\n",
      "Epoch # 2323, Training Loss = [[0.14249364]], Testing Loss = [[0.98135114]]\n",
      "Epoch # 2324, Training Loss = [[0.14234547]], Testing Loss = [[0.98017044]]\n",
      "Epoch # 2325, Training Loss = [[0.14219759]], Testing Loss = [[0.97899137]]\n",
      "Epoch # 2326, Training Loss = [[0.14205]], Testing Loss = [[0.97781392]]\n",
      "Epoch # 2327, Training Loss = [[0.14190271]], Testing Loss = [[0.9766381]]\n",
      "Epoch # 2328, Training Loss = [[0.1417557]], Testing Loss = [[0.97546389]]\n",
      "Epoch # 2329, Training Loss = [[0.14160899]], Testing Loss = [[0.9742913]]\n",
      "Epoch # 2330, Training Loss = [[0.14146256]], Testing Loss = [[0.97312033]]\n",
      "Epoch # 2331, Training Loss = [[0.14131642]], Testing Loss = [[0.97195097]]\n",
      "Epoch # 2332, Training Loss = [[0.14117057]], Testing Loss = [[0.97078322]]\n",
      "Epoch # 2333, Training Loss = [[0.14102501]], Testing Loss = [[0.96961708]]\n",
      "Epoch # 2334, Training Loss = [[0.14087973]], Testing Loss = [[0.96845255]]\n",
      "Epoch # 2335, Training Loss = [[0.14073474]], Testing Loss = [[0.96728961]]\n",
      "Epoch # 2336, Training Loss = [[0.14059004]], Testing Loss = [[0.96612828]]\n",
      "Epoch # 2337, Training Loss = [[0.14044561]], Testing Loss = [[0.96496854]]\n",
      "Epoch # 2338, Training Loss = [[0.14030147]], Testing Loss = [[0.9638104]]\n",
      "Epoch # 2339, Training Loss = [[0.14015762]], Testing Loss = [[0.96265385]]\n",
      "Epoch # 2340, Training Loss = [[0.14001404]], Testing Loss = [[0.96149889]]\n",
      "Epoch # 2341, Training Loss = [[0.13987075]], Testing Loss = [[0.96034552]]\n",
      "Epoch # 2342, Training Loss = [[0.13972773]], Testing Loss = [[0.95919374]]\n",
      "Epoch # 2343, Training Loss = [[0.139585]], Testing Loss = [[0.95804353]]\n",
      "Epoch # 2344, Training Loss = [[0.13944254]], Testing Loss = [[0.95689491]]\n",
      "Epoch # 2345, Training Loss = [[0.13930036]], Testing Loss = [[0.95574787]]\n",
      "Epoch # 2346, Training Loss = [[0.13915846]], Testing Loss = [[0.9546024]]\n",
      "Epoch # 2347, Training Loss = [[0.13901683]], Testing Loss = [[0.95345851]]\n",
      "Epoch # 2348, Training Loss = [[0.13887549]], Testing Loss = [[0.95231618]]\n",
      "Epoch # 2349, Training Loss = [[0.13873441]], Testing Loss = [[0.95117542]]\n",
      "Epoch # 2350, Training Loss = [[0.13859361]], Testing Loss = [[0.95003623]]\n",
      "Epoch # 2351, Training Loss = [[0.13845309]], Testing Loss = [[0.94889861]]\n",
      "Epoch # 2352, Training Loss = [[0.13831283]], Testing Loss = [[0.94776254]]\n",
      "Epoch # 2353, Training Loss = [[0.13817285]], Testing Loss = [[0.94662804]]\n",
      "Epoch # 2354, Training Loss = [[0.13803314]], Testing Loss = [[0.94549508]]\n",
      "Epoch # 2355, Training Loss = [[0.1378937]], Testing Loss = [[0.94436369]]\n",
      "Epoch # 2356, Training Loss = [[0.13775453]], Testing Loss = [[0.94323384]]\n",
      "Epoch # 2357, Training Loss = [[0.13761563]], Testing Loss = [[0.94210555]]\n",
      "Epoch # 2358, Training Loss = [[0.137477]], Testing Loss = [[0.9409788]]\n",
      "Epoch # 2359, Training Loss = [[0.13733864]], Testing Loss = [[0.93985359]]\n",
      "Epoch # 2360, Training Loss = [[0.13720054]], Testing Loss = [[0.93872993]]\n",
      "Epoch # 2361, Training Loss = [[0.13706271]], Testing Loss = [[0.93760781]]\n",
      "Epoch # 2362, Training Loss = [[0.13692515]], Testing Loss = [[0.93648722]]\n",
      "Epoch # 2363, Training Loss = [[0.13678785]], Testing Loss = [[0.93536817]]\n",
      "Epoch # 2364, Training Loss = [[0.13665082]], Testing Loss = [[0.93425065]]\n",
      "Epoch # 2365, Training Loss = [[0.13651405]], Testing Loss = [[0.93313466]]\n",
      "Epoch # 2366, Training Loss = [[0.13637754]], Testing Loss = [[0.9320202]]\n",
      "Epoch # 2367, Training Loss = [[0.13624129]], Testing Loss = [[0.93090727]]\n",
      "Epoch # 2368, Training Loss = [[0.13610531]], Testing Loss = [[0.92979586]]\n",
      "Epoch # 2369, Training Loss = [[0.13596959]], Testing Loss = [[0.92868597]]\n",
      "Epoch # 2370, Training Loss = [[0.13583413]], Testing Loss = [[0.9275776]]\n",
      "Epoch # 2371, Training Loss = [[0.13569892]], Testing Loss = [[0.92647074]]\n",
      "Epoch # 2372, Training Loss = [[0.13556398]], Testing Loss = [[0.9253654]]\n",
      "Epoch # 2373, Training Loss = [[0.1354293]], Testing Loss = [[0.92426157]]\n",
      "Epoch # 2374, Training Loss = [[0.13529487]], Testing Loss = [[0.92315925]]\n",
      "Epoch # 2375, Training Loss = [[0.1351607]], Testing Loss = [[0.92205844]]\n",
      "Epoch # 2376, Training Loss = [[0.13502678]], Testing Loss = [[0.92095913]]\n",
      "Epoch # 2377, Training Loss = [[0.13489313]], Testing Loss = [[0.91986132]]\n",
      "Epoch # 2378, Training Loss = [[0.13475972]], Testing Loss = [[0.91876502]]\n",
      "Epoch # 2379, Training Loss = [[0.13462657]], Testing Loss = [[0.91767021]]\n",
      "Epoch # 2380, Training Loss = [[0.13449368]], Testing Loss = [[0.9165769]]\n",
      "Epoch # 2381, Training Loss = [[0.13436104]], Testing Loss = [[0.91548508]]\n",
      "Epoch # 2382, Training Loss = [[0.13422865]], Testing Loss = [[0.91439475]]\n",
      "Epoch # 2383, Training Loss = [[0.13409651]], Testing Loss = [[0.91330591]]\n",
      "Epoch # 2384, Training Loss = [[0.13396462]], Testing Loss = [[0.91221856]]\n",
      "Epoch # 2385, Training Loss = [[0.13383299]], Testing Loss = [[0.91113269]]\n",
      "Epoch # 2386, Training Loss = [[0.1337016]], Testing Loss = [[0.91004831]]\n",
      "Epoch # 2387, Training Loss = [[0.13357046]], Testing Loss = [[0.9089654]]\n",
      "Epoch # 2388, Training Loss = [[0.13343957]], Testing Loss = [[0.90788397]]\n",
      "Epoch # 2389, Training Loss = [[0.13330893]], Testing Loss = [[0.90680402]]\n",
      "Epoch # 2390, Training Loss = [[0.13317854]], Testing Loss = [[0.90572554]]\n",
      "Epoch # 2391, Training Loss = [[0.13304839]], Testing Loss = [[0.90464853]]\n",
      "Epoch # 2392, Training Loss = [[0.13291849]], Testing Loss = [[0.90357299]]\n",
      "Epoch # 2393, Training Loss = [[0.13278884]], Testing Loss = [[0.90249891]]\n",
      "Epoch # 2394, Training Loss = [[0.13265943]], Testing Loss = [[0.9014263]]\n",
      "Epoch # 2395, Training Loss = [[0.13253026]], Testing Loss = [[0.90035515]]\n",
      "Epoch # 2396, Training Loss = [[0.13240134]], Testing Loss = [[0.89928546]]\n",
      "Epoch # 2397, Training Loss = [[0.13227266]], Testing Loss = [[0.89821723]]\n",
      "Epoch # 2398, Training Loss = [[0.13214422]], Testing Loss = [[0.89715046]]\n",
      "Epoch # 2399, Training Loss = [[0.13201603]], Testing Loss = [[0.89608513]]\n",
      "Epoch # 2400, Training Loss = [[0.13188807]], Testing Loss = [[0.89502126]]\n",
      "Epoch # 2401, Training Loss = [[0.13176036]], Testing Loss = [[0.89395884]]\n",
      "Epoch # 2402, Training Loss = [[0.13163288]], Testing Loss = [[0.89289786]]\n",
      "Epoch # 2403, Training Loss = [[0.13150565]], Testing Loss = [[0.89183833]]\n",
      "Epoch # 2404, Training Loss = [[0.13137865]], Testing Loss = [[0.89078024]]\n",
      "Epoch # 2405, Training Loss = [[0.13125189]], Testing Loss = [[0.88972359]]\n",
      "Epoch # 2406, Training Loss = [[0.13112537]], Testing Loss = [[0.88866837]]\n",
      "Epoch # 2407, Training Loss = [[0.13099909]], Testing Loss = [[0.8876146]]\n",
      "Epoch # 2408, Training Loss = [[0.13087304]], Testing Loss = [[0.88656225]]\n",
      "Epoch # 2409, Training Loss = [[0.13074723]], Testing Loss = [[0.88551134]]\n",
      "Epoch # 2410, Training Loss = [[0.13062165]], Testing Loss = [[0.88446186]]\n",
      "Epoch # 2411, Training Loss = [[0.13049631]], Testing Loss = [[0.8834138]]\n",
      "Epoch # 2412, Training Loss = [[0.1303712]], Testing Loss = [[0.88236717]]\n",
      "Epoch # 2413, Training Loss = [[0.13024632]], Testing Loss = [[0.88132196]]\n",
      "Epoch # 2414, Training Loss = [[0.13012168]], Testing Loss = [[0.88027817]]\n",
      "Epoch # 2415, Training Loss = [[0.12999727]], Testing Loss = [[0.8792358]]\n",
      "Epoch # 2416, Training Loss = [[0.12987309]], Testing Loss = [[0.87819485]]\n",
      "Epoch # 2417, Training Loss = [[0.12974914]], Testing Loss = [[0.87715531]]\n",
      "Epoch # 2418, Training Loss = [[0.12962542]], Testing Loss = [[0.87611718]]\n",
      "Epoch # 2419, Training Loss = [[0.12950193]], Testing Loss = [[0.87508047]]\n",
      "Epoch # 2420, Training Loss = [[0.12937867]], Testing Loss = [[0.87404516]]\n",
      "Epoch # 2421, Training Loss = [[0.12925564]], Testing Loss = [[0.87301125]]\n",
      "Epoch # 2422, Training Loss = [[0.12913284]], Testing Loss = [[0.87197875]]\n",
      "Epoch # 2423, Training Loss = [[0.12901027]], Testing Loss = [[0.87094765]]\n",
      "Epoch # 2424, Training Loss = [[0.12888792]], Testing Loss = [[0.86991795]]\n",
      "Epoch # 2425, Training Loss = [[0.1287658]], Testing Loss = [[0.86888965]]\n",
      "Epoch # 2426, Training Loss = [[0.1286439]], Testing Loss = [[0.86786274]]\n",
      "Epoch # 2427, Training Loss = [[0.12852223]], Testing Loss = [[0.86683723]]\n",
      "Epoch # 2428, Training Loss = [[0.12840078]], Testing Loss = [[0.86581311]]\n",
      "Epoch # 2429, Training Loss = [[0.12827956]], Testing Loss = [[0.86479037]]\n",
      "Epoch # 2430, Training Loss = [[0.12815857]], Testing Loss = [[0.86376902]]\n",
      "Epoch # 2431, Training Loss = [[0.12803779]], Testing Loss = [[0.86274906]]\n",
      "Epoch # 2432, Training Loss = [[0.12791724]], Testing Loss = [[0.86173048]]\n",
      "Epoch # 2433, Training Loss = [[0.12779691]], Testing Loss = [[0.86071328]]\n",
      "Epoch # 2434, Training Loss = [[0.1276768]], Testing Loss = [[0.85969746]]\n",
      "Epoch # 2435, Training Loss = [[0.12755691]], Testing Loss = [[0.85868301]]\n",
      "Epoch # 2436, Training Loss = [[0.12743724]], Testing Loss = [[0.85766994]]\n",
      "Epoch # 2437, Training Loss = [[0.12731779]], Testing Loss = [[0.85665824]]\n",
      "Epoch # 2438, Training Loss = [[0.12719856]], Testing Loss = [[0.85564791]]\n",
      "Epoch # 2439, Training Loss = [[0.12707955]], Testing Loss = [[0.85463895]]\n",
      "Epoch # 2440, Training Loss = [[0.12696076]], Testing Loss = [[0.85363136]]\n",
      "Epoch # 2441, Training Loss = [[0.12684219]], Testing Loss = [[0.85262513]]\n",
      "Epoch # 2442, Training Loss = [[0.12672383]], Testing Loss = [[0.85162026]]\n",
      "Epoch # 2443, Training Loss = [[0.12660569]], Testing Loss = [[0.85061675]]\n",
      "Epoch # 2444, Training Loss = [[0.12648776]], Testing Loss = [[0.84961461]]\n",
      "Epoch # 2445, Training Loss = [[0.12637005]], Testing Loss = [[0.84861381]]\n",
      "Epoch # 2446, Training Loss = [[0.12625256]], Testing Loss = [[0.84761437]]\n",
      "Epoch # 2447, Training Loss = [[0.12613528]], Testing Loss = [[0.84661628]]\n",
      "Epoch # 2448, Training Loss = [[0.12601821]], Testing Loss = [[0.84561955]]\n",
      "Epoch # 2449, Training Loss = [[0.12590136]], Testing Loss = [[0.84462416]]\n",
      "Epoch # 2450, Training Loss = [[0.12578471]], Testing Loss = [[0.84363011]]\n",
      "Epoch # 2451, Training Loss = [[0.12566829]], Testing Loss = [[0.84263741]]\n",
      "Epoch # 2452, Training Loss = [[0.12555207]], Testing Loss = [[0.84164605]]\n",
      "Epoch # 2453, Training Loss = [[0.12543606]], Testing Loss = [[0.84065604]]\n",
      "Epoch # 2454, Training Loss = [[0.12532027]], Testing Loss = [[0.83966735]]\n",
      "Epoch # 2455, Training Loss = [[0.12520468]], Testing Loss = [[0.83868001]]\n",
      "Epoch # 2456, Training Loss = [[0.12508931]], Testing Loss = [[0.837694]]\n",
      "Epoch # 2457, Training Loss = [[0.12497414]], Testing Loss = [[0.83670932]]\n",
      "Epoch # 2458, Training Loss = [[0.12485919]], Testing Loss = [[0.83572597]]\n",
      "Epoch # 2459, Training Loss = [[0.12474444]], Testing Loss = [[0.83474395]]\n",
      "Epoch # 2460, Training Loss = [[0.1246299]], Testing Loss = [[0.83376326]]\n",
      "Epoch # 2461, Training Loss = [[0.12451556]], Testing Loss = [[0.83278389]]\n",
      "Epoch # 2462, Training Loss = [[0.12440144]], Testing Loss = [[0.83180584]]\n",
      "Epoch # 2463, Training Loss = [[0.12428752]], Testing Loss = [[0.83082911]]\n",
      "Epoch # 2464, Training Loss = [[0.1241738]], Testing Loss = [[0.8298537]]\n",
      "Epoch # 2465, Training Loss = [[0.12406029]], Testing Loss = [[0.8288796]]\n",
      "Epoch # 2466, Training Loss = [[0.12394698]], Testing Loss = [[0.82790682]]\n",
      "Epoch # 2467, Training Loss = [[0.12383388]], Testing Loss = [[0.82693536]]\n",
      "Epoch # 2468, Training Loss = [[0.12372099]], Testing Loss = [[0.8259652]]\n",
      "Epoch # 2469, Training Loss = [[0.12360829]], Testing Loss = [[0.82499635]]\n",
      "Epoch # 2470, Training Loss = [[0.1234958]], Testing Loss = [[0.82402881]]\n",
      "Epoch # 2471, Training Loss = [[0.12338351]], Testing Loss = [[0.82306257]]\n",
      "Epoch # 2472, Training Loss = [[0.12327142]], Testing Loss = [[0.82209763]]\n",
      "Epoch # 2473, Training Loss = [[0.12315954]], Testing Loss = [[0.821134]]\n",
      "Epoch # 2474, Training Loss = [[0.12304785]], Testing Loss = [[0.82017166]]\n",
      "Epoch # 2475, Training Loss = [[0.12293636]], Testing Loss = [[0.81921062]]\n",
      "Epoch # 2476, Training Loss = [[0.12282508]], Testing Loss = [[0.81825088]]\n",
      "Epoch # 2477, Training Loss = [[0.12271399]], Testing Loss = [[0.81729243]]\n",
      "Epoch # 2478, Training Loss = [[0.1226031]], Testing Loss = [[0.81633527]]\n",
      "Epoch # 2479, Training Loss = [[0.12249241]], Testing Loss = [[0.8153794]]\n",
      "Epoch # 2480, Training Loss = [[0.12238192]], Testing Loss = [[0.81442481]]\n",
      "Epoch # 2481, Training Loss = [[0.12227163]], Testing Loss = [[0.81347151]]\n",
      "Epoch # 2482, Training Loss = [[0.12216153]], Testing Loss = [[0.8125195]]\n",
      "Epoch # 2483, Training Loss = [[0.12205163]], Testing Loss = [[0.81156877]]\n",
      "Epoch # 2484, Training Loss = [[0.12194193]], Testing Loss = [[0.81061931]]\n",
      "Epoch # 2485, Training Loss = [[0.12183242]], Testing Loss = [[0.80967114]]\n",
      "Epoch # 2486, Training Loss = [[0.1217231]], Testing Loss = [[0.80872424]]\n",
      "Epoch # 2487, Training Loss = [[0.12161398]], Testing Loss = [[0.80777861]]\n",
      "Epoch # 2488, Training Loss = [[0.12150506]], Testing Loss = [[0.80683426]]\n",
      "Epoch # 2489, Training Loss = [[0.12139632]], Testing Loss = [[0.80589117]]\n",
      "Epoch # 2490, Training Loss = [[0.12128779]], Testing Loss = [[0.80494936]]\n",
      "Epoch # 2491, Training Loss = [[0.12117944]], Testing Loss = [[0.80400881]]\n",
      "Epoch # 2492, Training Loss = [[0.12107129]], Testing Loss = [[0.80306952]]\n",
      "Epoch # 2493, Training Loss = [[0.12096332]], Testing Loss = [[0.8021315]]\n",
      "Epoch # 2494, Training Loss = [[0.12085555]], Testing Loss = [[0.80119474]]\n",
      "Epoch # 2495, Training Loss = [[0.12074797]], Testing Loss = [[0.80025924]]\n",
      "Epoch # 2496, Training Loss = [[0.12064059]], Testing Loss = [[0.799325]]\n",
      "Epoch # 2497, Training Loss = [[0.12053339]], Testing Loss = [[0.79839201]]\n",
      "Epoch # 2498, Training Loss = [[0.12042638]], Testing Loss = [[0.79746027]]\n",
      "Epoch # 2499, Training Loss = [[0.12031956]], Testing Loss = [[0.79652979]]\n",
      "Epoch # 2500, Training Loss = [[0.12021292]], Testing Loss = [[0.79560055]]\n",
      "Epoch # 2501, Training Loss = [[0.12010648]], Testing Loss = [[0.79467257]]\n",
      "Epoch # 2502, Training Loss = [[0.12000022]], Testing Loss = [[0.79374583]]\n",
      "Epoch # 2503, Training Loss = [[0.11989416]], Testing Loss = [[0.79282033]]\n",
      "Epoch # 2504, Training Loss = [[0.11978827]], Testing Loss = [[0.79189608]]\n",
      "Epoch # 2505, Training Loss = [[0.11968258]], Testing Loss = [[0.79097307]]\n",
      "Epoch # 2506, Training Loss = [[0.11957707]], Testing Loss = [[0.79005129]]\n",
      "Epoch # 2507, Training Loss = [[0.11947175]], Testing Loss = [[0.78913076]]\n",
      "Epoch # 2508, Training Loss = [[0.11936661]], Testing Loss = [[0.78821146]]\n",
      "Epoch # 2509, Training Loss = [[0.11926165]], Testing Loss = [[0.78729339]]\n",
      "Epoch # 2510, Training Loss = [[0.11915688]], Testing Loss = [[0.78637655]]\n",
      "Epoch # 2511, Training Loss = [[0.1190523]], Testing Loss = [[0.78546095]]\n",
      "Epoch # 2512, Training Loss = [[0.1189479]], Testing Loss = [[0.78454657]]\n",
      "Epoch # 2513, Training Loss = [[0.11884368]], Testing Loss = [[0.78363342]]\n",
      "Epoch # 2514, Training Loss = [[0.11873964]], Testing Loss = [[0.78272149]]\n",
      "Epoch # 2515, Training Loss = [[0.11863579]], Testing Loss = [[0.78181078]]\n",
      "Epoch # 2516, Training Loss = [[0.11853211]], Testing Loss = [[0.7809013]]\n",
      "Epoch # 2517, Training Loss = [[0.11842862]], Testing Loss = [[0.77999303]]\n",
      "Epoch # 2518, Training Loss = [[0.11832531]], Testing Loss = [[0.77908599]]\n",
      "Epoch # 2519, Training Loss = [[0.11822218]], Testing Loss = [[0.77818015]]\n",
      "Epoch # 2520, Training Loss = [[0.11811923]], Testing Loss = [[0.77727554]]\n",
      "Epoch # 2521, Training Loss = [[0.11801646]], Testing Loss = [[0.77637213]]\n",
      "Epoch # 2522, Training Loss = [[0.11791387]], Testing Loss = [[0.77546993]]\n",
      "Epoch # 2523, Training Loss = [[0.11781145]], Testing Loss = [[0.77456894]]\n",
      "Epoch # 2524, Training Loss = [[0.11770922]], Testing Loss = [[0.77366916]]\n",
      "Epoch # 2525, Training Loss = [[0.11760716]], Testing Loss = [[0.77277058]]\n",
      "Epoch # 2526, Training Loss = [[0.11750528]], Testing Loss = [[0.77187321]]\n",
      "Epoch # 2527, Training Loss = [[0.11740358]], Testing Loss = [[0.77097704]]\n",
      "Epoch # 2528, Training Loss = [[0.11730205]], Testing Loss = [[0.77008206]]\n",
      "Epoch # 2529, Training Loss = [[0.11720071]], Testing Loss = [[0.76918829]]\n",
      "Epoch # 2530, Training Loss = [[0.11709953]], Testing Loss = [[0.76829571]]\n",
      "Epoch # 2531, Training Loss = [[0.11699853]], Testing Loss = [[0.76740432]]\n",
      "Epoch # 2532, Training Loss = [[0.11689771]], Testing Loss = [[0.76651413]]\n",
      "Epoch # 2533, Training Loss = [[0.11679706]], Testing Loss = [[0.76562513]]\n",
      "Epoch # 2534, Training Loss = [[0.11669659]], Testing Loss = [[0.76473732]]\n",
      "Epoch # 2535, Training Loss = [[0.11659629]], Testing Loss = [[0.76385069]]\n"
     ]
    }
   ],
   "source": [
    "w0_initial = 0\n",
    "w_vec_initial = np.zeros((18,1))\n",
    "epsilon = 10**(-3)\n",
    "epoch = 0\n",
    "tol = 10**(-4)\n",
    "\n",
    "while True:\n",
    "\n",
    "    del_by_dels = del_by_del_w(w0_initial,w_vec_initial)\n",
    "    \n",
    "    w0_final = w0_initial - (epsilon * del_by_dels[0])\n",
    "    w_vec_final = w_vec_initial - (epsilon * del_by_dels[1])\n",
    "\n",
    "    mse_initial_train = mse(w0_initial,w_vec_initial,X_train,y_train)\n",
    "    mse_final_train = mse(w0_final,w_vec_final,X_train,y_train)\n",
    "\n",
    "    mse_initial_test = mse(w0_initial,w_vec_initial,X_test,y_test)\n",
    "\n",
    "    if abs(mse_initial_train[0][0]-mse_final_train[0][0]) < tol:\n",
    "        break\n",
    "\n",
    "    print(\"Epoch # {}, Training Loss = {}, Testing Loss = {}\".format(epoch,mse_initial_train[0][0],mse_initial_test[0][0]))\n",
    "\n",
    "    w0_initial = w0_final\n",
    "    w_vec_initial = w_vec_final\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2.92062123e-02],\n",
       "       [ 3.63359607e-02],\n",
       "       [-4.34273326e-02],\n",
       "       [ 8.01006197e-02],\n",
       "       [ 2.06089579e-01],\n",
       "       [ 7.87733516e-02],\n",
       "       [ 1.28381784e-01],\n",
       "       [ 1.36890988e-01],\n",
       "       [-2.88109548e-02],\n",
       "       [-2.79705316e-03],\n",
       "       [-8.87146451e-05],\n",
       "       [-2.58828931e-02],\n",
       "       [-8.21891212e-03],\n",
       "       [-4.29251264e-02],\n",
       "       [ 1.66126361e-02],\n",
       "       [-2.33173413e-03],\n",
       "       [ 6.04420879e-02],\n",
       "       [-1.52141936e-02]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w_vec_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6528574,
     "sourceId": 10551533,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
